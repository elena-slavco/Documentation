{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Triply Documentation What can we help you with? TriplyDB Learn how to integrate TriplyDB in your workflow TriplyETL Create production-grade Extract-Transform-Load pipelines Yasgui Learn about the SPARQL editor Didn't find what you were looking for? Contact us","title":"Home"},{"location":"#welcome-to-the-triply-documentation","text":"What can we help you with?","title":"Welcome to the Triply Documentation"},{"location":"#triplydb","text":"Learn how to integrate TriplyDB in your workflow","title":"TriplyDB"},{"location":"#triplyetl","text":"Create production-grade Extract-Transform-Load pipelines","title":"TriplyETL"},{"location":"#yasgui","text":"Learn about the SPARQL editor Didn't find what you were looking for? Contact us","title":"Yasgui"},{"location":"generics/JSON-LD-frames/","text":"What are JSON-LD frames Linked data queries most often support two ways of returning results. Either in flat dataformat, for example .csv , where each result is a separate line or record. Or as a set of linked data triples. An unordered list, in a data formats such as .ttl or .nq . The data can be interpreted by linked data tooling but the data does not follow a predefined structure. When a REST-API is queried the data is returned according to a predefined structure. The API already knows beforehand how the data will look like. With JSON-LD frames there now is a way to create predefined REST-APIs. JSON-LD frames are a deterministic translation from a graph, which has an unordered set of triples where no node is \"first\" or \"special\", into a tree, which has ordered branches and exactly one \"root\" node. In other words, JSON-LD framing allows one to force a specific tree layout to a JSON-LD document. This makes it possible to translate SPARQL queries to REST-APIs. The TriplyDB API for saved queries has been equipped with a JSON-LD profiler which can apply a JSON-LD profile to a JSON-LD result, transforming the plain JSON-LD to framed JSON. To do this you need two things. A SPARQL construct query and a JSON-LD frame. When you have both of these, you can retrieve plain JSON from a SPARQL query. The revelant cURL command when both the SPARQL query and JSON-LD frame are available is: curl -X POST [SAVED-QUERY-URL] \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Authorization: Bearer [YOUR_TOKEN]' \\ -H 'Content-type: application/json' \\ -d '[YOUR_FRAME]' When sending a curl request, a few things are important. First, the request needs to be a POST request. Only a POST request can accept a frame as a body. The Accept header needs to be set to a specific value. The Accept header needs to have both the expected returned content-type and the JSON-LD profile, e.g. application/ld+json;profile=http://www.w3.org/ns/json-ld#framed . When querying an internal or private query you need to add an authorization token. Finally, it is important to set the Content-type . It refers to the content-type of the input body and needs to be application/json , as the frame is of type application/json . The SPARQL Query Let's start with the SPARQL query. A JSON-LD frame query needs a SPARQL CONSTRUCT query to create an RDF graph that is self contained and populated with relevant vocabulary and data. The graph in JSON-LD is used as input for the REST API call. The SPARQL CONSTRUCT query can be designed with API variables. Do note that API variables with OPTIONAL s can sometimes behave a bit different than regular API variables. This is due to how SPARQL interprets OPTIONAL s. If an API variable is used in an OPTIONAL , the query will return false positives, as the OPTIONAL does not filter out results matching the API-variable. Also note that the use of UNION s can have unexpected effects on the SPARQL query. A union could split up the result set of the SPARQL query. Meaning that the SPARQL engine first exhausts the top part of the UNION and then starts with the second part of the UNION . This means that the first part of the result set can be disconnected from the second part. If the limit is set too small the result set is separated in two different JSON-LD documents. This could result in missing data in the response. Finally, please note that it can happen that you set a pageSize of 10 but the response contains less than 10 results, while the next page is not empty. This is possible as the result set of the WHERE clause is limited with a limit and not the CONSTRUCT clause. This means that two rows of the resulting WHERE clause are condensed into a single result in the CONSTRUCT clause. Thus the response of the API can differ from the pageSize . The result is a set of triples according to the query. Saving the SPARQL query will resolve in a saved query. The saved query has an API URL that we can now use in our cURL command. The URL most of the time starts with api and ends with run . The saved query url of an example query is: https://api.triplydb.com/queries/JD/JSON-LD-frame/run You could use API variables with a ? e.g. ?[queryVariable]=[value] The Frame The SPARQL query is not enough to provide the RDF data in a JSON serialization format. It requires additional syntactic conformities that cannot be defined in a SPARQL query. Thus the SPARQL query that was created needs a frame to restructure JSON-LD objects into JSON. The JSON-LD 1.1 standard allows for restructuring JSON-LD objects with a frame to JSON. A JSON-LD frame consists out of 2 parts. The @context of the response, and the structure of the response. The complete specification on JSON-LD frames can be found online The @context is the translation of the linked data to the JSON naming. In the @context all the IRIs that occur in the JSON-LD response are documented, with key-value pairs, where the key corresponds to a name the IRI will take in the REST-API response and the value corresponds to the IRI in the JSON-LD response. Most of the time the key-value pairs are one-to-one relations, where one key is mapped to a single string. Sometimes the value is an object. The object contains at least the @id , which is the IRI in the JSON-LD response. The object can also contain other modifiers, that change the REST-API response. Examples are, @type to define the datatype of the object value, or @container to define the container where the value in the REST-API response is stored in. The context can also hold references to vocabularies or prefixes. The second part of the JSON-LD frame is the structure of the data. The structure defines how the REST-API response will look like. Most of the time the structure starts with @type to denote the type that the rootnode should have. Setting the @type is the most straightforward way of selecting your rootnode. The structure is built outward from the rootnode. You can define a leafnode in the structure by adding an opening and closing bracket, as shown in the example. To define a nested node you first need to define the key that is a object property in the JSON-LD response that points to another IRI. Then from that IRI the node is created filling in the properties of that node. { \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } } The JSON-LD frame together with the SPARQL query will now result in a REST-API result: curl -X POST https://api.triplydb.com/queries/JD/JSON-LD-frame/run \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Content-type: application/json' \\ -d '{ \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } }' The JSON-LD frame turns SPARQL results for the query in step 1 into a format that is accepted as plain REST API request. Using SPARQL to create a frame Another way to create a frame is by using the SPARQL editor in TriplyDB. You can access the JSON-LD editor by clicking the three dots next to the SPARQL editor, and then selecting \"To JSON-LD frame editor\". Afterwards, the JSON script from above should be added to the JSON-LD Frame editor. Running the script results in the following REST-API result: This can also be accessed by the generated API Link above the SPARQL editor. Copying and pasting the generated link will direct you to a page where you can view the script:","title":"JSON-LD frames"},{"location":"generics/JSON-LD-frames/#what-are-json-ld-frames","text":"Linked data queries most often support two ways of returning results. Either in flat dataformat, for example .csv , where each result is a separate line or record. Or as a set of linked data triples. An unordered list, in a data formats such as .ttl or .nq . The data can be interpreted by linked data tooling but the data does not follow a predefined structure. When a REST-API is queried the data is returned according to a predefined structure. The API already knows beforehand how the data will look like. With JSON-LD frames there now is a way to create predefined REST-APIs. JSON-LD frames are a deterministic translation from a graph, which has an unordered set of triples where no node is \"first\" or \"special\", into a tree, which has ordered branches and exactly one \"root\" node. In other words, JSON-LD framing allows one to force a specific tree layout to a JSON-LD document. This makes it possible to translate SPARQL queries to REST-APIs. The TriplyDB API for saved queries has been equipped with a JSON-LD profiler which can apply a JSON-LD profile to a JSON-LD result, transforming the plain JSON-LD to framed JSON. To do this you need two things. A SPARQL construct query and a JSON-LD frame. When you have both of these, you can retrieve plain JSON from a SPARQL query. The revelant cURL command when both the SPARQL query and JSON-LD frame are available is: curl -X POST [SAVED-QUERY-URL] \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Authorization: Bearer [YOUR_TOKEN]' \\ -H 'Content-type: application/json' \\ -d '[YOUR_FRAME]' When sending a curl request, a few things are important. First, the request needs to be a POST request. Only a POST request can accept a frame as a body. The Accept header needs to be set to a specific value. The Accept header needs to have both the expected returned content-type and the JSON-LD profile, e.g. application/ld+json;profile=http://www.w3.org/ns/json-ld#framed . When querying an internal or private query you need to add an authorization token. Finally, it is important to set the Content-type . It refers to the content-type of the input body and needs to be application/json , as the frame is of type application/json .","title":"What are JSON-LD frames"},{"location":"generics/JSON-LD-frames/#the-sparql-query","text":"Let's start with the SPARQL query. A JSON-LD frame query needs a SPARQL CONSTRUCT query to create an RDF graph that is self contained and populated with relevant vocabulary and data. The graph in JSON-LD is used as input for the REST API call. The SPARQL CONSTRUCT query can be designed with API variables. Do note that API variables with OPTIONAL s can sometimes behave a bit different than regular API variables. This is due to how SPARQL interprets OPTIONAL s. If an API variable is used in an OPTIONAL , the query will return false positives, as the OPTIONAL does not filter out results matching the API-variable. Also note that the use of UNION s can have unexpected effects on the SPARQL query. A union could split up the result set of the SPARQL query. Meaning that the SPARQL engine first exhausts the top part of the UNION and then starts with the second part of the UNION . This means that the first part of the result set can be disconnected from the second part. If the limit is set too small the result set is separated in two different JSON-LD documents. This could result in missing data in the response. Finally, please note that it can happen that you set a pageSize of 10 but the response contains less than 10 results, while the next page is not empty. This is possible as the result set of the WHERE clause is limited with a limit and not the CONSTRUCT clause. This means that two rows of the resulting WHERE clause are condensed into a single result in the CONSTRUCT clause. Thus the response of the API can differ from the pageSize . The result is a set of triples according to the query. Saving the SPARQL query will resolve in a saved query. The saved query has an API URL that we can now use in our cURL command. The URL most of the time starts with api and ends with run . The saved query url of an example query is: https://api.triplydb.com/queries/JD/JSON-LD-frame/run You could use API variables with a ? e.g. ?[queryVariable]=[value]","title":"The SPARQL Query"},{"location":"generics/JSON-LD-frames/#the-frame","text":"The SPARQL query is not enough to provide the RDF data in a JSON serialization format. It requires additional syntactic conformities that cannot be defined in a SPARQL query. Thus the SPARQL query that was created needs a frame to restructure JSON-LD objects into JSON. The JSON-LD 1.1 standard allows for restructuring JSON-LD objects with a frame to JSON. A JSON-LD frame consists out of 2 parts. The @context of the response, and the structure of the response. The complete specification on JSON-LD frames can be found online The @context is the translation of the linked data to the JSON naming. In the @context all the IRIs that occur in the JSON-LD response are documented, with key-value pairs, where the key corresponds to a name the IRI will take in the REST-API response and the value corresponds to the IRI in the JSON-LD response. Most of the time the key-value pairs are one-to-one relations, where one key is mapped to a single string. Sometimes the value is an object. The object contains at least the @id , which is the IRI in the JSON-LD response. The object can also contain other modifiers, that change the REST-API response. Examples are, @type to define the datatype of the object value, or @container to define the container where the value in the REST-API response is stored in. The context can also hold references to vocabularies or prefixes. The second part of the JSON-LD frame is the structure of the data. The structure defines how the REST-API response will look like. Most of the time the structure starts with @type to denote the type that the rootnode should have. Setting the @type is the most straightforward way of selecting your rootnode. The structure is built outward from the rootnode. You can define a leafnode in the structure by adding an opening and closing bracket, as shown in the example. To define a nested node you first need to define the key that is a object property in the JSON-LD response that points to another IRI. Then from that IRI the node is created filling in the properties of that node. { \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } } The JSON-LD frame together with the SPARQL query will now result in a REST-API result: curl -X POST https://api.triplydb.com/queries/JD/JSON-LD-frame/run \\ -H 'Accept: application/ld+json;profile=http://www.w3.org/ns/json-ld#framed' \\ -H 'Content-type: application/json' \\ -d '{ \"@context\": { \"addresses\": \"ex:address\", \"Address\": \"ex:Address\", \"Object\": \"ex:Object\", \"street\": \"ex:street\", \"number\": { \"@id\": \"ex:number\", \"@type\": \"xsd:integer\" }, \"labels\": { \"@id\": \"ex:label\", \"@container\": \"@set\" }, \"ex\": \"https://triply.cc/example/\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\" }, \"@type\": \"Object\", \"labels\": {}, \"addresses\": { \"street\": {}, \"number\": {} } }' The JSON-LD frame turns SPARQL results for the query in step 1 into a format that is accepted as plain REST API request.","title":"The Frame"},{"location":"generics/JSON-LD-frames/#using-sparql-to-create-a-frame","text":"Another way to create a frame is by using the SPARQL editor in TriplyDB. You can access the JSON-LD editor by clicking the three dots next to the SPARQL editor, and then selecting \"To JSON-LD frame editor\". Afterwards, the JSON script from above should be added to the JSON-LD Frame editor. Running the script results in the following REST-API result: This can also be accessed by the generated API Link above the SPARQL editor. Copying and pasting the generated link will direct you to a page where you can view the script:","title":"Using SPARQL to create a frame"},{"location":"generics/api-token/","text":"Applications (see TriplyDB.js ) and pipelines (see TriplyETL ) often require access rights to interact with TriplyDB instances. Specifically, reading non-public data and writing any (public or non-public) data requires setting an API token. The token ensures that only users that are specifically authorized for certain datasets are able to access and/or modify those datasets. The following steps must be performed in order to create an API token: Log into the web GUI of the TriplyDB server where you have an account and for which you want to obtain special access rights in your application or pipeline. Many organizations use their own TriplyDB server. If your organization does not yet have a TriplyDB server, you can also create a free account over at TriplyDB.com . Go to your user settings page. This page is reached by clicking on the user menu in the top-right corner and choosing \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click on \u201cCreate token\u201d. Enter a name that describes the purpose of the token. This can be the name of the application or pipeline for which the API token will be used. You can use the name to manage the token later. For example, you can remove tokens for applications that are no longer used later on. It is good practice to create different API tokens for different applications. Choose the permission level that is sufficient for what you want to do with the API token. Notice that \u201cManagement access\u201d is often not needed. \u201cRead access\u201d is sufficient for read-only applications. \u201cWrite access\u201d is sufficient for most pipelines and applications that require write access. Management access: if your application must create or change organization accounts in the TriplyDB server. Write access: if your application must write (meta)data in the TriplyDB server. Read access: if your application must read public and/or private data from the TriplyDB server. Click the \u201cCreate\u201d button to create your token. The token (a long sequence of characters) will now appear in a dialog. For security reasons, the token will only be shown once. You can copy the token over to the application where you want to use it.","title":"API Token"},{"location":"generics/sparql-pagination/","text":"This page explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js. Pagination with the saved query API Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. The RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return an response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\" Pagination with TriplyDB.js TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: Import the triplydb library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context . ts import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: ts // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() To iterate the results of your SPARQL query you have three options: a. Iterate through the results per row in a for -loop: ts // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. b. Save the results to a file. This is only supported for SPARQL construct queries: ts // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') c. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. ts // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"SPARQL pagination"},{"location":"generics/sparql-pagination/#pagination-with-the-saved-query-api","text":"Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. The RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return an response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\"","title":"Pagination with the saved query API"},{"location":"generics/sparql-pagination/#pagination-with-triplydbjs","text":"TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: Import the triplydb library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context . ts import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: ts // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() To iterate the results of your SPARQL query you have three options: a. Iterate through the results per row in a for -loop: ts // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. b. Save the results to a file. This is only supported for SPARQL construct queries: ts // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') c. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. ts // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"Pagination with TriplyDB.js"},{"location":"hello-world/","text":"Photo by Joshua Earle via Unsplash Note how all headlines below show an anchor link when you hover them? \\ That's gatsby-remark-autolink-headers hooking up all MarkdownRemark headers with anchor links for us. Markdown in Gatsby Markdown parsing in Gatsby is done with gatsby-transformer-remark , which uses the excellent remark under the hood.\\ Alongside remark we also use gatsby-remark-smartypants , which provides smart punctuation through retext-smartypants . The examples on this page cover the basic Markdown syntax and are adapted from Markdown Here's Cheatsheet ( CC-BY ). This is intended as a quick reference and showcase. For more complete info, see John Gruber's original spec and the GitHub-flavored Markdown info page . Table of Contents Headers Emphasis Lists Links Images Tables Footnotes Blockquotes Inline HTML Horizontal Rule Line Breaks Headers # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 ------ H1 H2 H3 H4 H5 H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 Alt-H2 Emphasis Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Lists In this example, leading and trailing spaces are shown with with dots: \u22c5 1. First ordered list item 2. Another item \u22c5\u22c5\u22c5\u22c5* Unordered sub-list. 1. Actual numbers don't matter, just that it's a number \u22c5\u22c5\u22c5\u22c51. Ordered sub-list 4. And another item. \u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5 \u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5 * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Actual numbers don't matter, just that it's a number Ordered sub-list And another item. You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). To have a line break without a paragraph, you will need to use two trailing spaces.\\ Note that this line is separate, but within the same paragraph. Unordered list can use asterisks Or minuses Or pluses Links There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm an inline-style link with title](https://www.google.com \"Google's Homepage\") [I'm a reference-style link][Arbitrary case-insensitive reference text] [I'm a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <http://www.example.com> and sometimes example.com (but not on GitHub, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I'm an inline-style link I'm an inline-style link with title I'm a reference-style link I'm a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself . URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on GitHub, for example). Some text to show that the reference links can follow later. Images Here's our logo (hover to see the title text): Inline-style: ![alt text](https://pbs.twimg.com/profile_images/875556871427375106/Xuq8DypK_bigger.jpg \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://pbs.twimg.com/profile_images/875556871427375106/Xuq8DypK_bigger.jpg \"Logo Title Text 2\" Here's our logo (hover to see the title text): Inline-style: Reference-style: Tables Tables aren't part of the core Markdown spec, but they are part of our implementation. They are an easy way of adding tables to your email -- a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned \\$1600 col 2 is centered \\$12 zebra stripes are neat \\$1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3 Footnotes Footnotes are also not a core feature of markdown, but they're a common extension feature. The footnote syntax looks like this: This line has a footnote [^1]. Scroll down or click the link to see it. That renders like this: This line has a footnote [^1]. Scroll down or click the link to see it. Blockquotes > Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. Inline HTML You can also use raw HTML in your Markdown, and it'll mostly work pretty well. <dl> <dt>Definition list</dt> <dd>Is something people use sometimes.</dd> <dt>Markdown in HTML</dt> <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd> </dl> Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags . Horizontal Rule Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more... Hyphens Asterisks Underscores Line Breaks Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but...\\ This line is only separated by a single newline, so it's a separate line in the same paragraph . [^1]: The footnote appears at the bottom of the page","title":"Hello World: The remark Kitchen Sink"},{"location":"hello-world/#markdown-in-gatsby","text":"Markdown parsing in Gatsby is done with gatsby-transformer-remark , which uses the excellent remark under the hood.\\ Alongside remark we also use gatsby-remark-smartypants , which provides smart punctuation through retext-smartypants . The examples on this page cover the basic Markdown syntax and are adapted from Markdown Here's Cheatsheet ( CC-BY ). This is intended as a quick reference and showcase. For more complete info, see John Gruber's original spec and the GitHub-flavored Markdown info page .","title":"Markdown in Gatsby"},{"location":"hello-world/#table-of-contents","text":"Headers Emphasis Lists Links Images Tables Footnotes Blockquotes Inline HTML Horizontal Rule Line Breaks","title":"Table of Contents"},{"location":"hello-world/#headers","text":"# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 ------","title":"Headers"},{"location":"hello-world/#h1","text":"","title":"H1"},{"location":"hello-world/#h2","text":"","title":"H2"},{"location":"hello-world/#h3","text":"","title":"H3"},{"location":"hello-world/#h4","text":"","title":"H4"},{"location":"hello-world/#h5","text":"","title":"H5"},{"location":"hello-world/#h6","text":"Alternatively, for H1 and H2, an underline-ish style:","title":"H6"},{"location":"hello-world/#alt-h1","text":"","title":"Alt-H1"},{"location":"hello-world/#alt-h2","text":"","title":"Alt-H2"},{"location":"hello-world/#emphasis","text":"Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~","title":"Emphasis"},{"location":"hello-world/#lists","text":"In this example, leading and trailing spaces are shown with with dots: \u22c5 1. First ordered list item 2. Another item \u22c5\u22c5\u22c5\u22c5* Unordered sub-list. 1. Actual numbers don't matter, just that it's a number \u22c5\u22c5\u22c5\u22c51. Ordered sub-list 4. And another item. \u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5 \u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5 * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Actual numbers don't matter, just that it's a number Ordered sub-list And another item. You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). To have a line break without a paragraph, you will need to use two trailing spaces.\\ Note that this line is separate, but within the same paragraph. Unordered list can use asterisks Or minuses Or pluses","title":"Lists"},{"location":"hello-world/#links","text":"There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm an inline-style link with title](https://www.google.com \"Google's Homepage\") [I'm a reference-style link][Arbitrary case-insensitive reference text] [I'm a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <http://www.example.com> and sometimes example.com (but not on GitHub, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I'm an inline-style link I'm an inline-style link with title I'm a reference-style link I'm a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself . URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on GitHub, for example). Some text to show that the reference links can follow later.","title":"Links"},{"location":"hello-world/#images","text":"Here's our logo (hover to see the title text): Inline-style: ![alt text](https://pbs.twimg.com/profile_images/875556871427375106/Xuq8DypK_bigger.jpg \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://pbs.twimg.com/profile_images/875556871427375106/Xuq8DypK_bigger.jpg \"Logo Title Text 2\" Here's our logo (hover to see the title text): Inline-style: Reference-style:","title":"Images"},{"location":"hello-world/#tables","text":"Tables aren't part of the core Markdown spec, but they are part of our implementation. They are an easy way of adding tables to your email -- a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned \\$1600 col 2 is centered \\$12 zebra stripes are neat \\$1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3","title":"Tables"},{"location":"hello-world/#footnotes","text":"Footnotes are also not a core feature of markdown, but they're a common extension feature. The footnote syntax looks like this: This line has a footnote [^1]. Scroll down or click the link to see it. That renders like this: This line has a footnote [^1]. Scroll down or click the link to see it.","title":"Footnotes"},{"location":"hello-world/#blockquotes","text":"> Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.","title":"Blockquotes"},{"location":"hello-world/#inline-html","text":"You can also use raw HTML in your Markdown, and it'll mostly work pretty well. <dl> <dt>Definition list</dt> <dd>Is something people use sometimes.</dd> <dt>Markdown in HTML</dt> <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd> </dl> Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags .","title":"Inline HTML"},{"location":"hello-world/#horizontal-rule","text":"Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more... Hyphens Asterisks Underscores","title":"Horizontal Rule"},{"location":"hello-world/#line-breaks","text":"Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but...\\ This line is only separated by a single newline, so it's a separate line in the same paragraph . [^1]: The footnote appears at the bottom of the page","title":"Line Breaks"},{"location":"triply-api/","text":"Each Triply instance has a fully RESTful API. All functionality, from managing the Triply instance to working with your data, is done through the API. This document describes the general setup of the API, contact support@triply.cc for more information. Authentication When a dataset is published publicly, most of the read operation on that dataset can be performed without authentication. Write operations and read operations on datasets that are published internally or privately require authentication. Creating an API token Authentication is implemented through API tokens. An API token can be created within the TriplyDB UI in the following way: Log into your TriplyDB instance. Click on the user menu in the top-right corner and click on \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click the \u201cCreate token\u201d button, enter a description for the token (e.g., \u201ctest-token\u201d) and select the appropriate access rights. Click on \u201cCreate\u201d and copy the created API token (a lengthy string of characters). This string is only shown once, upon creation, and must not be shared with others. (Other users can create their own token in the here described way.) Using the API token API tokens are used by specifying them in an HTTP request header as follows: Authorization: Bearer TOKEN In the above, TOKEN should be replaced by your personal API token (a lengthy sequence of characters). See Creating an API token for information on how to create an API token. Exporting linked data Every TriplyDB API path that returns linked data provides a number of serializations to choose from. We support the following serializations: Serialization Media type File extension TriG application/trig .trig N-Triples application/n-triples .nt N-Quads application/n-quads .nq Turtle text/turtle .ttl JSON-LD * application/ld+json .jsonld To request a serialization, use one of the following mechanisms: Add an Accept header to the request. E.g. Accept: application/n-triples Add the extension to the URL path. E.g. https://api.triplydb.com/datasets/Triply/iris/download.nt * Downloading datasets in JSON-LD format is not supported. Datasets Triply API requests are always directed towards a specific URI path. URI paths will often have the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/ Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. ACCOUNT :: The name of a specific user or a specific organization. DATASET :: The name of a specific dataset. Here is an example of a URI path that points to the Triply API for the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/ Accounts Information about TriplyDB accounts (organizations and users) can be retrieved from the following API path: https://api.INSTANCE/accounts Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. Here is an example of a URI path that points to the Triply API for the Triply organization account: https://api.triplydb.com/accounts/Triply Queries {#queries} TriplyDB allows users to save SPARQL queries. The metadata for all saved query can be accessed as follows: https://api.triplydb.com/queries By adding an account name (for example: 'Triply'), metadata for all saved queries for that account can be accessed as follows: https://api.triplydb.com/queries/Triply By adding an account name and a query name (for example: 'Triply/flower-length'), metadata for one specific saved query can be accessed as follows: https://api.triplydb.com/queries/Triply/flower-length Query metadata (GRLC) {#grlc} You can retrieve a text-based version of each query, by requesting the text/plain content type: curl -vL -H 'Accept: text/plain' 'https://api.triplydb.com/queries/JD/pokemonNetwork' This returns the query string, together with metadata annotations. These metadata annotations use the GRLC format . For example: #+ description: This query shows a small subgraph from the Pokemon dataset. #+ endpoint: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql #+ endpoint_in_url: false construct where { ?s ?p ?o. } limit 100 Notice that the GRLC annotations are encoded in SPARQL comments, i.e. lines that start with the hash character ( # ). This makes the result immediately usable as a SPARQL query. The above example includes the following GRLC annotations: description gives a human-readable description of the meaning of the query. This typically includes an explanation of the purpose or goal for which this query is used, the content returned, or the process or task in which this query is used. endpoint The URL of the SPARQL endpoint where queries are sent to. endpoint_in_url configures whether the URL of the SPARQL endpoint should be specified through the API. In TriplyDB, this configuration is by default set to false . (Users of the REST API typically expect domain parameters such as countryName or maximumAge , but they do not necessarily expect technical parameters like an endpoint URL.) LD Browser API Triply APIs provide a convenient way to access data used by LD Browser , which offers a comprehensive overview of a specific IRI. By using Triply API for a specific IRI, you can retrieve the associated 'document' in the .nt format that describes the IRI. To make an API request for a specific instance, you can use the following URI path: https://api.triplydb.com/datasets/ACCOUNT/DATASET/describe.nt?resource=RESOURCE To illustrate this, let's take the example of the DBpedia dataset and the specific instance of 'Mona Lisa' . If you use this URI path: https://api.triplydb.com/datasets/DBpedia-association/dbpedia/describe.nt?resource=http%3A%2F%2Fdbpedia.org%2Fresource%2FMona_Lisa in your browser, the .nt document describing the 'Mona Lisa' instance will be automatically downloaded. You can then upload this file to a dataset and visualize it in a graph . Figure 1 illustrates the retrieved graph for the \u2018Mona Lisa\u2019 instance. The requested resource will be displayed in the center of the graph, forming an 'ego graph'. It will include all direct properties, as well as some indirect properties that are also pulled in by LD Browser. The labels for all classes and properties will be included for easy human-readable display. In addition, this API also supports traversing blank node-replacing well-known IRIs (CBD style), and limits the number of objects per subject/property to manage the description size. This corresponds to the \"Show more\" button in the LD Browser GUI, ensuring a manageable and user-friendly experience. Triple Pattern Fragments (TPF) Triple Pattern Fragments (TPF) is a community standard that allows individual linked datasets to be queried for Triply Patterns (TP), a subset of the more complex SPARQL query language. The Triply API implements Triple Pattern Fragments version 2019-01-18 and Linked Data Fragments version 2016-06-05. The Triple Pattern Fragments (TPF) API is available for all datasets in Triply and does not require running a dedicated service. URI path TPF requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/fragments Reply format Since TPF replies distinguish between data and metadata that are stored in different graphs, it is recommended to request the TriG content type with the following HTTP request header: Accept: application/trig Query parameters Triple Pattern Fragments (TPF) uses the following query parameters in order to retrieve only those triples that adhere to a specified Triple Pattern: Key Value Purpose subject A URL-encoded IRI. Only return triples where the given IRI appears in the subject position. predicate A URL-encoded IRI. Only return triples where the given IRI appears in the predicate position. object A URL-encoded IRI or literal. Only return triples where the given IRI or literal appears in the object position. Example request curl -G \\ 'https://api.triplydb.com/datasets/academy/pokemon/fragments' \\ --data-urlencode 'predicate=http://www.w3.org/2000/01/rdf-schema#label' \\ -H 'Accept: application/trig' Exporting data To export the linked data, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download Query parameters By default, an export includes all linked data graphs. Use a query argument to specify a particular graph. Key Value Purpose graph A URL-encoded IRI. Only download the export of the given graph IRI. Therefore, to export the linked data of a graph , use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download/?graph=GRAPH To find out which graphs are available, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/graphs Example requests Export a dataset: curl 'https://api.triplydb.com/datasets/academy/pokemon/download' \\ -H 'Accept: application/trig' > exportDataset.trig.gz Export a graph: First, find out which graphs are available: curl 'https://api.triplydb.com/datasets/academy/pokemon/graphs' Then, download one of the graph: curl 'curl 'https://api.triplydb.com/datasets/academy/pokemon/download?graph=https://triplydb.com/academy/pokemon/graphs/data' -H 'Accept: application/trig' > exportGraph.trig.gz Services Some API requests require the availability of a specific service over the dataset. These requests are directed towards a URI path of the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/ Upper-case letter words must be replaced by the following values: SERVICE :: The name of a specific service that has been started for the corresponding dataset. See the previous section for Datasets to learn the meaning of INSTANCE , ACCOUNT , and DATASET . Here is an example of a URI path that points to a SPARQL endpoint over the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql See the following sections for more information on how to query the endpoints provided by services: - SPARQL - Elasticsearch SPARQL There are two service types in TriplyDB that expose the SPARQL 1.1 Query Language: \"Sparql\" and \"Jena\". The former works well for large quantities of instance data with a relatively small data model; the latter works well for smaller quantities of data with a richer data model. SPARQL services expose a generic endpoint URI at the following location (where ACCOUNT , DATASET and SERVICE are user-chosen names): https://api.triplydb.com/datasets/ACCOUNT/DATASET/services/SERVICE/sparql Everybody who has access to the dataset also has access to its services, including its SPARQL services: - For Public datasets, everybody on the Internet or Intranet can issue queries. - For Internal datasets, only users that are logged into the triple store can issue queries. - For Private datasets, only users that are logged into the triple store and are members of ACCOUNT can issue queries. Notice that for professional use it is easier and better to use saved queries . Saved queries have persistent URIs, descriptive metadata, versioning, and support for reliable large-scale pagination ( see how to use pagination with saved query API ). Still, if you do not have a saved query at your disposal and want to perform a custom SPARQL request against an accessible endpoint, you can do so. TriplyDB implements the SPARQL 1.1 Query Protocol standard for this purpose. Sending a SPARQL Query request According to the SPARQL 1.1 Protocol, queries can be send in the 3 different ways that are displayed in Table 1 . For small query strings it is possible to send an HTTP GET request (row 1 in Table 1 ). A benefit of this approach is that all information is stored in one URI. For public data, copy/pasting this URI in a web browser runs the query. For larger query strings it is required to send an HTTP POST request (rows 2 and 3 in Table 1 ). The reason for this is that longer query strings result in longer URIs when following the HTTP GET approach. Some applications do not support longer URIs, or they even silently truncate them resulting in an error down the line. The direct POST approach (row 3 in Table 1 ) is the best of these 3 variants, since it most clearly communicates that it is sending a SPARQL query request (see the Content-Type column). HTTP Method Query String Parameters Request Content-Type Request Message Body query via GET GET query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) none none query via URL-encoded POST POST none application/x-www-form-urlencoded URL-encoded, ampersand-separated query parameters. query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) query via POST directly POST default-graph-uri (0 or more) named-graph-uri (0 or more) application/sparql-query Unencoded SPARQL query string Table 1 - Overview of the three different ways in which SPARQL queries can be issues over HTTP. SPARQL Query result formats SPARQL services are able to return results in different formats. The user can specify the preferred format by specifying the corresponding Media Type in the HTTP Accept header. TriplyDB supports the following Media Types. Notice that the chosen result format must be supported for your query form. Result format Media Type Query forms CSV text/csv SELECT JSON application/json ASK, SELECT JSON-LD application/ld+json CONSTRUCT, DESCRIBE N-Quads application/n-quads CONSTRUCT, DESCRIBE N-Triples application/n-triples CONSTRUCT, DESCRIBE RDF/XML application/rdf+xml CONSTRUCT, DESCRIBE SPARQL JSON application/sparql-results+json ASK, SELECT SPARQL XML application/sparql-results+xml ASK, SELECT TriG application/trig CONSTRUCT, DESCRIBE TSV text/tab-separated-values SELECT Turtle text/turtle CONSTRUCT, DESCRIBE Examples of SPARQL Query requests This section contains examples of SPARQL HTTP requests. The requests run either of the following two SPARQL queries against a public SPARQL endpoint that contains data about Pokemon: select * { ?s ?p ?o. } limit 1 construct where { ?s ?p ?o. } limit 1 The examples made use of the popular command-line tool cURL . These examples should also work in any other HTTP client tool or library. GET request curl https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql?query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] URL-encoded POST request curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ --data query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] Direct POST request curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] SPARQL JSON Like the previous example, but with an Accept header that specifies Media Type application/sparql-results+json : curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: { \"head\": { \"vars\": [\"s\", \"p\", \"o\"] }, \"results\": { \"bindings\": [ { \"s\": { \"type\": \"uri\", \"value\": \"https://triplydb.com/academy/pokemon/vocab/\" }, \"p\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" }, \"o\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/2002/07/owl#Ontology\" } } ] } } SPARQL XML Like the previous example, but with Media Type application/sparql-results+xml in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+xml' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: <sparql xmlns=\"http://www.w3.org/2005/sparql-results#\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.w3.org/2001/sw/DataAccess/rf1/result2.xsd\"> <head> <variable name=\"s\"/> <variable name=\"p\"/> <variable name=\"o\"/> </head> <results distinct=\"false\" ordered=\"true\"> <result> <binding name=\"s\"> <uri>https://triplydb.com/academy/pokemon/vocab/</uri> </binding> <binding name=\"p\"> <uri>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</uri> </binding> <binding name=\"o\"> <uri>http://www.w3.org/2002/07/owl#Ontology</uri> </binding> </result> </results> </sparql> SPARQL tab-separated values Like the previous examples, but with Media Type text/tab-separated-values in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/tab-separated-values' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' \"s\" \"p\" \"o\" \"https://triplydb.com/academy/pokemon/vocab/\" \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" \"http://www.w3.org/2002/07/owl#Ontology\" SPARQL comma-separated values Like the previous examples, but with Media Type text/csv in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/csv' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: \"s\",\"p\",\"o\" \"https://triplydb.com/academy/pokemon/vocab/\",\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\"http://www.w3.org/2002/07/owl#Ontology\" JSON-LD Like the previous examples, but with a SPARQL construct query and Media Type application/ld+json in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/ld+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] } N-Quads Like the previous examples, but with Media Type application/n-quads in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-quads' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] } N-Triples Like the previous examples, but with Media Type application/n-triples in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-triples' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: <https://triplydb.com/academy/pokemon/vocab/> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Ontology> . TriG Like the previous examples, but with Media Type application/trig in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/trig' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology . Turtle Like the previous examples, but with Media Type text/turtle in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/turtle' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology . Elasticsearch The text search API returns a list of linked data entities based on a supplied text string. The text string is matched against the text in literals and IRIs that appear in the linked data description of the returned entities. The text search API is only available for a dataset after an Elasticsearch service has been created for that dataset. Two types of searches can be performed: a simple search, and a custom search. Simple searches require one search term for a fuzzy match. Custom searches accept a JSON object conforming to the Elasticsearch query DSL . URI path Text search requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/search Reply format The reply format is a JSON object. Search results are returned in the JSON array that is stored under key sequence \"hits\"/\"hits\" . The order in which search results appear in the array is meaningful: better matches appear earlier. Every search result is represented by a JSON object. The name of the linked data entity is specified under key sequence \"_id\" . Properties of the linked data entity are stored as IRI keys. The values of these properties appear in a JSON array in order to allow more than one object term per predicate term (as is often the case in linked data). The following code snippet shows part of the reply for the below example request. The reply includes two results for search string \u201cmew\u201d, returning the Pok\u00e9mon Mew (higher ranked result) and Mewtwo (lower ranked result). { \"hits\": { \"hits\": [ { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mew\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/151\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 100 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"\u30df\u30e5\u30a6\" ], \u2026 }, { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mewtwo\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/150\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 110 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEWTU\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"\u30df\u30e5\u30a6\u30c4\u30fc\" ], \u2026 } ] }, \u2026 } Examples Simple search Perform a search for the string mew : curl 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search?query=mew' Custom search Perform a search using the custom query: { \"query\": { \"simple_query_string\": { \"query\": \"pikachu\" } } } curl -X POST 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search' \\ -d '{\"query\":{\"simple_query_string\":{\"query\":\"pikachu\"}}}' \\ -H 'content-type: application/json'","title":"Triply API"},{"location":"triply-api/#authentication","text":"When a dataset is published publicly, most of the read operation on that dataset can be performed without authentication. Write operations and read operations on datasets that are published internally or privately require authentication.","title":"Authentication"},{"location":"triply-api/#creating-an-api-token","text":"Authentication is implemented through API tokens. An API token can be created within the TriplyDB UI in the following way: Log into your TriplyDB instance. Click on the user menu in the top-right corner and click on \u201cUser settings\u201d. Go to the \u201cAPI tokens\u201d tab. Click the \u201cCreate token\u201d button, enter a description for the token (e.g., \u201ctest-token\u201d) and select the appropriate access rights. Click on \u201cCreate\u201d and copy the created API token (a lengthy string of characters). This string is only shown once, upon creation, and must not be shared with others. (Other users can create their own token in the here described way.)","title":"Creating an API token"},{"location":"triply-api/#using-the-api-token","text":"API tokens are used by specifying them in an HTTP request header as follows: Authorization: Bearer TOKEN In the above, TOKEN should be replaced by your personal API token (a lengthy sequence of characters). See Creating an API token for information on how to create an API token.","title":"Using the API token"},{"location":"triply-api/#exporting-linked-data","text":"Every TriplyDB API path that returns linked data provides a number of serializations to choose from. We support the following serializations: Serialization Media type File extension TriG application/trig .trig N-Triples application/n-triples .nt N-Quads application/n-quads .nq Turtle text/turtle .ttl JSON-LD * application/ld+json .jsonld To request a serialization, use one of the following mechanisms: Add an Accept header to the request. E.g. Accept: application/n-triples Add the extension to the URL path. E.g. https://api.triplydb.com/datasets/Triply/iris/download.nt * Downloading datasets in JSON-LD format is not supported.","title":"Exporting linked data"},{"location":"triply-api/#datasets","text":"Triply API requests are always directed towards a specific URI path. URI paths will often have the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/ Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. ACCOUNT :: The name of a specific user or a specific organization. DATASET :: The name of a specific dataset. Here is an example of a URI path that points to the Triply API for the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/","title":"Datasets"},{"location":"triply-api/#accounts","text":"Information about TriplyDB accounts (organizations and users) can be retrieved from the following API path: https://api.INSTANCE/accounts Upper-case letter words must be replaced by the following values: INSTANCE :: The host name of the TriplyDB instance that you want to use. Here is an example of a URI path that points to the Triply API for the Triply organization account: https://api.triplydb.com/accounts/Triply","title":"Accounts"},{"location":"triply-api/#queries-queries","text":"TriplyDB allows users to save SPARQL queries. The metadata for all saved query can be accessed as follows: https://api.triplydb.com/queries By adding an account name (for example: 'Triply'), metadata for all saved queries for that account can be accessed as follows: https://api.triplydb.com/queries/Triply By adding an account name and a query name (for example: 'Triply/flower-length'), metadata for one specific saved query can be accessed as follows: https://api.triplydb.com/queries/Triply/flower-length","title":"Queries {#queries}"},{"location":"triply-api/#query-metadata-grlc-grlc","text":"You can retrieve a text-based version of each query, by requesting the text/plain content type: curl -vL -H 'Accept: text/plain' 'https://api.triplydb.com/queries/JD/pokemonNetwork' This returns the query string, together with metadata annotations. These metadata annotations use the GRLC format . For example: #+ description: This query shows a small subgraph from the Pokemon dataset. #+ endpoint: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql #+ endpoint_in_url: false construct where { ?s ?p ?o. } limit 100 Notice that the GRLC annotations are encoded in SPARQL comments, i.e. lines that start with the hash character ( # ). This makes the result immediately usable as a SPARQL query. The above example includes the following GRLC annotations: description gives a human-readable description of the meaning of the query. This typically includes an explanation of the purpose or goal for which this query is used, the content returned, or the process or task in which this query is used. endpoint The URL of the SPARQL endpoint where queries are sent to. endpoint_in_url configures whether the URL of the SPARQL endpoint should be specified through the API. In TriplyDB, this configuration is by default set to false . (Users of the REST API typically expect domain parameters such as countryName or maximumAge , but they do not necessarily expect technical parameters like an endpoint URL.)","title":"Query metadata (GRLC) {#grlc}"},{"location":"triply-api/#ld-browser-api","text":"Triply APIs provide a convenient way to access data used by LD Browser , which offers a comprehensive overview of a specific IRI. By using Triply API for a specific IRI, you can retrieve the associated 'document' in the .nt format that describes the IRI. To make an API request for a specific instance, you can use the following URI path: https://api.triplydb.com/datasets/ACCOUNT/DATASET/describe.nt?resource=RESOURCE To illustrate this, let's take the example of the DBpedia dataset and the specific instance of 'Mona Lisa' . If you use this URI path: https://api.triplydb.com/datasets/DBpedia-association/dbpedia/describe.nt?resource=http%3A%2F%2Fdbpedia.org%2Fresource%2FMona_Lisa in your browser, the .nt document describing the 'Mona Lisa' instance will be automatically downloaded. You can then upload this file to a dataset and visualize it in a graph . Figure 1 illustrates the retrieved graph for the \u2018Mona Lisa\u2019 instance. The requested resource will be displayed in the center of the graph, forming an 'ego graph'. It will include all direct properties, as well as some indirect properties that are also pulled in by LD Browser. The labels for all classes and properties will be included for easy human-readable display. In addition, this API also supports traversing blank node-replacing well-known IRIs (CBD style), and limits the number of objects per subject/property to manage the description size. This corresponds to the \"Show more\" button in the LD Browser GUI, ensuring a manageable and user-friendly experience.","title":"LD Browser API"},{"location":"triply-api/#triple-pattern-fragments-tpf","text":"Triple Pattern Fragments (TPF) is a community standard that allows individual linked datasets to be queried for Triply Patterns (TP), a subset of the more complex SPARQL query language. The Triply API implements Triple Pattern Fragments version 2019-01-18 and Linked Data Fragments version 2016-06-05. The Triple Pattern Fragments (TPF) API is available for all datasets in Triply and does not require running a dedicated service.","title":"Triple Pattern Fragments (TPF)"},{"location":"triply-api/#uri-path","text":"TPF requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/fragments","title":"URI path"},{"location":"triply-api/#reply-format","text":"Since TPF replies distinguish between data and metadata that are stored in different graphs, it is recommended to request the TriG content type with the following HTTP request header: Accept: application/trig","title":"Reply format"},{"location":"triply-api/#query-parameters","text":"Triple Pattern Fragments (TPF) uses the following query parameters in order to retrieve only those triples that adhere to a specified Triple Pattern: Key Value Purpose subject A URL-encoded IRI. Only return triples where the given IRI appears in the subject position. predicate A URL-encoded IRI. Only return triples where the given IRI appears in the predicate position. object A URL-encoded IRI or literal. Only return triples where the given IRI or literal appears in the object position.","title":"Query parameters"},{"location":"triply-api/#example-request","text":"curl -G \\ 'https://api.triplydb.com/datasets/academy/pokemon/fragments' \\ --data-urlencode 'predicate=http://www.w3.org/2000/01/rdf-schema#label' \\ -H 'Accept: application/trig'","title":"Example request"},{"location":"triply-api/#exporting-data","text":"To export the linked data, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download","title":"Exporting data"},{"location":"triply-api/#query-parameters_1","text":"By default, an export includes all linked data graphs. Use a query argument to specify a particular graph. Key Value Purpose graph A URL-encoded IRI. Only download the export of the given graph IRI. Therefore, to export the linked data of a graph , use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/download/?graph=GRAPH To find out which graphs are available, use the following path: https://api.INSTANCE/datasets/ACCOUNT/DATATSET/graphs","title":"Query parameters"},{"location":"triply-api/#example-requests","text":"Export a dataset: curl 'https://api.triplydb.com/datasets/academy/pokemon/download' \\ -H 'Accept: application/trig' > exportDataset.trig.gz Export a graph: First, find out which graphs are available: curl 'https://api.triplydb.com/datasets/academy/pokemon/graphs' Then, download one of the graph: curl 'curl 'https://api.triplydb.com/datasets/academy/pokemon/download?graph=https://triplydb.com/academy/pokemon/graphs/data' -H 'Accept: application/trig' > exportGraph.trig.gz","title":"Example requests"},{"location":"triply-api/#services","text":"Some API requests require the availability of a specific service over the dataset. These requests are directed towards a URI path of the following form: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/ Upper-case letter words must be replaced by the following values: SERVICE :: The name of a specific service that has been started for the corresponding dataset. See the previous section for Datasets to learn the meaning of INSTANCE , ACCOUNT , and DATASET . Here is an example of a URI path that points to a SPARQL endpoint over the Pok\u00e9mon dataset: https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql See the following sections for more information on how to query the endpoints provided by services: - SPARQL - Elasticsearch","title":"Services"},{"location":"triply-api/#sparql","text":"There are two service types in TriplyDB that expose the SPARQL 1.1 Query Language: \"Sparql\" and \"Jena\". The former works well for large quantities of instance data with a relatively small data model; the latter works well for smaller quantities of data with a richer data model. SPARQL services expose a generic endpoint URI at the following location (where ACCOUNT , DATASET and SERVICE are user-chosen names): https://api.triplydb.com/datasets/ACCOUNT/DATASET/services/SERVICE/sparql Everybody who has access to the dataset also has access to its services, including its SPARQL services: - For Public datasets, everybody on the Internet or Intranet can issue queries. - For Internal datasets, only users that are logged into the triple store can issue queries. - For Private datasets, only users that are logged into the triple store and are members of ACCOUNT can issue queries. Notice that for professional use it is easier and better to use saved queries . Saved queries have persistent URIs, descriptive metadata, versioning, and support for reliable large-scale pagination ( see how to use pagination with saved query API ). Still, if you do not have a saved query at your disposal and want to perform a custom SPARQL request against an accessible endpoint, you can do so. TriplyDB implements the SPARQL 1.1 Query Protocol standard for this purpose.","title":"SPARQL"},{"location":"triply-api/#sending-a-sparql-query-request","text":"According to the SPARQL 1.1 Protocol, queries can be send in the 3 different ways that are displayed in Table 1 . For small query strings it is possible to send an HTTP GET request (row 1 in Table 1 ). A benefit of this approach is that all information is stored in one URI. For public data, copy/pasting this URI in a web browser runs the query. For larger query strings it is required to send an HTTP POST request (rows 2 and 3 in Table 1 ). The reason for this is that longer query strings result in longer URIs when following the HTTP GET approach. Some applications do not support longer URIs, or they even silently truncate them resulting in an error down the line. The direct POST approach (row 3 in Table 1 ) is the best of these 3 variants, since it most clearly communicates that it is sending a SPARQL query request (see the Content-Type column). HTTP Method Query String Parameters Request Content-Type Request Message Body query via GET GET query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) none none query via URL-encoded POST POST none application/x-www-form-urlencoded URL-encoded, ampersand-separated query parameters. query (exactly 1) default-graph-uri (0 or more) named-graph-uri (0 or more) query via POST directly POST default-graph-uri (0 or more) named-graph-uri (0 or more) application/sparql-query Unencoded SPARQL query string Table 1 - Overview of the three different ways in which SPARQL queries can be issues over HTTP.","title":"Sending a SPARQL Query request"},{"location":"triply-api/#sparql-query-result-formats","text":"SPARQL services are able to return results in different formats. The user can specify the preferred format by specifying the corresponding Media Type in the HTTP Accept header. TriplyDB supports the following Media Types. Notice that the chosen result format must be supported for your query form. Result format Media Type Query forms CSV text/csv SELECT JSON application/json ASK, SELECT JSON-LD application/ld+json CONSTRUCT, DESCRIBE N-Quads application/n-quads CONSTRUCT, DESCRIBE N-Triples application/n-triples CONSTRUCT, DESCRIBE RDF/XML application/rdf+xml CONSTRUCT, DESCRIBE SPARQL JSON application/sparql-results+json ASK, SELECT SPARQL XML application/sparql-results+xml ASK, SELECT TriG application/trig CONSTRUCT, DESCRIBE TSV text/tab-separated-values SELECT Turtle text/turtle CONSTRUCT, DESCRIBE","title":"SPARQL Query result formats"},{"location":"triply-api/#examples-of-sparql-query-requests","text":"This section contains examples of SPARQL HTTP requests. The requests run either of the following two SPARQL queries against a public SPARQL endpoint that contains data about Pokemon: select * { ?s ?p ?o. } limit 1 construct where { ?s ?p ?o. } limit 1 The examples made use of the popular command-line tool cURL . These examples should also work in any other HTTP client tool or library.","title":"Examples of SPARQL Query requests"},{"location":"triply-api/#get-request","text":"curl https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql?query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ]","title":"GET request"},{"location":"triply-api/#url-encoded-post-request","text":"curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ --data query=select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo.%20%7D%20limit%201 Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ]","title":"URL-encoded POST request"},{"location":"triply-api/#direct-post-request","text":"curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: [ { \"s\": \"https://triplydb.com/academy/pokemon/vocab/\", \"p\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"o\": \"http://www.w3.org/2002/07/owl#Ontology\" } ]","title":"Direct POST request"},{"location":"triply-api/#sparql-json","text":"Like the previous example, but with an Accept header that specifies Media Type application/sparql-results+json : curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: { \"head\": { \"vars\": [\"s\", \"p\", \"o\"] }, \"results\": { \"bindings\": [ { \"s\": { \"type\": \"uri\", \"value\": \"https://triplydb.com/academy/pokemon/vocab/\" }, \"p\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" }, \"o\": { \"type\": \"uri\", \"value\": \"http://www.w3.org/2002/07/owl#Ontology\" } } ] } }","title":"SPARQL JSON"},{"location":"triply-api/#sparql-xml","text":"Like the previous example, but with Media Type application/sparql-results+xml in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/sparql-results+xml' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: <sparql xmlns=\"http://www.w3.org/2005/sparql-results#\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.w3.org/2001/sw/DataAccess/rf1/result2.xsd\"> <head> <variable name=\"s\"/> <variable name=\"p\"/> <variable name=\"o\"/> </head> <results distinct=\"false\" ordered=\"true\"> <result> <binding name=\"s\"> <uri>https://triplydb.com/academy/pokemon/vocab/</uri> </binding> <binding name=\"p\"> <uri>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</uri> </binding> <binding name=\"o\"> <uri>http://www.w3.org/2002/07/owl#Ontology</uri> </binding> </result> </results> </sparql>","title":"SPARQL XML"},{"location":"triply-api/#sparql-tab-separated-values","text":"Like the previous examples, but with Media Type text/tab-separated-values in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/tab-separated-values' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' \"s\" \"p\" \"o\" \"https://triplydb.com/academy/pokemon/vocab/\" \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" \"http://www.w3.org/2002/07/owl#Ontology\"","title":"SPARQL tab-separated values"},{"location":"triply-api/#sparql-comma-separated-values","text":"Like the previous examples, but with Media Type text/csv in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/csv' \\ -H 'Content-Type: application/sparql-query' \\ -d 'select * { ?s ?p ?o } limit 1' Result: \"s\",\"p\",\"o\" \"https://triplydb.com/academy/pokemon/vocab/\",\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\"http://www.w3.org/2002/07/owl#Ontology\"","title":"SPARQL comma-separated values"},{"location":"triply-api/#json-ld","text":"Like the previous examples, but with a SPARQL construct query and Media Type application/ld+json in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/ld+json' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] }","title":"JSON-LD"},{"location":"triply-api/#n-quads","text":"Like the previous examples, but with Media Type application/n-quads in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-quads' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: { \"@graph\": [ { \"@id\": \"https://triplydb.com/academy/pokemon/vocab/\", \"@type\": \"http://www.w3.org/2002/07/owl#Ontology\" } ] }","title":"N-Quads"},{"location":"triply-api/#n-triples","text":"Like the previous examples, but with Media Type application/n-triples in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/n-triples' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: <https://triplydb.com/academy/pokemon/vocab/> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2002/07/owl#Ontology> .","title":"N-Triples"},{"location":"triply-api/#trig","text":"Like the previous examples, but with Media Type application/trig in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: application/trig' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology .","title":"TriG"},{"location":"triply-api/#turtle","text":"Like the previous examples, but with Media Type text/turtle in the Accept header: curl -X POST https://api.triplydb.com/datasets/academy/pokemon/services/pokemon/sparql \\ -H 'Accept: text/turtle' \\ -H 'Content-Type: application/sparql-query' \\ -d 'construct where { ?s ?p ?o } limit 1' Result: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . <https://triplydb.com/academy/pokemon/vocab/> rdf:type owl:Ontology .","title":"Turtle"},{"location":"triply-api/#elasticsearch","text":"The text search API returns a list of linked data entities based on a supplied text string. The text string is matched against the text in literals and IRIs that appear in the linked data description of the returned entities. The text search API is only available for a dataset after an Elasticsearch service has been created for that dataset. Two types of searches can be performed: a simple search, and a custom search. Simple searches require one search term for a fuzzy match. Custom searches accept a JSON object conforming to the Elasticsearch query DSL .","title":"Elasticsearch"},{"location":"triply-api/#uri-path_1","text":"Text search requests are sent to the following URI path: https://api.INSTANCE/datasets/ACCOUNT/DATASET/services/SERVICE/search","title":"URI path"},{"location":"triply-api/#reply-format_1","text":"The reply format is a JSON object. Search results are returned in the JSON array that is stored under key sequence \"hits\"/\"hits\" . The order in which search results appear in the array is meaningful: better matches appear earlier. Every search result is represented by a JSON object. The name of the linked data entity is specified under key sequence \"_id\" . Properties of the linked data entity are stored as IRI keys. The values of these properties appear in a JSON array in order to allow more than one object term per predicate term (as is often the case in linked data). The following code snippet shows part of the reply for the below example request. The reply includes two results for search string \u201cmew\u201d, returning the Pok\u00e9mon Mew (higher ranked result) and Mewtwo (lower ranked result). { \"hits\": { \"hits\": [ { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mew\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/151\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 100 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"MEW\", \"\u30df\u30e5\u30a6\" ], \u2026 }, { \"_id\": \"https://triply.cc/academy/pokemon/id/pokemon/mewtwo\", \"http://open vocab org/terms/canonicalUri\": [ \"http://pokedex.dataincubator.org/pokemon/150\" ], \"https://triply cc/academy/pokemon/def/baseAttack\": [ 110 ], \"https://triply cc/academy/pokemon/def/name\": [ \"MEWTU\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"MEWTWO\", \"\u30df\u30e5\u30a6\u30c4\u30fc\" ], \u2026 } ] }, \u2026 }","title":"Reply format"},{"location":"triply-api/#examples","text":"","title":"Examples"},{"location":"triply-api/#simple-search","text":"Perform a search for the string mew : curl 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search?query=mew'","title":"Simple search"},{"location":"triply-api/#custom-search","text":"Perform a search using the custom query: { \"query\": { \"simple_query_string\": { \"query\": \"pikachu\" } } } curl -X POST 'https://api.triplydb.com/datasets/academy/pokemon/services/search/search' \\ -d '{\"query\":{\"simple_query_string\":{\"query\":\"pikachu\"}}}' \\ -H 'content-type: application/json'","title":"Custom search"},{"location":"triply-db-getting-started/","text":"Introduction TriplyDB allows you to store, publish, and use linked data Knowledge Graphs. TriplyDB makes it easy to upload linked data and expose it through various APIs (SPARQL, Elasticsearch, LDF, REST). Read More Uploading Data This section explains how to create a linked dataset in TriplyDB. Creating a new dataset The following steps allow a new linked datasets to be created: Log into a TriplyDB instance. Click the + button on the dataset pane that appears on the right-hand side of the home screen. This brings up the dialog for creating a new dataset. Enter a dataset name that consists of alpha-numeric characters ( A-Za-z0-9 ) and hyphens ( - ). Optionally, enter a dataset display name. This name will be used in the GUI and will be exposed in dataset metadata. Optionally, enter a dataset description. You can use rich text formatting by using Markdown. See our section about Markdown for details. Optionally, change the access level for the dataset. By default this is set to \u201cPrivate\u201d. See dataset access levels for more information. When datasets are Public (see Access Levels ), they automatically expose metadata and are automatically crawled and indexed by popular search engines (see Metadata ). Adding data Once the dataset is created, the \u201cAdd data\u201d view is displayed (see screenshot). In this view data can be added in three ways: File upload Select files from your local machine. It is also possible to drag-and-drop local files on the cloud icon with the upward pointing arrow. URL upload Copy/paste a URL that points to an online RDF file. Import Import a dataset that is already published in the same TriplyDB instance. The \u201cAdd data\u201d view is also available for existing datasets: Go to the \u201cGraphs\u201d view by clicking on the graph icon in the left-hand sidebar. Click the \u201cImport a new graph\u201d button. Adding data: File upload The file upload feature allows you to upload RDF files from your local machine. This can be done in either of two ways: Click on the cloud icons to open a dialog that allows local RDF files from your machine to be selected. Drag-and-drop RDF files from your local machine onto the cloud icon. The following RDF serialization formats are currently supported: Serialization Format File extension N-Quads .nq N-Triples .nt RDF/XML .rdf , .owl , .owx TriG .trig Turtle .ttl , .n3 JSON-LD .jsonld , .json Up to 1,000 separate files can be uploaded in one go. It is also possible to upload compressed files and archives. When the number of files exceeds 1,000, it is more efficient to combine them in archives and upload those. This allows an arbitrary number of files to be uploaded. The following archive/compression formats are currently supported: Format File extensions gzip .gz bzip2 .bz2 tar tar XZ .xz ZIP .zip Adding data by URL upload The second option for adding data is to include it from an online URL location. This is done by entering the URL inside the \u201cAdd data from a URL\u201d text field. Adding data by import The third option for adding data is to import from datasets that are published in the same TriplyDB instance. This is done with the \u201cAdd data from an existing dataset\u201d dropdown list (see screenshot). Adding malformed data TriplyDB only allows valid RDF data to be added. If data is malformed, TriplyDB will show an error message that indicates which part of the RDF data is malformed (see screenshot). If such malformed data is encountered, the RDF file must first be corrected and uploaded again. TriplyDB follows the linked data standards strictly. Many triple stores allow incorrect RDF data to be added. This may seem convenient during the loading phase, but often results in errors when standards-compliant tools start using the data. Assets: binary data Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB and can be integrated into the Knowledge Graph. Publishing data With TriplyDB you can easily make your data available to the outside world. Publishing your dataset You can publish your dataset by setting the visibility to \u201cPublic\u201d in the dataset settings menu. Making a dataset public in TriplyDB has the following consequences: The dataset can be searched for and visited by anybody on the web. The dataset will be indexed by web search engines such as Google Dataset Search . Any services that are started for that dataset will be available to anybody on the web. This includes SPARQL , text search , and linked data Fragments . Entering metadata Adding metadata to your datasets is important. This makes it easier to find your dataset later and also allows search engines and social media applications to understand your dataset. Metadata is entered from the dataset settings page, which is accessed by clicking on the \u201cDataset settings\u201d (cog icon) option from the left-hand sidebar (see screenshot). The dataset homepage looks empty without metadata. Notice the cog wheel icon, which provides access to the \u201cDataset settings\u201d page. The dataset settings page allows the following metadata to be entered: The dataset description. This can consist of text with (optional) Markdown formatting. The avatar (image) of the dataset. The access level of the dataset. The topics of the dataset. The example resources for the dataset. The license of the dataset. The dataset settings page allows various forms of dataset metadata to be added. Within the TriplyDB instance your dataset is now more findable for users. Whenever a user searches on one of the topics of your dataset, or types in a word that is present in the description of your dataset, the dataset will be shown as a search result. The metadata will allow TriplyDB to give a better impression of your dataset when a user visits: The dataset home page after metadata has been entered. Search engines and social media applications can recognize the metadata that is entered for datasets in TriplyDB. The following screenshot shows the widget created by the Slack chat application upon entering a link to the dataset. Notice that the chat application understands metadata properties like title, description, and image. Widget created by the Slack chat application upon sharing a link to a dataset in TriplyDB. Starting services By default, datasets in TriplyDB can be queried through TriplyDB-js as well as through the Linked Data Fragments API. In order to allow additional query paradigms, specific services can be started from the \u201cCreate service\u201d page. This page is accessed by clicking on the \u201cServices\u201d icon in the left-hand sidebar. TriplyDB instances can be configured with different types of services. The below screenshot shows the \u201cCreate service\u201d page for a TriplyDB instance that allows SPARQL, Jena SPARQL, and Elasticsearch services to be created. Notice that three different types of services can be created. It is possible to create multiple services for one dataset. Existing services Existing services are displayed on service widgets (see screenshot). From these widgets, services can be created or deleted. Datasets can change whenever a graph is added, deleted or renamed. When this happens, the data in a service is out of sync with the data in the dataset and a synchronization button will appear in the service widget. By clicking the button, the service will be synchronized with the current state of the dataset. Viewing Data TriplyDB offers several ways to explore your datasets. Linked Data Browser The linked data browser offers to traverse the data by focusing on one node at the time. The node is describe using it's properties, which can be followed to other nodes in the graph. The following properties provide additional information about your linked data, enabling the LD-browser to display visualizations and provide a better user experience. Types The predicate rdf:type allows you to specify the type or class of a resource in your linked data: By using rdf:type , you can indicate the category or classification of the resource, which can help the LD-browser understand the nature of the data and display it appropriately. In the example below , you can see that \"Iris setosa\" is the type of flowering plant due to the usage of the rdf:type property. Labels Labels are typically used to provide a concise and meaningful title or display name for a resource, making it easier for users to understand the content of your linked data. These predicates allow you to provide human-readable labels or names for your resources: The property rdfs:label The property skos:prefLabel In the example below , the rdfs:label property was used to denote the label(name) of the Pokemon, resulting in the display of \"Pikachu\" above its corresponding image. Descriptions Descriptions can provide additional context or information about a resource, helping users understand its purpose, content, or significance. These predicates allow you to provide textual descriptions or comments about your resources: The property sdo:description The property rdfs:comment In the following example rdfs:comment was used to provide additional information on Iris Setosa. Geo These are some of the predicates used for representing geographic information in your LD-browser: The property geo:asWKT : This property allows you to specify the geometries of geographic features using the Well-Known Text (WKT) format, which can be visualized on a map in the LD-browser. The property geo:hasGeometry : This property is used to link a geographic feature with its corresponding geometry. The property geo:location : This property is used to indicate the location of a resource using geographic coordinates, such as latitude and longitude, in your linked data. In the following example geo:hasGeometry property was used to showcase a map depicting the location of Instituut voor Beeld en Geluid. Images These predicates allow you to associate images or visual representations with your resources: Class sdo:ImageObject The property foaf:depiction The property foaf:thumbnail The property foaf:img The property sdo:image The property sdo:contentUrl By using these predicates, you can provide URLs or references to images that can be displayed alongside your linked data in the LD-browser. In the example below , foaf:depiction was used to display picture of Pikachu in the LD-browser: Audio These predicates allow you to associate audio content with your resources: The class sdo:AudioObject The property sdo:audio The property sdo:contentUrl You can use these predicates to provide URLs or references to audio files that can be played or streamed within the LD-browser. In the following example , sdo:audio was used to showcase audio content of the Carnival Festival within the LD-browser. Video These predicates allow you to associate video content with your resources: Class sdo:VideoObject Property sdo:video Property sdo:contentUrl You can use these predicates to provide URLs or references to video files that can be played or streamed within the LD-browser.The video formats that are included in this dataset are \".mp4\", \".webm\", \".ogg\". In the following example , sdo:contentUrl was used to showcase video content of the Kleine Piep within the LD-browser. Linked Data Table The linked data Table shows a dataset at the triple level. The first three columns represent the subject, predicate, and object position of the triple. The fourth column represents the graph to which the triple belongs. The linked data Table can be used to perform simple queries by filling in the subject, predicate, object, and/or graph using the text field. Terms in the linked data Table can be dragged and dropped between columns. This allows a simple form of graph navigation. For example, an object term can be dragged to the subject column in order to show the triples in which that term appears in the subject position. Queries in the linked data Table can also be performed automatically through the Statements API and the Triple Pattern Fragments API . SPARQL IDE When a dataset has a running SPARQL service, the data can be queried from the SPARQL IDE. The SPARQL IDE is an extended version of the Open Source Yasgui query editor. Saving a SPARQL query It is often useful to save a SPARQL query for later use. This is achieved by clicking on the save icon in the top-right corner of the SPARQL Editor. Doing so will create a Save Query . Sharing a SPARQL query It is sometimes useful to share a SPARQL query with somebody else, or to have a cURL command that can be used to run the same SPARQL query from a command line. This is achieved by clicking on the share icon in the top-right corner of the SPARQL Editor. This brings us a dialog from which the SPARQL query can be copied in the following three forms: The URL-encoded SPARQL query. This is a long URL that includes the endpoint, the query, and visualization settings. Notice that this URL can be quite long for complex queries and/or visualizations. Long URLs are not supported by some application that cut off a URL after a maximum length (often 1,024 characters). Use one of the other two options or use Saved Queries to avoid such restrictions. A short URL that redirects to the full URL-encoded SPARQL query. A cURL command that can be copy/pasted into a terminal application that supports this command. cURL is often used by programmers to test HTTP(S) requests. Saved Queries are a more modern way of sharing SPARQL queries. They do not have any of the technical limitations that occur with URL-encoded queries. Transfer a SPARQL query The SPARQL queries could be transferred to another account or an organization. To do that, go to the setting field at the query page, choose transfer and then choose where the SPARQL query should be moved to. After the destination is set you would be redirected to the SPARQL query new page. The SPARQL query could be transferred from an account to an organization and vice versa. Copy a SPARQL query Users can copy SPARQL queries to another account or an organization. To do that, click on three dots in the upper right corner of the query and choose the copy option. Then, choose where the SPARQL query should be moved to. After setting the destination, you will be redirected to the SPARQL query new page. The SPARQL query can be copied from an account to an organization and vice versa. ElasticSearch When a dataset has a running Elasticsearch service, textual searches can be performed over the entire dataset. Text search with Elasticsearch works like a search engine and returns any node that contains your search term, or contains the search term in any of its properties. It is also possible to write a custom query using the Elasticsearch Query DSL (Domain Specific Language) . Insights The insights page has been developed to give you a succinct overview of the linked data at hand. It holds two views: the class frequency and the class hierarchy view. Class frequency The class frequency diagram shows how often classes and properties appear in a graph. The drop-down on the top of the visualization selects the graph for which the class frequency is drawn. The visualization shows the 10 most frequent classes in the selected graph. The exact number of occurrences can be seen when hovering over the bar of a class, also showing the complete IRI/prefixed IRI. When clicking on the bar of a class the node will expand and show the 10 most frequent predicates of that class. Class hierarchy The class hierarchy diagram shows the hierarchy of the dataset in three different visualizations. Each of the diagrams are created by the rdfs:subClassOf relations and the classes in the dataset. TriplyDB has three different visualization methods for the classHierarchy: Bubbles visualization Treemap visualization Sunburst visualization All three visualizations are interactive in two ways. It is possible to hover over them, which will show information about the layer the mouse is on, or to click on them, so the visualization zooms in one or more layers. For each visualization it is also possible to zoom out: Bubbles visualization: click the outside of the bubble Treemap visualization: use the breadcrumbs trail shown above the visualization Sunburst visualization: click the innermost circle of the visualization When does the class hierarchy show? A class only appears in the class hierarchy tab if it has instances (connected to the class via rdf:type ). The class hierarchy cannot be shown if it contains a cycle, meaning that some class is (indirectly) its own subclass. Exporting Data This section explains how a user of TriplyDB can export linked data stored in the triple store. Export Datasets The data stored on TriplyDB is stored in two different containers: datasets and graphs. Each triple contained in a dataset is part of exactly one graph. A graph is always a part of a dataset and a dataset can have multiple graphs. The following screenshot shows the dataset \"Pok\u00e9mon\" that contains three graphs: \"data\" and \"vocab\". The graph \"data\" contains 28.588 triples and the graph \"vocab\" 185 triples. By summing up the amount of triples contained in the two graphs the dataset \"Pok\u00e9mon\" contains 28.773 triples in total. To export the dataset users can click on the downwards facing arrow. In our example, the dataset is automatically downloaded as the file \"pokemon.trig\" and compressed with .gz. The name of the file is the name of the dataset. The used serialization format is \".trig\" because that is the standard format to store triples that are appended to graphs. It is also possible to export the whole dataset on the graphs interface. Select \"Graphs\" and \"EXPORT ALL GRAPHS\". Export Graphs To export only one graph select \"Graphs\" and the arrow next to the graph that should be exported. In this case the downloaded file \"https___triplydb.com_academy_pokemon_graphs_data.trig.gz\" is named after the graph and also compressed with \"gz\". Extract The process of extracting the compressed file is the same for exporting graphs and datasets. The downloaded and compressed file is automatically stored in the \"Downloads\" folder. Select the file with the ending \".gz\" and open it with a double click. This opens an application that looks similar to the following screenshot: Select the file that should be extracted. In this case select \"pokemon.trig\" and click on \"Extract\". In the following step choose the location where the extracted file should be stored. Saved Queries A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query. How to save a query There are two ways to create a saved query. You need to be logged in and have authorization rights on the dataset to use this feature When working from the SPARQL IDE Using the Saved Queries tab in a dataset Creating a saved query with the SPARQL IDE is done by writing a query/visualization and hitting the save button Creating a new version Updating the saved query can be done by clicking a query in the Saved Queries tab and editing the query or the visualization. Hit the save button to save it as a new version. Using a saved query Sharing a saved query To share a saved query, for example in Data Stories , you can copy the link that is used when you open the query in TriplyDB. Let's say you have a query called Timelined-Cars-BETA in the dataset core under the account dbpedia and you want to use version 6. Then the following link would be used https://triplydb.com/DBpedia-association/-/queries/timeline-cars/8 If you want to always use the latest query, you can simply omit the version number like so https://triplydb.com/DBpedia-association/-/queries/timeline-cars Downloading a query result The result of a query can be downloaded via the TriplyDB interface. After downloading the query, open it in TriplyDB. e.g. https://triplydb.com/DBpedia-association/-/queries/timeline-cars/8. Choose the option Response and click on the download icon or scroll down and click on Download result . The downloaded file is automatically stored in the Downloads -folder and has the name of the query. In our example, the file is called timeline-cars.json . The downloaded file contains the query result as a json-object. TriplyDB also displays the json-object when selecting the option Response . To download the query result in CSV-format, select the option Table and click on the download icon. The downloaded file is named after the query with the suffix .csv . Download more than 10 000 query results - SPARQL pagination This section explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js. Pagination with the saved query API Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. he RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return an response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\" Pagination with TriplyDB.js TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: Import the TriplyDB library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context . ts import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: ts // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() To iterate the results of your SPARQL query you have three options: a. Iterate through the results per row in a for -loop: ts // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. b. Save the results to a file. This is only supported for SPARQL construct queries: ts // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') c. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. ts // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray() Using a saved query as REST-API (Advanced) Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. A saved query can be used as a RESTful API to retrieve data from your linked dataset. The URL next to the keywork API is the RESTful API URL and can be used with RESTful API libraries. You can copy the RESTful API by pressing the copy button just behind the URL. Pressing the copy button from the above query will result in the following run url: https://api.triplydb.com/queries/DBpedia-association/timeline-cars/run When you copy this URL in your browser or fetch the URL with curl, you will get a get request to a RESTful API and get a JSON representation of the data in your browser or command window. Using a saved query in Python or R notebooks (Advanced) SPARQL queries as a RESTful API, also means you can transport your data to your Python script, R script or Jupyter notebook. To use the result set from your SPARQL query you need to connect your script to the saved SPARQL query. To do this you will need to write a small connector. To help you out TriplyDB has added a code snippet generator for Python and R. This snippet contains the code to retrieve the data from the SPARQL query into your script or notebook. You can open the code snippet generator by clicking on the '' button on the right side of the screen. Clicking the '' button opens the code snippet screen. Here you select the snippet in the language you want to have, either Python or R. You can then copy the snippet, by clicking the 'copy to clipboard' button or selecting the snippet and pressing ctrl-c . Now you can paste the code in the location you want to use the data. The data is stored in the data variable in JSON format. When the SPARQL query is not public, but instead either private or internal, you will need to add an authorization header to the get request. Without the authorization header the request will return an incorrect response. Checkout Creating your API token about creating your API-token for the authorization header. Check out the SPARQL pagination page when you want to query a SPARQL query that holds more than 10.000 results. The SPARQL pagination page will explain how you can retrieve the complete set. Query metadata {#query-metadata} Every Saved Query has a metadata section. This metadata section includes the following two links: - A link to the dataset over which the query is executed. Clicking this links navigates to the dataset homepage. - A link to the service by which the query is executed. Clicking this link navigates to the services page that includes that service. Users can specify a query title and description, both of which are included as metadata. The access level and version of the query are also exposed as metadata. See the following screenshot for how the metadata fields are shown in TriplyDB: Users can specify additional metadata inside the query string, by using the GRLC annotation format. GRLC annotations start with the hash and plus sign characters ( #+ ). Visit the GRLC project to learn more about this format. For example, the following GRLC annotation could indicate to a software application that the query should be repeated every hour: #+ frequency: hourly See the Triply API documentation for how to retrieve query metadata, including how to retrieve GRLC annotations. Data stories {#data-stories} A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results. Creating a data story You can create your own data story via the stories tab on TriplyDB. If this is your first time creating a data story, your view will look something like the image below. If you already are a proud owner of a data story, you will find it here. To create a new one, you can click the orange \"Create story\" button and you\u2019ll see the same form. In this form, you can fill in the title and set the access level for a data story. When everything is set, press the \"Create story\" button.This will take you to a new page where you can customize the data story. Editing a data story As shown in the image below, in the top right corner of the page, there is a menu button. Here you will find the following: - Story settings : Here you can change the title and the access level of your story. - Change banner : Here you can change the banner, just choose an image that you want as your banner (wide images work best). - Transfer : To transfer the story to a different user or organization. - Copy : To copy the story to a different user or organization. - Delete : To delete the story. In the right lower corner you see a button with a notepad. With this button, you can toggle between the edit view, which allows you to edit the story, and the reader view, which is how readers of your story will perceive this page. Adding elements To create your first element press \"+ Add new element\". This will open a new form as shown in the images below. Here you can select what kind of element you want to add to your data story; you\u2019ll have the option to write text, to select an already existing SPARQL query, or even to create a new SPARQL query. Existing query Let\u2019s start by selecting a query for our data story. Maybe you have already created one, but if you haven\u2019t, you can select one of the queries available to you. You can search in the Query search bar and select the one you want, for example \"our-first-select-query\". Optionally you can select the version of the query and set the caption. When everything is set, press \"Create story element\". And look, we have added our first element to our story! Paragraph Data sometimes needs accompanying text to be completely understandable. TriplyDB not only supports writing plain text, but TriplyDB paragraphs are also markdown compliant. The markdown that you\u2019ll add in the paragraph text box will be rendered as HTML and can be previewed. TriplyDB also supports images, and even code blocks with highlighting for the most common linked data and programming languages. Sharing and embedding Before you know it, you will have created your first data story. Congratulations! Now it is time to share it with the world, but don\u2019t forget to set the access level to \u201cpublic\u201d. Then you have two options: 1. You can simply share the URL in TriplyDB. 2. You can embed the Data Story on your own webpage. Scroll all the way to the end of your Data Story and click the \u201c Embed\u201d button. This brings up a code snippet that you can copy/paste into your own HTML web page. Admin settings Pages You can use the console to perform administrator tasks. The administrator tasks are performed within the admin settings page. The admin settings pages are accessible by clicking on the user menu in the top-right corner and selecting the \u201cAdmin settings\u201d menu item. You must have administrator privileges to access these pages and perform administrator tasks. Overview page The first page that comes into view when opening the admin settings pages is the overview page. This page contains an overview of all the important statistics of the instance. The page also shows how close the instance is to hitting one or more limits. If no limit is set, the statistics are shown as a counter. If a limit is set a gauge is shown with a green, orange or red bar. The colors denote how far that statistic of the instance is to the limit. Green means not close to the limit, Orange means close to the limit, Red means over the limit. General overview The general overview gives an insight into the software version of the instance. Each instance consists of a console and an API. The console is the web interface of the instance and has a build date corresponding to the build date of the docker image of the console and a version number corresponding to the version of the docker image. The API is the layer between the console and the data. The API is separate from the console and is a different docker image. The API also has a version and build date of the docker image. Also contains a starting time, and an updated time, the moments when the docker image is started for this instance or when the docker image is updated for the instance. Accounts overview The accounts overview shows how many organizations and users are in this instance. The organizations and users are shown in a counter if no limit is set. If a limit is set on the number of organizations and/or users of the instance a gauge is shown. Data overview The data overview shows multiple statistics about datasets. The first counter shows the amount of datasets on the instance. The second and third counters show the amount of graphs and statements in all graphs. The fourth and fifth counters show the amount of unique graphs and statements. When a graph is copied from one dataset to another, the data in that graph does not change. The amount of unique data does not change either. The amount of unique data is a more representative way of calculating the amount of data in the instance. All statistics are shown in a counter, if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown. Services overview The data overview shows how multiple statistics about services. The first counter shows the total amount of services on the instance, The second counter shows the total amount of statements in all the services. Then for each of our service types a specific counter is created. Each containing the amount of services and the amount of statements in that service. All statistics are shown in a counter if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown. Settings page The settings page is the main page for administrators to institute instance wide changes. An administrator can change the site logo's here, change the contact email or update site wide prefixes. Set logos and banner For changing the logos and the banner follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. Under \"Site logos\" and \"Site banner\" you can upload a site logo (square and landscape) or a banner. Make sure you use SVG files with a maximum size of 5 MB. Setting metadata For changing the metadata follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site metadata\", it looks as follows: Here you can set the name, tag line, description and welcome text. The name of your website appears in your browser tab. The welcome text appears on the homepage of your TriplyDB instance. The tagline and description are for metadata purposes (e.g. findability and website previews). Setting contact email For changing the contact email follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Contact Email\". Here, you can change the contact email to a new contact email for the instance. Setting example datasets Example datasets are introduction datasets on the frontpage of your instance. The Example datasets are datasets that are interesting for people that visit your page to see and interact with. Most often you'll use open datasets to show them off on the frontpage. You can also use internal or private datasets, but these will only be visible if the person seeing them has the right access rights. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page, navigate to \"Example datasets\". Here, you can execute the following changes: You can move datasets up and down in the order by clicking and holding your left mouse button over the three horizontal lines in front of the dataset name. You can then drag the selected dataset to their new spot. In the search field below the already added datasets you can add a new example dataset by typing in the search field and selecting the correct dataset. You can remove datasets by pressing the x on the right side of the dataset name to remove it from the example dataset list. Setting Starter dataset The starter dataset is a beginner-friendly linked dataset that can be an introduction into linked data when a user creates an account for the first time. The starter dataset is visible for a user when the user has not yet created a dataset on its own account. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Starter dataset\". Here you can change the starter dataset to a new starter dataset for the instance by typing in the search bar a name of an existing dataset to replace the started dataset. This dataset then will be presented to users on their account page, with an option to import(copy) them immediately. This needs to be a public dataset! If it's not public, new users will have to create a dataset. The starter dataset is only shown if the user currently has no datasets. Setting Authentication One of the roles of an administrator is to make sure only the right people will sign up for the TriplyDB instance. To do this, an administrator can set up authentication protocols. The authentication protocols can block people from signing up to instances where they are not allowed to sign up to. For changing the authentication protocols follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Authentication\". Now you can change the password sign up. Allowing people to only register with a password or they are only allowed to register with a google or Github account. When password signup is enabled, the administrator can also set the permitted signup domains. Only users with e-mail addresses that match these domains are allowed to sign-up. Wildcards are allowed and domains are comma separated, for example: mydomain.com,*.mydomain.com. Setting Site-wide prefixes One of the advantages of using TriplyDB is that you can set site-wide prefixes once and use them everywhere on the instance. Site-wide prefixes are prefixes defined in the admin settings and can be used for all datasets that contain the IRIs matching the prefixes. For editing the side-wide prefixes follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site-wide prefixes\". Here, you can execute the following changes: Each field of the already added site-wide prefixes you can edit. You can edit the prefix label by typing in the first field. You can edit the prefix IRI and in the second field. Pressing UPDATE PREFIXES updates the list. In the last field below the already added site-wide prefixes you can add a new site-wide prefix by typing in the first field the prefix label, and in the second field the prefix IRI. Pressing UPDATE PREFIXES updates the list. You can remove prefixes by pressing the x on the right side of the prefixes name to remove it from the site-wide prefixes list. Account overview page The account page governs all the accounts of an instance. The paginated table shows all the accounts of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific accounts according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all accounts automatically on the created at date with the latest created at date accounts first. The filters on top of the table can be used to filter the following columns: Name The name of the account, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the account. Type Type of the account, this can either be 'Organization' or 'User'. In the filter you can select a specific account type or 'All' account types. Display name The display name of the account, often an account has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Email The email address of the account. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Created at How long ago an account was created. When you hover over the text you can see the precise moment an account is created. You can order accounts based on the moment of creation. Updated at How long ago an account has been updated with new metadata such as display name or password. When you hover over the text you can see the precise moment an account is last updated. You can order accounts based on the moment of updated at time. Last activity How long ago the account has been last active. When you hover over the text you can see the precise moment an account was last active. You can order the accounts based on the moment of last time the account was active. Role Role of the account, this can either be 'light', 'regular' or 'administrator'. In the filter you can select a specific role or 'All' roles. Verified An account can be verified or not, to verify an account, the user needs to click on the verify button in the email. Or an administrator has verified the account in the account settings of that account. Only 'users' need to be verified. Disabled An account can be disabled or not, to disabled an account, the user needs to click on the disabled button in their user settings. Or an administrator has disabled the account in the account settings of that account. legal consent An account can have accepted the legal consent or not, to accept legal consent, the user needs to click on the accept legal consent either when creating an account or by checking it in the user settings. Only 'users' need to have accepted legal consent. For each account you can execute the following actions: Open account settings For each account, there is a button such that the administrator can directly go to the account settings of the user or organization. The account settings are behind the `cogwheel` button. Add new user(s) Go to the \u201cAccounts tab\u201d to receive an overview of all accounts on the TriplyDB instance. The type of account can be observed based on the following icons: Icon Account type organization user Create a new user New users can only be created by administrators by performing the following steps: Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \u201cAccounts\u201d tab. This brings up an overview of all users and organizations on the TriplyDB instance. Click the \u201cAdd user\u201d button. Fill in the user name and email address of the prospective user. The user name must consist of alphanumeric characters ( A-Za-z ) and hyphens ( - ). Click the \u201cAdd user\u201d button. This sends an account creation email to the prospective user, containing a link that allows them to log in. In addition to the above default procedure, the following two options are provided for user account creation: Temporary account : By default, user accounts do not expire. Sometimes it is useful to create a temporary account by specifying a concrete date in the \u201cAccount expiration date\u201d widget. Preset password : By default, a user can set her password after logging in for the first time by clicking on the link in the account creation email. When a password is entered in the \u201cPassword\u201d field, the user must enter this password in order to log in for the first time. Datasets page The account page governs all the datasets of an instance. The paginated table shows all the datasets of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific datasets according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all datasets automatically on the created at date with the latest created at date datasets first. The filters on top of the table can be used to filter the following columns: Name The name of the dataset, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the dataset. Access level Access level of the dataset, this can either be 'Public', 'Internal' or 'Private'. In the filter you can select a specific access level or 'All' access levels. Display name The display name of the dataset, often a dataset has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Owner The owner of the dataset. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Graph count The amount of graphs in a dataset. These are all the total amount of graphs in a dataset, and can be filtered with the slider. Statement count The amount of statements in a dataset. These are all the statements of all the graphs, and can be filtered with the slider. Service count The amount of services in a dataset. These can be filtered with the slider. Asset count The amount of assets in a dataset. These can be filtered with the slider. Created at How long ago a dataset has been created. When you hover over the text you can see the precise moment a dataset is created. You can order datasets based on the moment of creation. Updated at How long ago a dataset has been updated with new metadata such as display name or new data. When you hover over the text you can see the precise moment an account is last updated. You can order dataset based on the moment of updated at time. Last graph edit How long ago the last graph has been edited, either new data is uploaded or removed, or the graph names changed. When you hover over the text you can see the precise moment a dataset was edited. You can order the accounts based on the moment of last time the dataset was last edited. For each dataset you can execute the following actions: Open dataset settings For each dataset there is button such that the administrator can directly go to the dataset settings of the dataset. The dataset settings are behind the `cogwheel` button. Services page The services page governs all the services of an instance. The paginated table shows all the services of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific services according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all services automatically if a service is in an error state or not. All services that are in error state will be shown at the top of the table. This way immediate action can be taken to check the service. The filters on top of the table can be used to filter the following columns: Name The name of the SPARQL service, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the service. Type Type of the service, this can either be 'Virtuoso', 'Jena', 'Blazegraph', 'Prolog' or 'Elasticsearch'. In the filter you can select a specific service type or 'All' service types. Status The status of the service, can be 'Starting', 'Running', 'Stopped', 'Updating' or 'Error'. In the filter you can select a specific service status or 'All' services statuses Statements The amount of statements in a service. These are all the loaded statements in the service, and can be filtered with the slider. Loaded graphs Amount of graphs loaded in the service. All the statements of all the graphs together will count up to the total amount of statements. Dataset The dataset the service belongs to. The dataset is clickable and brings you to the dataset page. The datasets can be filtered based on the sequence of characters appearing in the filter. Owner The owner of the dataset is also the owner of the service. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Created How long ago a service has been created. When you hover over the text you can see the precise moment a service is created. You can order the services based on the moment of creation. Last queried How long ago the service has been last queried. When you hover over the text you can see the precise moment a service is last queried. You can order the services based on the moment of last time the service has been queried. Auto stops Some services are equipped with an auto stop feature. This feature reduces the memory resources when a service is not queried in a while. The column `Auto stops` shows how long it will take before a service is auto-stopped. You can order the services on when the auto-stop feature kicks in. Each time a service is used the timer is reset. Version A service always has a particular version. A service is not automatically updated as it could be that the service has possible down time. The owner of the service can update a service when they deem it necessary to update to the latest version. For each service you can execute the following actions: Update the service When a service can be updated an orange arrow will appear just below the service. When you press the update service button the service is automatically updated to the latest service version. Open additional information For each service there is additional information available. The additional information is behind the `i` button. The additional information contains information about the graphs in the dataset and a raw information view of the service metadata. Inspect the logs For each service there is a log available. The logs are behind the `text` button. The logs contain information Synchronize the service The service can be outdated. This happens when the data in the dataset does not corresponds with the data in the service. When this happens the service can be synchronized from here to make it up to date with the latest version of the data. Remove the service When a service is no longer necessary or there needs to be made some space on the instance a service can be removed from here. Some of these actions can be cumbersome when you need to do them one at a time. To help with this, on the left side of the table you can click on the tickbox. This will select all the services that match search criteria if there search criteria and all tables when there are no search criteria. When pressed you can now remove all selected services or update all selected services to a new software version. Redirects page The great thing about linked data is that IRIs are used to define objects in linked data. Then when you visit the IRIs you find useful information about the object. But sometimes the data is not on the location where the IRI is pointing towards. You have the IRI: https://example.org/resource/Amsterdam but the information about the object is located in the dataset https://api.triplydb.com/MyAccount/myCities. This is a problem as the IRI is pointing to a location that does not contain the data, and the data is at a location that is not found without the correct IRI. This is where you can use redirects to redirect the user from the IRI to the location where the data is found. How to setup a redirects for dereferencing Redirects enable easy dereferencing of resources. For example, you can dereference a resource https://example.org/resource/Amsterdam into dataset https://api.triplydb.com/MyAccount/myCities by following these steps: First update the web server of where the IRI is originally pointing towards the redirect API. In this example all subpaths of /resource are to be redirected from https://example.org to https://api.triplydb.com/redirect/$requestUri. this means that when a request for https://example.org/resource/Amsterdam comes to the web server of https://example.org it will be redirected to https://api.triplydb.com/redirect/https://example.org/resource/Amsterdam. Now that the external web server is set up to redirect to TriplyDB, TriplyDB needs to be configured to accept the request and redirect it to the correct dataset. This is done by adding a rule on the administrator redirects page. To add a rule, press the ADD RULE button to begin with the creation of a new rule. For this example we want to add a prefix rule with the pattern to match https://example.org/resource/City/. The prefix rule needs a dataset to redirect to. This will be the dataset https://api.triplydb.com/myAccount/myCities. Press CREATE RULE to create the rule. Each rule is evaluated when a request comes in https://api.triplydb.com/redirect/$requestUri and mapping rules are evaluated from top (highest priority) to bottom (lowest priority). When a match is found the requestUri is then redirected to that location. TriplyDB supports two types of mapping rules: Prefix Prefix rules trigger when the start of a resource matches the specified string. Regex Regular Expression rules trigger when a resource matches a Regular Expression. Reference Access levels TriplyDB uses the following access levels for datasets, queries, and stories. Access level Description Icon Private The dataset/query/story is only visible to you. Internal The dataset/query/story is only visible to people who are logged in to the same TriplyDB. Public The dataset/query/story is visible to everybody on the Internet. Access level dependencies The access levels for datasets, queries, and stories may affect each other. For example, if a public query references a private dataset, other users will be able to view the query string, but none of the query results. TriplyDB always uses the most conservative access level in such cases, ensuring that information is never exposed unintentionally. Access levels and workflows These access levels are often used for the following workflow: You create a new dataset/query/story starts with access level \u2018Private\u2019. As the dataset/query/story progresses, give it access level \u2018Internal\u2019 to receive feedback from other users. Once the dataset/query/story is ready, give it access level \u2018Public\u2019 to publish it to the world. Markdown support Triply allows rich text formatting to be used in the following places: Dataset description Account description Saved Query description Data Story elements Site welcome message The following Markdown elements are supported: Headings Headings are used to divide a text into different sections. The hash character ( # ) at the beginning of a line indicates a heading is used. Multiple hash characters indicate nested headings. # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 Text styling Style Syntax Output Bold **bold** bold Italic _italic_ italic Strikethrough ~~strikethrough~~ ~~strikethrough~~ Hyperlinks Style Syntax Output Raw URL <https://triply.cc> https://triply.cc Labeled URL [label](https://triply.cc) label Notice that URLs can also be relative. This allows you to refer to other datasets, saved queries, etc. by using relative paths. Code There are options for formatting in-line code as well as multi-line code blocks. In-line code Code can also be used in-line with single backticks: Use `code` inside a sentence. Multi-line code blocks Multi-line code blocks start and end with three consecutive backticks. The following Markdown denotes two lines of Turtle: select * { graph ?g { ?s ?p ?o. } } The above is rendered as follows: select * { graph ?g { ?s ?p ?o. } } Code language The opening backticks are optionally following by the name of the code language. The following code languages are supported: Language Syntax SPARQL sparql Turtle ttl TypeScript typescript R r Python python The other supported languages are: Bash ( bash ), C ( c ), C++ ( cpp ), C# ( csharp ), Extended Backus-Naur Form ( ebnf ), Go ( go ), Haskell ( haskell ), Java ( java ), JavaScript ( javascript ), LaTeX ( latex ), Makefile ( makefile ), Markdown ( markdown ), Objective C ( objectivec ), Pascal ( pascal ), Perl ( perl ), Powershell ( powershell ), Prolog ( prolog ), Regular Expression ( regex ), Ruby ( ruby ), Scala ( scala ), SQL ( sql ), Yaml ( yaml ).","title":"TriplyDB"},{"location":"triply-db-getting-started/#introduction","text":"TriplyDB allows you to store, publish, and use linked data Knowledge Graphs. TriplyDB makes it easy to upload linked data and expose it through various APIs (SPARQL, Elasticsearch, LDF, REST). Read More","title":"Introduction"},{"location":"triply-db-getting-started/#uploading-data","text":"This section explains how to create a linked dataset in TriplyDB.","title":"Uploading Data"},{"location":"triply-db-getting-started/#creating-a-new-dataset","text":"The following steps allow a new linked datasets to be created: Log into a TriplyDB instance. Click the + button on the dataset pane that appears on the right-hand side of the home screen. This brings up the dialog for creating a new dataset. Enter a dataset name that consists of alpha-numeric characters ( A-Za-z0-9 ) and hyphens ( - ). Optionally, enter a dataset display name. This name will be used in the GUI and will be exposed in dataset metadata. Optionally, enter a dataset description. You can use rich text formatting by using Markdown. See our section about Markdown for details. Optionally, change the access level for the dataset. By default this is set to \u201cPrivate\u201d. See dataset access levels for more information. When datasets are Public (see Access Levels ), they automatically expose metadata and are automatically crawled and indexed by popular search engines (see Metadata ).","title":"Creating a new dataset"},{"location":"triply-db-getting-started/#adding-data","text":"Once the dataset is created, the \u201cAdd data\u201d view is displayed (see screenshot). In this view data can be added in three ways: File upload Select files from your local machine. It is also possible to drag-and-drop local files on the cloud icon with the upward pointing arrow. URL upload Copy/paste a URL that points to an online RDF file. Import Import a dataset that is already published in the same TriplyDB instance. The \u201cAdd data\u201d view is also available for existing datasets: Go to the \u201cGraphs\u201d view by clicking on the graph icon in the left-hand sidebar. Click the \u201cImport a new graph\u201d button.","title":"Adding data"},{"location":"triply-db-getting-started/#adding-data-file-upload","text":"The file upload feature allows you to upload RDF files from your local machine. This can be done in either of two ways: Click on the cloud icons to open a dialog that allows local RDF files from your machine to be selected. Drag-and-drop RDF files from your local machine onto the cloud icon. The following RDF serialization formats are currently supported: Serialization Format File extension N-Quads .nq N-Triples .nt RDF/XML .rdf , .owl , .owx TriG .trig Turtle .ttl , .n3 JSON-LD .jsonld , .json Up to 1,000 separate files can be uploaded in one go. It is also possible to upload compressed files and archives. When the number of files exceeds 1,000, it is more efficient to combine them in archives and upload those. This allows an arbitrary number of files to be uploaded. The following archive/compression formats are currently supported: Format File extensions gzip .gz bzip2 .bz2 tar tar XZ .xz ZIP .zip","title":"Adding data: File upload"},{"location":"triply-db-getting-started/#adding-data-by-url-upload","text":"The second option for adding data is to include it from an online URL location. This is done by entering the URL inside the \u201cAdd data from a URL\u201d text field.","title":"Adding data by URL upload"},{"location":"triply-db-getting-started/#adding-data-by-import","text":"The third option for adding data is to import from datasets that are published in the same TriplyDB instance. This is done with the \u201cAdd data from an existing dataset\u201d dropdown list (see screenshot).","title":"Adding data by import"},{"location":"triply-db-getting-started/#adding-malformed-data","text":"TriplyDB only allows valid RDF data to be added. If data is malformed, TriplyDB will show an error message that indicates which part of the RDF data is malformed (see screenshot). If such malformed data is encountered, the RDF file must first be corrected and uploaded again. TriplyDB follows the linked data standards strictly. Many triple stores allow incorrect RDF data to be added. This may seem convenient during the loading phase, but often results in errors when standards-compliant tools start using the data.","title":"Adding malformed data"},{"location":"triply-db-getting-started/#assets-binary-data","text":"Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB and can be integrated into the Knowledge Graph.","title":"Assets: binary data"},{"location":"triply-db-getting-started/#publishing-data","text":"With TriplyDB you can easily make your data available to the outside world.","title":"Publishing data"},{"location":"triply-db-getting-started/#publishing-your-dataset","text":"You can publish your dataset by setting the visibility to \u201cPublic\u201d in the dataset settings menu. Making a dataset public in TriplyDB has the following consequences: The dataset can be searched for and visited by anybody on the web. The dataset will be indexed by web search engines such as Google Dataset Search . Any services that are started for that dataset will be available to anybody on the web. This includes SPARQL , text search , and linked data Fragments .","title":"Publishing your dataset"},{"location":"triply-db-getting-started/#entering-metadata","text":"Adding metadata to your datasets is important. This makes it easier to find your dataset later and also allows search engines and social media applications to understand your dataset. Metadata is entered from the dataset settings page, which is accessed by clicking on the \u201cDataset settings\u201d (cog icon) option from the left-hand sidebar (see screenshot). The dataset homepage looks empty without metadata. Notice the cog wheel icon, which provides access to the \u201cDataset settings\u201d page. The dataset settings page allows the following metadata to be entered: The dataset description. This can consist of text with (optional) Markdown formatting. The avatar (image) of the dataset. The access level of the dataset. The topics of the dataset. The example resources for the dataset. The license of the dataset. The dataset settings page allows various forms of dataset metadata to be added. Within the TriplyDB instance your dataset is now more findable for users. Whenever a user searches on one of the topics of your dataset, or types in a word that is present in the description of your dataset, the dataset will be shown as a search result. The metadata will allow TriplyDB to give a better impression of your dataset when a user visits: The dataset home page after metadata has been entered. Search engines and social media applications can recognize the metadata that is entered for datasets in TriplyDB. The following screenshot shows the widget created by the Slack chat application upon entering a link to the dataset. Notice that the chat application understands metadata properties like title, description, and image. Widget created by the Slack chat application upon sharing a link to a dataset in TriplyDB.","title":"Entering metadata"},{"location":"triply-db-getting-started/#starting-services","text":"By default, datasets in TriplyDB can be queried through TriplyDB-js as well as through the Linked Data Fragments API. In order to allow additional query paradigms, specific services can be started from the \u201cCreate service\u201d page. This page is accessed by clicking on the \u201cServices\u201d icon in the left-hand sidebar. TriplyDB instances can be configured with different types of services. The below screenshot shows the \u201cCreate service\u201d page for a TriplyDB instance that allows SPARQL, Jena SPARQL, and Elasticsearch services to be created. Notice that three different types of services can be created. It is possible to create multiple services for one dataset.","title":"Starting services"},{"location":"triply-db-getting-started/#existing-services","text":"Existing services are displayed on service widgets (see screenshot). From these widgets, services can be created or deleted. Datasets can change whenever a graph is added, deleted or renamed. When this happens, the data in a service is out of sync with the data in the dataset and a synchronization button will appear in the service widget. By clicking the button, the service will be synchronized with the current state of the dataset.","title":"Existing services"},{"location":"triply-db-getting-started/#viewing-data","text":"TriplyDB offers several ways to explore your datasets.","title":"Viewing Data"},{"location":"triply-db-getting-started/#linked-data-browser","text":"The linked data browser offers to traverse the data by focusing on one node at the time. The node is describe using it's properties, which can be followed to other nodes in the graph. The following properties provide additional information about your linked data, enabling the LD-browser to display visualizations and provide a better user experience.","title":"Linked Data Browser"},{"location":"triply-db-getting-started/#types","text":"The predicate rdf:type allows you to specify the type or class of a resource in your linked data: By using rdf:type , you can indicate the category or classification of the resource, which can help the LD-browser understand the nature of the data and display it appropriately. In the example below , you can see that \"Iris setosa\" is the type of flowering plant due to the usage of the rdf:type property.","title":"Types"},{"location":"triply-db-getting-started/#labels","text":"Labels are typically used to provide a concise and meaningful title or display name for a resource, making it easier for users to understand the content of your linked data. These predicates allow you to provide human-readable labels or names for your resources: The property rdfs:label The property skos:prefLabel In the example below , the rdfs:label property was used to denote the label(name) of the Pokemon, resulting in the display of \"Pikachu\" above its corresponding image.","title":"Labels"},{"location":"triply-db-getting-started/#descriptions","text":"Descriptions can provide additional context or information about a resource, helping users understand its purpose, content, or significance. These predicates allow you to provide textual descriptions or comments about your resources: The property sdo:description The property rdfs:comment In the following example rdfs:comment was used to provide additional information on Iris Setosa.","title":"Descriptions"},{"location":"triply-db-getting-started/#geo","text":"These are some of the predicates used for representing geographic information in your LD-browser: The property geo:asWKT : This property allows you to specify the geometries of geographic features using the Well-Known Text (WKT) format, which can be visualized on a map in the LD-browser. The property geo:hasGeometry : This property is used to link a geographic feature with its corresponding geometry. The property geo:location : This property is used to indicate the location of a resource using geographic coordinates, such as latitude and longitude, in your linked data. In the following example geo:hasGeometry property was used to showcase a map depicting the location of Instituut voor Beeld en Geluid.","title":"Geo"},{"location":"triply-db-getting-started/#images","text":"These predicates allow you to associate images or visual representations with your resources: Class sdo:ImageObject The property foaf:depiction The property foaf:thumbnail The property foaf:img The property sdo:image The property sdo:contentUrl By using these predicates, you can provide URLs or references to images that can be displayed alongside your linked data in the LD-browser. In the example below , foaf:depiction was used to display picture of Pikachu in the LD-browser:","title":"Images"},{"location":"triply-db-getting-started/#audio","text":"These predicates allow you to associate audio content with your resources: The class sdo:AudioObject The property sdo:audio The property sdo:contentUrl You can use these predicates to provide URLs or references to audio files that can be played or streamed within the LD-browser. In the following example , sdo:audio was used to showcase audio content of the Carnival Festival within the LD-browser.","title":"Audio"},{"location":"triply-db-getting-started/#video","text":"These predicates allow you to associate video content with your resources: Class sdo:VideoObject Property sdo:video Property sdo:contentUrl You can use these predicates to provide URLs or references to video files that can be played or streamed within the LD-browser.The video formats that are included in this dataset are \".mp4\", \".webm\", \".ogg\". In the following example , sdo:contentUrl was used to showcase video content of the Kleine Piep within the LD-browser.","title":"Video"},{"location":"triply-db-getting-started/#linked-data-table","text":"The linked data Table shows a dataset at the triple level. The first three columns represent the subject, predicate, and object position of the triple. The fourth column represents the graph to which the triple belongs. The linked data Table can be used to perform simple queries by filling in the subject, predicate, object, and/or graph using the text field. Terms in the linked data Table can be dragged and dropped between columns. This allows a simple form of graph navigation. For example, an object term can be dragged to the subject column in order to show the triples in which that term appears in the subject position. Queries in the linked data Table can also be performed automatically through the Statements API and the Triple Pattern Fragments API .","title":"Linked Data Table"},{"location":"triply-db-getting-started/#sparql-ide","text":"When a dataset has a running SPARQL service, the data can be queried from the SPARQL IDE. The SPARQL IDE is an extended version of the Open Source Yasgui query editor.","title":"SPARQL IDE"},{"location":"triply-db-getting-started/#saving-a-sparql-query","text":"It is often useful to save a SPARQL query for later use. This is achieved by clicking on the save icon in the top-right corner of the SPARQL Editor. Doing so will create a Save Query .","title":"Saving a SPARQL query"},{"location":"triply-db-getting-started/#sharing-a-sparql-query","text":"It is sometimes useful to share a SPARQL query with somebody else, or to have a cURL command that can be used to run the same SPARQL query from a command line. This is achieved by clicking on the share icon in the top-right corner of the SPARQL Editor. This brings us a dialog from which the SPARQL query can be copied in the following three forms: The URL-encoded SPARQL query. This is a long URL that includes the endpoint, the query, and visualization settings. Notice that this URL can be quite long for complex queries and/or visualizations. Long URLs are not supported by some application that cut off a URL after a maximum length (often 1,024 characters). Use one of the other two options or use Saved Queries to avoid such restrictions. A short URL that redirects to the full URL-encoded SPARQL query. A cURL command that can be copy/pasted into a terminal application that supports this command. cURL is often used by programmers to test HTTP(S) requests. Saved Queries are a more modern way of sharing SPARQL queries. They do not have any of the technical limitations that occur with URL-encoded queries.","title":"Sharing a SPARQL query"},{"location":"triply-db-getting-started/#transfer-a-sparql-query","text":"The SPARQL queries could be transferred to another account or an organization. To do that, go to the setting field at the query page, choose transfer and then choose where the SPARQL query should be moved to. After the destination is set you would be redirected to the SPARQL query new page. The SPARQL query could be transferred from an account to an organization and vice versa.","title":"Transfer a SPARQL query"},{"location":"triply-db-getting-started/#copy-a-sparql-query","text":"Users can copy SPARQL queries to another account or an organization. To do that, click on three dots in the upper right corner of the query and choose the copy option. Then, choose where the SPARQL query should be moved to. After setting the destination, you will be redirected to the SPARQL query new page. The SPARQL query can be copied from an account to an organization and vice versa.","title":"Copy a SPARQL query"},{"location":"triply-db-getting-started/#elasticsearch","text":"When a dataset has a running Elasticsearch service, textual searches can be performed over the entire dataset. Text search with Elasticsearch works like a search engine and returns any node that contains your search term, or contains the search term in any of its properties. It is also possible to write a custom query using the Elasticsearch Query DSL (Domain Specific Language) .","title":"ElasticSearch"},{"location":"triply-db-getting-started/#insights","text":"The insights page has been developed to give you a succinct overview of the linked data at hand. It holds two views: the class frequency and the class hierarchy view.","title":"Insights"},{"location":"triply-db-getting-started/#class-frequency","text":"The class frequency diagram shows how often classes and properties appear in a graph. The drop-down on the top of the visualization selects the graph for which the class frequency is drawn. The visualization shows the 10 most frequent classes in the selected graph. The exact number of occurrences can be seen when hovering over the bar of a class, also showing the complete IRI/prefixed IRI. When clicking on the bar of a class the node will expand and show the 10 most frequent predicates of that class.","title":"Class frequency"},{"location":"triply-db-getting-started/#class-hierarchy","text":"The class hierarchy diagram shows the hierarchy of the dataset in three different visualizations. Each of the diagrams are created by the rdfs:subClassOf relations and the classes in the dataset. TriplyDB has three different visualization methods for the classHierarchy: Bubbles visualization Treemap visualization Sunburst visualization All three visualizations are interactive in two ways. It is possible to hover over them, which will show information about the layer the mouse is on, or to click on them, so the visualization zooms in one or more layers. For each visualization it is also possible to zoom out: Bubbles visualization: click the outside of the bubble Treemap visualization: use the breadcrumbs trail shown above the visualization Sunburst visualization: click the innermost circle of the visualization","title":"Class hierarchy"},{"location":"triply-db-getting-started/#when-does-the-class-hierarchy-show","text":"A class only appears in the class hierarchy tab if it has instances (connected to the class via rdf:type ). The class hierarchy cannot be shown if it contains a cycle, meaning that some class is (indirectly) its own subclass.","title":"When does the class hierarchy show?"},{"location":"triply-db-getting-started/#exporting-data","text":"This section explains how a user of TriplyDB can export linked data stored in the triple store.","title":"Exporting Data"},{"location":"triply-db-getting-started/#export-datasets","text":"The data stored on TriplyDB is stored in two different containers: datasets and graphs. Each triple contained in a dataset is part of exactly one graph. A graph is always a part of a dataset and a dataset can have multiple graphs. The following screenshot shows the dataset \"Pok\u00e9mon\" that contains three graphs: \"data\" and \"vocab\". The graph \"data\" contains 28.588 triples and the graph \"vocab\" 185 triples. By summing up the amount of triples contained in the two graphs the dataset \"Pok\u00e9mon\" contains 28.773 triples in total. To export the dataset users can click on the downwards facing arrow. In our example, the dataset is automatically downloaded as the file \"pokemon.trig\" and compressed with .gz. The name of the file is the name of the dataset. The used serialization format is \".trig\" because that is the standard format to store triples that are appended to graphs. It is also possible to export the whole dataset on the graphs interface. Select \"Graphs\" and \"EXPORT ALL GRAPHS\".","title":"Export Datasets"},{"location":"triply-db-getting-started/#export-graphs","text":"To export only one graph select \"Graphs\" and the arrow next to the graph that should be exported. In this case the downloaded file \"https___triplydb.com_academy_pokemon_graphs_data.trig.gz\" is named after the graph and also compressed with \"gz\".","title":"Export Graphs"},{"location":"triply-db-getting-started/#extract","text":"The process of extracting the compressed file is the same for exporting graphs and datasets. The downloaded and compressed file is automatically stored in the \"Downloads\" folder. Select the file with the ending \".gz\" and open it with a double click. This opens an application that looks similar to the following screenshot: Select the file that should be extracted. In this case select \"pokemon.trig\" and click on \"Extract\". In the following step choose the location where the extracted file should be stored.","title":"Extract"},{"location":"triply-db-getting-started/#saved-queries","text":"A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query.","title":"Saved Queries"},{"location":"triply-db-getting-started/#how-to-save-a-query","text":"There are two ways to create a saved query. You need to be logged in and have authorization rights on the dataset to use this feature When working from the SPARQL IDE Using the Saved Queries tab in a dataset Creating a saved query with the SPARQL IDE is done by writing a query/visualization and hitting the save button","title":"How to save a query"},{"location":"triply-db-getting-started/#creating-a-new-version","text":"Updating the saved query can be done by clicking a query in the Saved Queries tab and editing the query or the visualization. Hit the save button to save it as a new version.","title":"Creating a new version"},{"location":"triply-db-getting-started/#using-a-saved-query","text":"","title":"Using a saved query"},{"location":"triply-db-getting-started/#sharing-a-saved-query","text":"To share a saved query, for example in Data Stories , you can copy the link that is used when you open the query in TriplyDB. Let's say you have a query called Timelined-Cars-BETA in the dataset core under the account dbpedia and you want to use version 6. Then the following link would be used https://triplydb.com/DBpedia-association/-/queries/timeline-cars/8 If you want to always use the latest query, you can simply omit the version number like so https://triplydb.com/DBpedia-association/-/queries/timeline-cars","title":"Sharing a saved query"},{"location":"triply-db-getting-started/#downloading-a-query-result","text":"The result of a query can be downloaded via the TriplyDB interface. After downloading the query, open it in TriplyDB. e.g. https://triplydb.com/DBpedia-association/-/queries/timeline-cars/8. Choose the option Response and click on the download icon or scroll down and click on Download result . The downloaded file is automatically stored in the Downloads -folder and has the name of the query. In our example, the file is called timeline-cars.json . The downloaded file contains the query result as a json-object. TriplyDB also displays the json-object when selecting the option Response . To download the query result in CSV-format, select the option Table and click on the download icon. The downloaded file is named after the query with the suffix .csv .","title":"Downloading a query result"},{"location":"triply-db-getting-started/#download-more-than-10-000-query-results-sparql-pagination","text":"This section explains how to retrieve all results from a SPARQL query using pagination. Often SPARQL queries can return more than 10.000 results, but due to limitations the result set will only consist out of the first 10.000 results. To retrieve more than 10.000 results you can use pagination. TriplyDB supports two methods to retrieve all results from a SPARQL query. Pagination with the saved query API or Pagination with TriplyDB.js.","title":"Download more than 10 000 query results - SPARQL pagination"},{"location":"triply-db-getting-started/#pagination-with-the-saved-query-api","text":"Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. The API for saved queries is extended with two arguments that the query is able to process paginated result sets. The arguments are \u2018page\u2019 and \u2018pageSize\u2019. An example of a paginated saved SPARQL query request would look like: https://api.triplydb.com/queries/academy/pokemon-color/run?page=3&pageSize=100 The example request argument \u2018page\u2019 corresponds to the requested page. In the example request this would correspond to the third page of paginated SPARQL query, according to the \u2018pageSize\u2019. There is no maximum \u2018page\u2019 limit, as a SPARQL query could return an arbitrary number of results. When no results can be retrieved for the requested page an empty page will be returned. The argument \u2018pageSize\u2019 corresponds to how many results each page would contain. The \u2018pageSize\u2019 has a default of 100 returned results and a maximum \u2018pageSize\u2019 limit of 10.000 returned results. The request will return an error when the \u2018pageSize\u2019 is set higher than 10.000. he RESTful API for the saved SPARQL queries follows the RFC 8288 standard. The request will return an response body containing the result set and a response header. The response header contains a link header with the relative \"next\" request, the relative \"prev\" request, and the relative \"first\" request. By following the \"next\" link header request you can chain the pagination and retrieve all results. link: <https://api.triplydb.com/queries/academy/pokemon-color/run?page=4&pageSize=100>; rel=\"next\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=2&pageSize=100>; rel=\"prev\", <https://api.triplydb.com/queries/academy/pokemon-color/run?page=1&pageSize=100>; rel=\"first\"","title":"Pagination with the saved query API"},{"location":"triply-db-getting-started/#pagination-with-triplydbjs","text":"TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows the user to connect to a TriplyDB instance via the TypeScript language. TriplyDB.js has the advantage that it can handle pagination internally so it can reliably retrieve a large number of results. To get the output for a construct or select query, follow these steps: Import the TriplyDB library and set your parameters, regarding the TriplyDB instance and the account in which you have saved the query as well as the name of the query. Do not forget that we perform TriplyDB.js requests within an async context . ts import Client from '@triply/triplydb' async function run() { // Your code goes here. const client = Client.get({token: process.env.TRIPLYDB_TOKEN}) const account = await client.getAccount('account-name') const query = await account.getQuery('name-of-some-query') } run() Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().statements() For select queries you use the bindings() call: ts const query = await account.getQuery('name-of-some-query') const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: ts // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() To iterate the results of your SPARQL query you have three options: a. Iterate through the results per row in a for -loop: ts // Iterating over the results. for await (const row of results) { // execute something } Note: For select queries the for -loop iterates over the rows of the result set. For construct queries the for -loop iterates over the statements in the result set. b. Save the results to a file. This is only supported for SPARQL construct queries: ts // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') c. Load all results into memory in the form of an Array. Note that this is almost never used. If you want to process results, then use the 3a option; if you want to persist results, then option 3b suits better. ts // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"Pagination with TriplyDB.js"},{"location":"triply-db-getting-started/#using-a-saved-query-as-rest-api-advanced","text":"Each TriplyDB instance has a fully RESTful API. The TriplyDB RESTful API is extended for saved SPARQL queries. A saved query can be used as a RESTful API to retrieve data from your linked dataset. The URL next to the keywork API is the RESTful API URL and can be used with RESTful API libraries. You can copy the RESTful API by pressing the copy button just behind the URL. Pressing the copy button from the above query will result in the following run url: https://api.triplydb.com/queries/DBpedia-association/timeline-cars/run When you copy this URL in your browser or fetch the URL with curl, you will get a get request to a RESTful API and get a JSON representation of the data in your browser or command window.","title":"Using a saved query as REST-API (Advanced)"},{"location":"triply-db-getting-started/#using-a-saved-query-in-python-or-r-notebooks-advanced","text":"SPARQL queries as a RESTful API, also means you can transport your data to your Python script, R script or Jupyter notebook. To use the result set from your SPARQL query you need to connect your script to the saved SPARQL query. To do this you will need to write a small connector. To help you out TriplyDB has added a code snippet generator for Python and R. This snippet contains the code to retrieve the data from the SPARQL query into your script or notebook. You can open the code snippet generator by clicking on the '' button on the right side of the screen. Clicking the '' button opens the code snippet screen. Here you select the snippet in the language you want to have, either Python or R. You can then copy the snippet, by clicking the 'copy to clipboard' button or selecting the snippet and pressing ctrl-c . Now you can paste the code in the location you want to use the data. The data is stored in the data variable in JSON format. When the SPARQL query is not public, but instead either private or internal, you will need to add an authorization header to the get request. Without the authorization header the request will return an incorrect response. Checkout Creating your API token about creating your API-token for the authorization header. Check out the SPARQL pagination page when you want to query a SPARQL query that holds more than 10.000 results. The SPARQL pagination page will explain how you can retrieve the complete set.","title":"Using a saved query in Python or R notebooks (Advanced)"},{"location":"triply-db-getting-started/#query-metadata-query-metadata","text":"Every Saved Query has a metadata section. This metadata section includes the following two links: - A link to the dataset over which the query is executed. Clicking this links navigates to the dataset homepage. - A link to the service by which the query is executed. Clicking this link navigates to the services page that includes that service. Users can specify a query title and description, both of which are included as metadata. The access level and version of the query are also exposed as metadata. See the following screenshot for how the metadata fields are shown in TriplyDB: Users can specify additional metadata inside the query string, by using the GRLC annotation format. GRLC annotations start with the hash and plus sign characters ( #+ ). Visit the GRLC project to learn more about this format. For example, the following GRLC annotation could indicate to a software application that the query should be repeated every hour: #+ frequency: hourly See the Triply API documentation for how to retrieve query metadata, including how to retrieve GRLC annotations.","title":"Query metadata {#query-metadata}"},{"location":"triply-db-getting-started/#data-stories-data-stories","text":"A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results.","title":"Data stories {#data-stories}"},{"location":"triply-db-getting-started/#creating-a-data-story","text":"You can create your own data story via the stories tab on TriplyDB. If this is your first time creating a data story, your view will look something like the image below. If you already are a proud owner of a data story, you will find it here. To create a new one, you can click the orange \"Create story\" button and you\u2019ll see the same form. In this form, you can fill in the title and set the access level for a data story. When everything is set, press the \"Create story\" button.This will take you to a new page where you can customize the data story.","title":"Creating a data story"},{"location":"triply-db-getting-started/#editing-a-data-story","text":"As shown in the image below, in the top right corner of the page, there is a menu button. Here you will find the following: - Story settings : Here you can change the title and the access level of your story. - Change banner : Here you can change the banner, just choose an image that you want as your banner (wide images work best). - Transfer : To transfer the story to a different user or organization. - Copy : To copy the story to a different user or organization. - Delete : To delete the story. In the right lower corner you see a button with a notepad. With this button, you can toggle between the edit view, which allows you to edit the story, and the reader view, which is how readers of your story will perceive this page.","title":"Editing a data story"},{"location":"triply-db-getting-started/#adding-elements","text":"To create your first element press \"+ Add new element\". This will open a new form as shown in the images below. Here you can select what kind of element you want to add to your data story; you\u2019ll have the option to write text, to select an already existing SPARQL query, or even to create a new SPARQL query.","title":"Adding elements"},{"location":"triply-db-getting-started/#existing-query","text":"Let\u2019s start by selecting a query for our data story. Maybe you have already created one, but if you haven\u2019t, you can select one of the queries available to you. You can search in the Query search bar and select the one you want, for example \"our-first-select-query\". Optionally you can select the version of the query and set the caption. When everything is set, press \"Create story element\". And look, we have added our first element to our story!","title":"Existing query"},{"location":"triply-db-getting-started/#paragraph","text":"Data sometimes needs accompanying text to be completely understandable. TriplyDB not only supports writing plain text, but TriplyDB paragraphs are also markdown compliant. The markdown that you\u2019ll add in the paragraph text box will be rendered as HTML and can be previewed. TriplyDB also supports images, and even code blocks with highlighting for the most common linked data and programming languages.","title":"Paragraph"},{"location":"triply-db-getting-started/#sharing-and-embedding","text":"Before you know it, you will have created your first data story. Congratulations! Now it is time to share it with the world, but don\u2019t forget to set the access level to \u201cpublic\u201d. Then you have two options: 1. You can simply share the URL in TriplyDB. 2. You can embed the Data Story on your own webpage. Scroll all the way to the end of your Data Story and click the \u201c Embed\u201d button. This brings up a code snippet that you can copy/paste into your own HTML web page.","title":"Sharing and embedding"},{"location":"triply-db-getting-started/#admin-settings-pages","text":"You can use the console to perform administrator tasks. The administrator tasks are performed within the admin settings page. The admin settings pages are accessible by clicking on the user menu in the top-right corner and selecting the \u201cAdmin settings\u201d menu item. You must have administrator privileges to access these pages and perform administrator tasks.","title":"Admin settings Pages"},{"location":"triply-db-getting-started/#overview-page","text":"The first page that comes into view when opening the admin settings pages is the overview page. This page contains an overview of all the important statistics of the instance. The page also shows how close the instance is to hitting one or more limits. If no limit is set, the statistics are shown as a counter. If a limit is set a gauge is shown with a green, orange or red bar. The colors denote how far that statistic of the instance is to the limit. Green means not close to the limit, Orange means close to the limit, Red means over the limit.","title":"Overview page"},{"location":"triply-db-getting-started/#general-overview","text":"The general overview gives an insight into the software version of the instance. Each instance consists of a console and an API. The console is the web interface of the instance and has a build date corresponding to the build date of the docker image of the console and a version number corresponding to the version of the docker image. The API is the layer between the console and the data. The API is separate from the console and is a different docker image. The API also has a version and build date of the docker image. Also contains a starting time, and an updated time, the moments when the docker image is started for this instance or when the docker image is updated for the instance.","title":"General overview"},{"location":"triply-db-getting-started/#accounts-overview","text":"The accounts overview shows how many organizations and users are in this instance. The organizations and users are shown in a counter if no limit is set. If a limit is set on the number of organizations and/or users of the instance a gauge is shown.","title":"Accounts overview"},{"location":"triply-db-getting-started/#data-overview","text":"The data overview shows multiple statistics about datasets. The first counter shows the amount of datasets on the instance. The second and third counters show the amount of graphs and statements in all graphs. The fourth and fifth counters show the amount of unique graphs and statements. When a graph is copied from one dataset to another, the data in that graph does not change. The amount of unique data does not change either. The amount of unique data is a more representative way of calculating the amount of data in the instance. All statistics are shown in a counter, if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown.","title":"Data overview"},{"location":"triply-db-getting-started/#services-overview","text":"The data overview shows how multiple statistics about services. The first counter shows the total amount of services on the instance, The second counter shows the total amount of statements in all the services. Then for each of our service types a specific counter is created. Each containing the amount of services and the amount of statements in that service. All statistics are shown in a counter if no limit is set. If a limit is set on one of the statistics of the instance a gauge is shown.","title":"Services overview"},{"location":"triply-db-getting-started/#settings-page","text":"The settings page is the main page for administrators to institute instance wide changes. An administrator can change the site logo's here, change the contact email or update site wide prefixes.","title":"Settings page"},{"location":"triply-db-getting-started/#set-logos-and-banner","text":"For changing the logos and the banner follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. Under \"Site logos\" and \"Site banner\" you can upload a site logo (square and landscape) or a banner. Make sure you use SVG files with a maximum size of 5 MB.","title":"Set logos and banner"},{"location":"triply-db-getting-started/#setting-metadata","text":"For changing the metadata follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site metadata\", it looks as follows: Here you can set the name, tag line, description and welcome text. The name of your website appears in your browser tab. The welcome text appears on the homepage of your TriplyDB instance. The tagline and description are for metadata purposes (e.g. findability and website previews).","title":"Setting metadata"},{"location":"triply-db-getting-started/#setting-contact-email","text":"For changing the contact email follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Contact Email\". Here, you can change the contact email to a new contact email for the instance.","title":"Setting contact email"},{"location":"triply-db-getting-started/#setting-example-datasets","text":"Example datasets are introduction datasets on the frontpage of your instance. The Example datasets are datasets that are interesting for people that visit your page to see and interact with. Most often you'll use open datasets to show them off on the frontpage. You can also use internal or private datasets, but these will only be visible if the person seeing them has the right access rights. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page, navigate to \"Example datasets\". Here, you can execute the following changes: You can move datasets up and down in the order by clicking and holding your left mouse button over the three horizontal lines in front of the dataset name. You can then drag the selected dataset to their new spot. In the search field below the already added datasets you can add a new example dataset by typing in the search field and selecting the correct dataset. You can remove datasets by pressing the x on the right side of the dataset name to remove it from the example dataset list.","title":"Setting example datasets"},{"location":"triply-db-getting-started/#setting-starter-dataset","text":"The starter dataset is a beginner-friendly linked dataset that can be an introduction into linked data when a user creates an account for the first time. The starter dataset is visible for a user when the user has not yet created a dataset on its own account. For editing the example datasets follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Starter dataset\". Here you can change the starter dataset to a new starter dataset for the instance by typing in the search bar a name of an existing dataset to replace the started dataset. This dataset then will be presented to users on their account page, with an option to import(copy) them immediately. This needs to be a public dataset! If it's not public, new users will have to create a dataset. The starter dataset is only shown if the user currently has no datasets.","title":"Setting Starter dataset"},{"location":"triply-db-getting-started/#setting-authentication","text":"One of the roles of an administrator is to make sure only the right people will sign up for the TriplyDB instance. To do this, an administrator can set up authentication protocols. The authentication protocols can block people from signing up to instances where they are not allowed to sign up to. For changing the authentication protocols follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Authentication\". Now you can change the password sign up. Allowing people to only register with a password or they are only allowed to register with a google or Github account. When password signup is enabled, the administrator can also set the permitted signup domains. Only users with e-mail addresses that match these domains are allowed to sign-up. Wildcards are allowed and domains are comma separated, for example: mydomain.com,*.mydomain.com.","title":"Setting Authentication"},{"location":"triply-db-getting-started/#setting-site-wide-prefixes","text":"One of the advantages of using TriplyDB is that you can set site-wide prefixes once and use them everywhere on the instance. Site-wide prefixes are prefixes defined in the admin settings and can be used for all datasets that contain the IRIs matching the prefixes. For editing the side-wide prefixes follow the next steps: 1. Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \"Settings\" tab. This brings up an overview of all the settings an admin can set for the TriplyDB instance. On that page navigate to \"Site-wide prefixes\". Here, you can execute the following changes: Each field of the already added site-wide prefixes you can edit. You can edit the prefix label by typing in the first field. You can edit the prefix IRI and in the second field. Pressing UPDATE PREFIXES updates the list. In the last field below the already added site-wide prefixes you can add a new site-wide prefix by typing in the first field the prefix label, and in the second field the prefix IRI. Pressing UPDATE PREFIXES updates the list. You can remove prefixes by pressing the x on the right side of the prefixes name to remove it from the site-wide prefixes list.","title":"Setting Site-wide prefixes"},{"location":"triply-db-getting-started/#account-overview-page","text":"The account page governs all the accounts of an instance. The paginated table shows all the accounts of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific accounts according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all accounts automatically on the created at date with the latest created at date accounts first. The filters on top of the table can be used to filter the following columns: Name The name of the account, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the account. Type Type of the account, this can either be 'Organization' or 'User'. In the filter you can select a specific account type or 'All' account types. Display name The display name of the account, often an account has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Email The email address of the account. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Created at How long ago an account was created. When you hover over the text you can see the precise moment an account is created. You can order accounts based on the moment of creation. Updated at How long ago an account has been updated with new metadata such as display name or password. When you hover over the text you can see the precise moment an account is last updated. You can order accounts based on the moment of updated at time. Last activity How long ago the account has been last active. When you hover over the text you can see the precise moment an account was last active. You can order the accounts based on the moment of last time the account was active. Role Role of the account, this can either be 'light', 'regular' or 'administrator'. In the filter you can select a specific role or 'All' roles. Verified An account can be verified or not, to verify an account, the user needs to click on the verify button in the email. Or an administrator has verified the account in the account settings of that account. Only 'users' need to be verified. Disabled An account can be disabled or not, to disabled an account, the user needs to click on the disabled button in their user settings. Or an administrator has disabled the account in the account settings of that account. legal consent An account can have accepted the legal consent or not, to accept legal consent, the user needs to click on the accept legal consent either when creating an account or by checking it in the user settings. Only 'users' need to have accepted legal consent. For each account you can execute the following actions: Open account settings For each account, there is a button such that the administrator can directly go to the account settings of the user or organization. The account settings are behind the `cogwheel` button.","title":"Account overview page"},{"location":"triply-db-getting-started/#add-new-users","text":"Go to the \u201cAccounts tab\u201d to receive an overview of all accounts on the TriplyDB instance. The type of account can be observed based on the following icons: Icon Account type organization user","title":"Add new user(s)"},{"location":"triply-db-getting-started/#create-a-new-user","text":"New users can only be created by administrators by performing the following steps: Click on the \u201cAdmin settings\u201d link in the user menu (top-right corner) and click the \u201cAccounts\u201d tab. This brings up an overview of all users and organizations on the TriplyDB instance. Click the \u201cAdd user\u201d button. Fill in the user name and email address of the prospective user. The user name must consist of alphanumeric characters ( A-Za-z ) and hyphens ( - ). Click the \u201cAdd user\u201d button. This sends an account creation email to the prospective user, containing a link that allows them to log in. In addition to the above default procedure, the following two options are provided for user account creation: Temporary account : By default, user accounts do not expire. Sometimes it is useful to create a temporary account by specifying a concrete date in the \u201cAccount expiration date\u201d widget. Preset password : By default, a user can set her password after logging in for the first time by clicking on the link in the account creation email. When a password is entered in the \u201cPassword\u201d field, the user must enter this password in order to log in for the first time.","title":"Create a new user"},{"location":"triply-db-getting-started/#datasets-page","text":"The account page governs all the datasets of an instance. The paginated table shows all the datasets of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific datasets according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all datasets automatically on the created at date with the latest created at date datasets first. The filters on top of the table can be used to filter the following columns: Name The name of the dataset, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the dataset. Access level Access level of the dataset, this can either be 'Public', 'Internal' or 'Private'. In the filter you can select a specific access level or 'All' access levels. Display name The display name of the dataset, often a dataset has both a name and a display name. The display name is not limited to a specific set of characters, as it is not used as an URL. You can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. Owner The owner of the dataset. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Graph count The amount of graphs in a dataset. These are all the total amount of graphs in a dataset, and can be filtered with the slider. Statement count The amount of statements in a dataset. These are all the statements of all the graphs, and can be filtered with the slider. Service count The amount of services in a dataset. These can be filtered with the slider. Asset count The amount of assets in a dataset. These can be filtered with the slider. Created at How long ago a dataset has been created. When you hover over the text you can see the precise moment a dataset is created. You can order datasets based on the moment of creation. Updated at How long ago a dataset has been updated with new metadata such as display name or new data. When you hover over the text you can see the precise moment an account is last updated. You can order dataset based on the moment of updated at time. Last graph edit How long ago the last graph has been edited, either new data is uploaded or removed, or the graph names changed. When you hover over the text you can see the precise moment a dataset was edited. You can order the accounts based on the moment of last time the dataset was last edited. For each dataset you can execute the following actions: Open dataset settings For each dataset there is button such that the administrator can directly go to the dataset settings of the dataset. The dataset settings are behind the `cogwheel` button.","title":"Datasets page"},{"location":"triply-db-getting-started/#services-page","text":"The services page governs all the services of an instance. The paginated table shows all the services of the instance. The table is equipped with filters and sorting mechanisms to quickly search for and locate specific services according to search and filter criteria. The table also has a search field at the right side to quickly do wildcard searching. The table sorts all services automatically if a service is in an error state or not. All services that are in error state will be shown at the top of the table. This way immediate action can be taken to check the service. The filters on top of the table can be used to filter the following columns: Name The name of the SPARQL service, you can order the table based on the alphanumeric ordering, and filter based on the sequence of characters appearing in the filter. The name is also a URL that brings you to the location of the service. Type Type of the service, this can either be 'Virtuoso', 'Jena', 'Blazegraph', 'Prolog' or 'Elasticsearch'. In the filter you can select a specific service type or 'All' service types. Status The status of the service, can be 'Starting', 'Running', 'Stopped', 'Updating' or 'Error'. In the filter you can select a specific service status or 'All' services statuses Statements The amount of statements in a service. These are all the loaded statements in the service, and can be filtered with the slider. Loaded graphs Amount of graphs loaded in the service. All the statements of all the graphs together will count up to the total amount of statements. Dataset The dataset the service belongs to. The dataset is clickable and brings you to the dataset page. The datasets can be filtered based on the sequence of characters appearing in the filter. Owner The owner of the dataset is also the owner of the service. The owner is a URL and brings you to the overview page of the owner. The owners can be filtered based on the sequence of characters appearing in the filter. Created How long ago a service has been created. When you hover over the text you can see the precise moment a service is created. You can order the services based on the moment of creation. Last queried How long ago the service has been last queried. When you hover over the text you can see the precise moment a service is last queried. You can order the services based on the moment of last time the service has been queried. Auto stops Some services are equipped with an auto stop feature. This feature reduces the memory resources when a service is not queried in a while. The column `Auto stops` shows how long it will take before a service is auto-stopped. You can order the services on when the auto-stop feature kicks in. Each time a service is used the timer is reset. Version A service always has a particular version. A service is not automatically updated as it could be that the service has possible down time. The owner of the service can update a service when they deem it necessary to update to the latest version. For each service you can execute the following actions: Update the service When a service can be updated an orange arrow will appear just below the service. When you press the update service button the service is automatically updated to the latest service version. Open additional information For each service there is additional information available. The additional information is behind the `i` button. The additional information contains information about the graphs in the dataset and a raw information view of the service metadata. Inspect the logs For each service there is a log available. The logs are behind the `text` button. The logs contain information Synchronize the service The service can be outdated. This happens when the data in the dataset does not corresponds with the data in the service. When this happens the service can be synchronized from here to make it up to date with the latest version of the data. Remove the service When a service is no longer necessary or there needs to be made some space on the instance a service can be removed from here. Some of these actions can be cumbersome when you need to do them one at a time. To help with this, on the left side of the table you can click on the tickbox. This will select all the services that match search criteria if there search criteria and all tables when there are no search criteria. When pressed you can now remove all selected services or update all selected services to a new software version.","title":"Services page"},{"location":"triply-db-getting-started/#redirects-page","text":"The great thing about linked data is that IRIs are used to define objects in linked data. Then when you visit the IRIs you find useful information about the object. But sometimes the data is not on the location where the IRI is pointing towards. You have the IRI: https://example.org/resource/Amsterdam but the information about the object is located in the dataset https://api.triplydb.com/MyAccount/myCities. This is a problem as the IRI is pointing to a location that does not contain the data, and the data is at a location that is not found without the correct IRI. This is where you can use redirects to redirect the user from the IRI to the location where the data is found.","title":"Redirects page"},{"location":"triply-db-getting-started/#how-to-setup-a-redirects-for-dereferencing","text":"Redirects enable easy dereferencing of resources. For example, you can dereference a resource https://example.org/resource/Amsterdam into dataset https://api.triplydb.com/MyAccount/myCities by following these steps: First update the web server of where the IRI is originally pointing towards the redirect API. In this example all subpaths of /resource are to be redirected from https://example.org to https://api.triplydb.com/redirect/$requestUri. this means that when a request for https://example.org/resource/Amsterdam comes to the web server of https://example.org it will be redirected to https://api.triplydb.com/redirect/https://example.org/resource/Amsterdam. Now that the external web server is set up to redirect to TriplyDB, TriplyDB needs to be configured to accept the request and redirect it to the correct dataset. This is done by adding a rule on the administrator redirects page. To add a rule, press the ADD RULE button to begin with the creation of a new rule. For this example we want to add a prefix rule with the pattern to match https://example.org/resource/City/. The prefix rule needs a dataset to redirect to. This will be the dataset https://api.triplydb.com/myAccount/myCities. Press CREATE RULE to create the rule. Each rule is evaluated when a request comes in https://api.triplydb.com/redirect/$requestUri and mapping rules are evaluated from top (highest priority) to bottom (lowest priority). When a match is found the requestUri is then redirected to that location. TriplyDB supports two types of mapping rules: Prefix Prefix rules trigger when the start of a resource matches the specified string. Regex Regular Expression rules trigger when a resource matches a Regular Expression.","title":"How to setup a redirects for dereferencing"},{"location":"triply-db-getting-started/#reference","text":"","title":"Reference"},{"location":"triply-db-getting-started/#access-levels","text":"TriplyDB uses the following access levels for datasets, queries, and stories. Access level Description Icon Private The dataset/query/story is only visible to you. Internal The dataset/query/story is only visible to people who are logged in to the same TriplyDB. Public The dataset/query/story is visible to everybody on the Internet.","title":"Access levels"},{"location":"triply-db-getting-started/#access-level-dependencies","text":"The access levels for datasets, queries, and stories may affect each other. For example, if a public query references a private dataset, other users will be able to view the query string, but none of the query results. TriplyDB always uses the most conservative access level in such cases, ensuring that information is never exposed unintentionally.","title":"Access level dependencies"},{"location":"triply-db-getting-started/#access-levels-and-workflows","text":"These access levels are often used for the following workflow: You create a new dataset/query/story starts with access level \u2018Private\u2019. As the dataset/query/story progresses, give it access level \u2018Internal\u2019 to receive feedback from other users. Once the dataset/query/story is ready, give it access level \u2018Public\u2019 to publish it to the world.","title":"Access levels and workflows"},{"location":"triply-db-getting-started/#markdown-support","text":"Triply allows rich text formatting to be used in the following places: Dataset description Account description Saved Query description Data Story elements Site welcome message The following Markdown elements are supported:","title":"Markdown support"},{"location":"triply-db-getting-started/#headings","text":"Headings are used to divide a text into different sections. The hash character ( # ) at the beginning of a line indicates a heading is used. Multiple hash characters indicate nested headings. # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6","title":"Headings"},{"location":"triply-db-getting-started/#text-styling","text":"Style Syntax Output Bold **bold** bold Italic _italic_ italic Strikethrough ~~strikethrough~~ ~~strikethrough~~","title":"Text styling"},{"location":"triply-db-getting-started/#hyperlinks","text":"Style Syntax Output Raw URL <https://triply.cc> https://triply.cc Labeled URL [label](https://triply.cc) label Notice that URLs can also be relative. This allows you to refer to other datasets, saved queries, etc. by using relative paths.","title":"Hyperlinks"},{"location":"triply-db-getting-started/#code","text":"There are options for formatting in-line code as well as multi-line code blocks.","title":"Code"},{"location":"triply-db-getting-started/#in-line-code","text":"Code can also be used in-line with single backticks: Use `code` inside a sentence.","title":"In-line code"},{"location":"triply-db-getting-started/#multi-line-code-blocks","text":"Multi-line code blocks start and end with three consecutive backticks. The following Markdown denotes two lines of Turtle: select * { graph ?g { ?s ?p ?o. } } The above is rendered as follows: select * { graph ?g { ?s ?p ?o. } }","title":"Multi-line code blocks"},{"location":"triply-db-getting-started/#code-language","text":"The opening backticks are optionally following by the name of the code language. The following code languages are supported: Language Syntax SPARQL sparql Turtle ttl TypeScript typescript R r Python python The other supported languages are: Bash ( bash ), C ( c ), C++ ( cpp ), C# ( csharp ), Extended Backus-Naur Form ( ebnf ), Go ( go ), Haskell ( haskell ), Java ( java ), JavaScript ( javascript ), LaTeX ( latex ), Makefile ( makefile ), Markdown ( markdown ), Objective C ( objectivec ), Pascal ( pascal ), Perl ( perl ), Powershell ( powershell ), Prolog ( prolog ), Regular Expression ( regex ), Ruby ( ruby ), Scala ( scala ), SQL ( sql ), Yaml ( yaml ).","title":"Code language"},{"location":"triply-etl/","text":"TriplyETL Overview TriplyETL allows you to create and maintain production-grade linked data pipelines. Getting Started explains how you can use TriplyETL for the first time. CLI explains the commands that you can use to manually create, run, and maintain ETL pipelines. Changelog documents the changes introduced by each TriplyETL version. Maintenance explains how you can perform updates and configure pipeline automation. TriplyETL uses the following unique approach: flowchart LR source -- \"1. Extract\" --> record record -- \"2. Transform\" --> record record -- \"3. Assert\" --> ld ld -- \"4. Enrich\" --> ld ld -- \"5. Validate\" --> ld ld -- \"6. Publish\" --> tdb source[Data Sources] record[Record] ld[Internal Store] tdb[(Triple Store)] This approach consists of the following steps (see diagram): Step 1 Extract extracts data records from one or more data sources. Step 2 Transform cleans, combines, and extends data in the Record representation. Step 3 Assert uses data from the Record to generate linked data assertions. Step 4 Enrich improves or extends linked data in the Internal Store. Step 5 Validate ensures that linked data in the Internal Store is correct. Step 6 Publish makes linked data available in a Triple Store for others to use. In addition, the following things are used throughout the 6 TriplyETL steps: Declarations introduce constants that you can reuse throughout your TriplyETL configuration. Control structures make parts of the TriplyETL configuration optional or repeating (loops). Debug functions allow you to gain insight in TriplyETL internals for the purpose of finding issues and performing maintenance. TriplyETL uses the following data environments (see diagram): The Data Sources are used as the input to the pipeline. The Record provides a uniform format for data from any source system. The Internal Store holds linked data that is generated inside the pipeline. The Triple Store is where the results of the pipeline are stored. Why TriplyETL? TriplyETL has the following core features, that set it apart from other data pipeline products: Backend-agnostic : TriplyETL supports a large number of data source formats and types. Source data is processed in a unified record. This decouples configuration from source format specific. In TriplyETL, changing the source system often only requires changing the extractor. Multi-paradigm : TriplyETL supports all major paradigms for transforming and asserting linked data: SPARQL, SHACL, RML (TBA), JSON-LD (TBA), and RDF All The Things (RATT). You can also write your own transformations in TypeScript for optimal extensibility. Scalable : TriplyETL processes data in a stream of self-contained records. This allows TriplyETL pipelines to run in parallel, ensuring a high pipeline throughput. Standards-compliant : TriplyETL implements the latest versions of the linked data standards and best practices: RDF 1.1, SHACL Core, SHACL Advanced, XML Schema Datatypes 1.1, IETF RFC3987 (IRIs), IETF RFC5646 (Language Tags), SPARQL 1.1 Query Languahge, SPARQL 1.1 Update, SPARQL 1.1 Federation, N-Triples 1.1, N-Quads 1.1, Turtle 1.1, TriG 1.1, RDF/XML 1.1, JSON-LD 1.1 (TBA), JSON-LD Framing (TBA), and JSON-LD Algorithms (TBA). High Quality : The output of TriplyETL pipelines is automatically validated against the specified data model, and/or against a set of preconfigured 'gold records'. Production-grade : TriplyETL pipelines run in GitLab CI/CD, and support the four DTAP environments that are often used in production systems: Development, Testing, Acceptance, Production.","title":"TriplyETL Overview"},{"location":"triply-etl/#triplyetl-overview","text":"TriplyETL allows you to create and maintain production-grade linked data pipelines. Getting Started explains how you can use TriplyETL for the first time. CLI explains the commands that you can use to manually create, run, and maintain ETL pipelines. Changelog documents the changes introduced by each TriplyETL version. Maintenance explains how you can perform updates and configure pipeline automation. TriplyETL uses the following unique approach: flowchart LR source -- \"1. Extract\" --> record record -- \"2. Transform\" --> record record -- \"3. Assert\" --> ld ld -- \"4. Enrich\" --> ld ld -- \"5. Validate\" --> ld ld -- \"6. Publish\" --> tdb source[Data Sources] record[Record] ld[Internal Store] tdb[(Triple Store)] This approach consists of the following steps (see diagram): Step 1 Extract extracts data records from one or more data sources. Step 2 Transform cleans, combines, and extends data in the Record representation. Step 3 Assert uses data from the Record to generate linked data assertions. Step 4 Enrich improves or extends linked data in the Internal Store. Step 5 Validate ensures that linked data in the Internal Store is correct. Step 6 Publish makes linked data available in a Triple Store for others to use. In addition, the following things are used throughout the 6 TriplyETL steps: Declarations introduce constants that you can reuse throughout your TriplyETL configuration. Control structures make parts of the TriplyETL configuration optional or repeating (loops). Debug functions allow you to gain insight in TriplyETL internals for the purpose of finding issues and performing maintenance. TriplyETL uses the following data environments (see diagram): The Data Sources are used as the input to the pipeline. The Record provides a uniform format for data from any source system. The Internal Store holds linked data that is generated inside the pipeline. The Triple Store is where the results of the pipeline are stored.","title":"TriplyETL Overview"},{"location":"triply-etl/#why-triplyetl","text":"TriplyETL has the following core features, that set it apart from other data pipeline products: Backend-agnostic : TriplyETL supports a large number of data source formats and types. Source data is processed in a unified record. This decouples configuration from source format specific. In TriplyETL, changing the source system often only requires changing the extractor. Multi-paradigm : TriplyETL supports all major paradigms for transforming and asserting linked data: SPARQL, SHACL, RML (TBA), JSON-LD (TBA), and RDF All The Things (RATT). You can also write your own transformations in TypeScript for optimal extensibility. Scalable : TriplyETL processes data in a stream of self-contained records. This allows TriplyETL pipelines to run in parallel, ensuring a high pipeline throughput. Standards-compliant : TriplyETL implements the latest versions of the linked data standards and best practices: RDF 1.1, SHACL Core, SHACL Advanced, XML Schema Datatypes 1.1, IETF RFC3987 (IRIs), IETF RFC5646 (Language Tags), SPARQL 1.1 Query Languahge, SPARQL 1.1 Update, SPARQL 1.1 Federation, N-Triples 1.1, N-Quads 1.1, Turtle 1.1, TriG 1.1, RDF/XML 1.1, JSON-LD 1.1 (TBA), JSON-LD Framing (TBA), and JSON-LD Algorithms (TBA). High Quality : The output of TriplyETL pipelines is automatically validated against the specified data model, and/or against a set of preconfigured 'gold records'. Production-grade : TriplyETL pipelines run in GitLab CI/CD, and support the four DTAP environments that are often used in production systems: Development, Testing, Acceptance, Production.","title":"Why TriplyETL?"},{"location":"triply-etl/assert/","text":"The Assert step uses data from the Record to add linked data to the Internal Store. graph LR source -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> tdb linkStyle 2 stroke:red,stroke-width:3px; ld[Internal Store] record[Record] source[Data Sources] tdb[(Triple Store)] Assertion are statements of fact. In linked data, assertions are commonly called 'triples' or 'quads'. A triple is composed of three parts: a subject term, a predicate term, and an object term. A quad or quadruple also has a fourth graph term. TriplyETL supports the following assertion approaches: 3A. RATT assertions are a core set of TypeScript functions that assert linked data. Next steps After linked data has been asserted into the Internal Store, the following steps can be performend: 4. Enrich improves or extends linked data in the Internal Store. 5. Validate ensures that linked data in the Internal Store is correct. 6. Publish makes linked data available in a Triple Store for others to use.","title":"3. TriplyETL: Assert"},{"location":"triply-etl/assert/#next-steps","text":"After linked data has been asserted into the Internal Store, the following steps can be performend: 4. Enrich improves or extends linked data in the Internal Store. 5. Validate ensures that linked data in the Internal Store is correct. 6. Publish makes linked data available in a Triple Store for others to use.","title":"Next steps"},{"location":"triply-etl/assert/json-ld/","text":"The JSON-LD standard includes the following algorithms that allow linked data to be added to the internal store: The Expansion algorithm allows a JSON-LD context to be applied to the record. The Deserialization algorithm allows linked data to be generated based on the expanded record. THIS DOCUMENTATION WILL BE MADE AVAILABLE SOON.","title":"3. Assert: JSON-LD"},{"location":"triply-etl/assert/ratt/","text":"RATT assertions are a core set of TypeScript functions that assert linked data. Overview The following assertion functions are available: Assertion Description iri() Create an IRI term. iris() Creates multiple IRI terms. literal() Creates a literal term. literals() Creates multiple literal terms. nestedPairs() Creates a nested node with multiple triples that use that node as their subject term. objects() Asserts multiple triples that share the same subject and predicate terms. pairs() Asserts multiple triples that share the same subject term. quad() Asserts a quadruple. quads() Asserts multiple quadruples. str() Creates a static string. triple() Asserts a triple. triples() Asserts multiple triples. All RATT assertions can be imported from the RATT library in TriplyETL: import { iri, iris, literal, literals, nestedPairs, objects, pairs, quad, quads, triple, triples } from '@triplyetl/etl/ratt' Function iri() {#iri} Asserts an IRI term based on a key and an optional IRI prefix: iri(prefix:PrefixedToIri, content:Key|StaticString) iri(content:Key|StaticString) Parameters prefix A prefix that is declared with declarePrefix() . content Either a key that contains a string value, or a static string. If the prefix is used, this content is placed after the prefix (sometimes referred to a the 'local name'). If the prefix parameter is not used, the content must specify the full IRI. Examples The following asserts an IRI based on a declared prefix ( prefix.ex ) and a key ( name ): triple(iri(prefix.ex, 'name'), a, owl.NamedIndividual), The following asserts an IRI based on a declared prefix ( prefix.ex ) and a static string (see function str() ): triple(iri(prefix.ex, str('bob')), a, owl.NamedIndividual), The following asserts an IRI based on the value stored in key 'url' . Notice that this value must encode a full absolute IRI. fromJson([{ 'url': 'https://example.com/bob' }]), triple(iri('url'), a, owl.NamedIndividual), See also If the same IRI is used in multiple statements, repeating the same assertion multiple times may impose a maintenance burden. In such cases, it is possible to first add the IRI to the record using transformation function addIri() , and refer to that one IRI in multiple statements. Use function iris() to create multiple IRIs in one step. Function iris() {#iris} Asserts multiple IRIs, one for each entry in an array of strings: iris(prefix:PrefixedToIri, content:Key|Array<StaticString>) iris(content:Key|Array<StaticString>) Parameters prefix A prefix that is declared with declarePrefix() . content Either a key that contains a array of string values, or an array of static strings. If the prefix is used, this content is placed after the prefix (sometimes referred to a the 'local name'). If the prefix parameter is not used, the content must specify the full IRI. Example The following code snippet asserts one IRI for each entry in record key 'children' : fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.id, 'parent'), sdo.children, iris(prefix.id, 'children')), This makes the following linked data assertions: id:John sdo:children id:Joe, id:Jane. Or diagrammatically: graph LR john -- sdo:children --> joe john -- sdo:children --> jane john[id:John]:::data joe[id:Joe]:::data jane[id:Jane]:::data classDef data fill:yellow Function literal() {#literal} Asserts a literal term: literal(lexicalForm, languageTagOrDatatype) Parameters lexicalForm A static string (see function str() ), or a key that contains a string value. languageTagOrDatatype A static language tag, or a static datatype IRI, or a key that contains either a language tag or a datatype IRI. Examples The following snippet asserts a language-tagged string: triple('city', sdo.name, literal('name', lang.nl)), The following snippet asserts a typed literal: triple('city', vocab.population, literal('population', xsd.nonNegativeInteger)), Notice that string literals can be asserted directly; the following two statements make the same assertion: triple('city', dct.identifier, literal('id', xsd.string)), triple('city', dct.identifier, 'id'), These assertions combined can result in the following linked data: id:amsterdam sdo:name 'Amsterdam'@nl vocab:population '800000'^^xsd:nonNegativeInteger dct:identifier '0200'. See also If the same literal is used in multiple statements, repeating the same assertion multiple times can impose a maintenance burden. In such cases, it is possible to first add the literal to the record with transformation addLiteral() , and refer to that one literal in multiple statements. Use assertion literals() to create multiple literals in one step. Function literals() {#literals} Asserts multiple literals, one for each given lexical form: literals(lexicalForms, languageTagOrDatatype) When the record contains a key that stores an array, it is possible to create one literal for each value in the array. Parameters lexicalForms A key that stores an array. languageTagOrDatatype A static language tag, or a static datatype IRI, or a key that stores a language tag or datatype IRI. Example: Fruit basket The following code snippet creates one literal for each value in the array that is stored in the 'contents' key: fromJson([{ id: 123, contents: ['apple', 'pear', 'banana'] }]), triple(iri(prefix.basket, 'id'), rdfs.member, literals('contents', lang.en)), This makes the following linked data assertions: basket:123 rdfs:member 'apple'@en, 'banana'@en, 'pear'@en. Or diagrammatically: graph LR basket -- rdfs:member --> apple basket -- rdfs:member --> banana basket -- rdfs:member --> pear apple[\"'apple'@en\"]:::data banana[\"'banana'@en\"]:::data basket[basket:123]:::data pear[\"'pear'@en\"]:::data classDef data fill:yellow Example: Names String literals can be asserted directly from a key that stores an array of strings. The following code snippet asserts one string literal for each child: fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.id, 'parent'), sdo.children, 'children'), This makes the following linked data assertions: id:John sdo:children 'Jane', 'Joe'. Or diagrammatically: graph LR john -- sdo:children --> jane john -- sdo:children --> joe jane['Jane']:::data joe['Joe']:::data john['John']:::data classDef data fill:yellow Notice that the same could have been achieved with an explicit datatype IRI: triple(iri(prefix.id, 'parent'), sdo.children, literals('children', xsd.string)), Function nestedPairs() {#nestedPairs} This function creates a nested node and makes multiple assertions about that node. Since linked data is composed of triples, more complex n-ary information must often be asserted with one or more nested nodes. Such nested nodes can be given a name with assertion iri() or transformation addIri() . Parameters subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). nestedNode The nested node. This must be an IRI (see function iri() ). pairs One or more pairs that make assertions about the nested node. Every pair consists of a predicate term and an object term (in that order). Example: Unit of measure The following example asserts a value together with a unit of measure. A well-known Skolem IRI or 'blank node' is used to attach the value and unit to: fromJson([{ id: '1', height: 15 }]), addSkolemIri({ prefix: prefix.skolem, key: '_height', }), nestedPairs(iri(prefix.product, 'id'), sdo.height, '_height', [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), This makes the following linked data assertions: product:1 sdo:height [ qudt:unit unit:CentiM; rdf:value 15 ]. Or diagrammatically: graph LR product -- sdo:height --> skolem skolem -- qudt:unit --> centim skolem -- rdf:value --> 15 product[product:1]:::data skolem[_:1]:::data centim[unit:CentiM]:::model 15:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown Example: Geometry The following example asserts a GeoSPARQL geometry. The geometry is created as a separate node. fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), nestedpairs(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), This generates the following linked data, where a well-known Skolem IRI is used for the geometry 'blank node': feature:1 geo:hasGeometry geometry:1. geometry:1 a geo:Geometry; geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt feature[feature:1]:::data geometry[geometry:1]:::data Geometry[geo:Geometry]:::model wkt[\"'Point(1.1 2.2)'^^geo:wktLiteral\"]:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown Assertions that use nestedpairs() provide a shorter notation for the following sequence of assertions that uses functions triple() and pairs() : fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), triple(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id')), pairs(iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), See also In some cases, it is inconvenient to come up with a naming scheme for intermediate nodes. In such cases, the following options are available: - Use transformation addHashedIri() to create a content-based IRI. - Use transformation addRandomIri() to create a random IRI. - Use transformation addSkolemIri() to create a random Skolem IRI. Function objects() {#objects} Asserts multiple triples that share the same subject term and predicate term. This function provides a shorthand notation for assertions that can also be made with multiple uses of the triple() assertion function. The notational convenience of this middleware is similar to predicate-object lists in TriG, Turtle, and SPARQL. Parameters subject A subject term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). predicate A predicate term. This must be an IRI (see function iri() ). objects An array of object terms. This must be either an IRI (see function iri() ) or a literal (see function literal ). Every distinct object term in the array results in a distinct triple assertion. Example: Alternative labels The following snippet asserts multiple alternative labels for a city: fromJson([{ name: 'Ney York', alt1: 'The Big Apple', alt2: 'The Capital of the World', alt3: 'The City of Dreams', alt4: 'The City That Never Sleeps', }]), objects(iri(prefix.city, 'name'), skos.altLabel, ['alt1', 'alt2', 'alt3', 'alt4'] ), This results in the following 4 linked data assertions: city:New%20York skos:altLabel 'The Big Apple'@en. 'The Capital of the World'@en, 'The City of Dreams'@en, 'The City That Never Sleeps'@en. Or diagrammatically: graph LR newYork -- skos:altLabel --> a & b & c & d newYork[city:New%20York]:::data a[\"'The Big Apple'@en\"]:::data b[\"'The Capital of the World'@en\"]:::data c[\"'The City of Dreams'@en\"]:::data d[\"'The City That Never Sleeps'@en\"]:::data classDef data fill:yellow Function pairs() {#pairs} Asserts multiple triples that share the same subject term. This function provides a shorthand notation for assertions that can also be made with multiple uses of assertion triple() . The notational convenience of this middleware is similar to predicate lists in TriG, Turtle, and SPARQL. Parameters subject The subject term of the asserted triples. pairs Zero or more pairs. Each pair is an array with a predicate term and an object term (in that order). Every distinct element in the pairs array results in a distinct triple assertion. Example: Alternative and preferred label The following snippet asserts a preferred label and an alternative label for cities: fromJson([ { name: 'London', alt: 'Home of the Big Ben' }, { name: 'Ney York', alt: 'The Big Apple' }, ]), pairs(iri(prefix.city, 'name'), [skos.prefLabel, literal('name', lang.en)], [skos.altLabel, literal('alt', lang.en)], ), This results in the following 4 linked data assertions: city:London skos:prefLabel 'London'@en; skos:altLabel 'Home of the Big Ben'@en. city:New%20York skos:prefLabel 'New York'@en; skos:altLabel 'The Big Apple'@en. Or diagrammatically: graph LR london -- skos:altLabel --> a london -- skos:prefLabel --> b newYork -- skos:altLabel --> c newYork -- skos:prefLabel --> d london[city:London]:::data newYork[city:New%20York]:::data a[\"'Home of the Big Ben'@en\"]:::data b[\"'London'@en\"]:::data c[\"'The Big Apple'@en\"]:::data d[\"'New York'@en\"]:::data classDef data fill:yellow Function quad() {#quad} Asserts a quadruple or 'quad', i.e. a statement that consists of a subject term, a predicate term, an object term, and a graph name. A quadruple is a triple with a graph name as its fourth parameter. Parameters subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). object An object term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). graph A graph name. This must be an IRI (see function iri() ). Example: Data and metadata An ETL may distinguish between data and metadata assertions. Both may be placed into distinct graphs. The following snippet makes one assertion in a metadata graph and one assertion in a data graph. quad(iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata), quad(iri(prefix.flower, '_id'), a, def.Flower, graph.data), See also Use function quads() to make multiple quadruple assertions. Function quads() {#quads} Asserts multiple quadruples or 'quads', i.e. statements that consists of a subject term, a predicate term, an object term, and a graph name. A quadruple is a triple with a graph name as its fourth parameter. Parameters subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). object An object term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). graph A graph name. This must be an IRI (see function iri() ). Example: Data and metadata An ETL can distinguish between data and metadata assertions. Both may be placed into distinct graphs. The following snippet makes assertions in a metadata graph and assertions in a data graph. quads( [iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata], ..., ), quads( [iri(prefix.flower, '_id'), a, def.Flower, graph.data], ..., ), See also Use function quad() for asserting a single quadruple. Function str() {#str} Asserts a static string value. When to use Strings in assertions are typically used to denote keys in the Record. For example, the string 'abc' in the following code snippet indicates that the value of key 'abc' should be used as the local name of the IRI in the subject position. The value of key 'abc' should also be used as the lexical form of the literal in the object position: triple(iri(prefix.id, 'abc'), rdfs.label, 'abc'), But sometimes we want to assert a static string, i.e. the actual string value 'abc' instead of the string value stored in a key with that name. In such cases the string function str() can be used. The following code snippet asserts the IRI id:abc and literal 'abc' : triple(iri(prefix.id, str('abc')), rdfs.label, str('abc')), Function triple() {#triple} Asserts a triple, i.e. a statement that consists of a subject term, a predicate term, and an object term. A triple is a sequence of three terms: subject, predicate, and object. A triple asserts a factual statement, claiming that the thing denoted by the subject term and the thing denotes by the object term are related to one another according to the relationship denoted by the predicate term. A triple is the smallest unit of meaning in linked data. Parameters subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). object An object term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). Example 1 The following triple asserts that something is a person. Notice that: - the subject term is an IRI that is constructed out of an IRI prefix ( prefix.person ) and a key that contains the IRI local name ( 'id' ), - and the predicate and object terms are IRIs that are imported from the vocabulary module. triple(iri(prefix.person, 'id'), a, foaf.Person), Example 2 The following triple asserts that something has an age that is derived from the 'age' key in the record. Notice that: the subject term is an IRI that is stored in the '_person' key of the record (possibly created with transformation function addIri() ), the predicate term is an IRI ( foaf.age ) that is imported from the vocabulary module, and the object term is a typed literal with a datatype IRI that is imported from the vocabulary module. triple('_person', foaf.age, literal('age', xsd.nonNegativeInteger)), Function triples() {#triples} Asserts multiple triples in the same named graph: triples(graph, triples) Parameters graph A graph name. This must be an IRI (see function iri() ). triples An array of triples. Every triple is represented by an array of 3 terms: subject, predicate, and object. When to use It is common for multiple statements to occur in the same graph. In such cases, it is suboptimal to repeat the graph name for multiple uses of the quad() function. In such cases, it is shorter to use the triples() function, where the graph name only needs to be specified once. Example Suppose that we want to distinguish between data and metadata assertions. We can do so by asserting them in distinct graphs. The following makes multiple metadata assertions in the metadata graph, followed by multiple data assertions in the data graph. triples(graph.metadata, [iri(prefix.dataset, str('flowers')), a, dcat.Dataset], ... ), triples(graph.data, [iri(prefix.flower, '_id'), a, def.Flower], ... ), See also The triples() function is conceptually similar to graph notation in the TriG standard. In TriG, the graph name is specified up front, and the triples within that graph are specified immediately afterwards: graph:flowers { id:123 a def:Flower. # other triples go here } Notice the correspondence with the following code snippet that uses the triples() function: triples(iri(prefix.ex, 'myGraph'), [iri(prefix.ex, 'id'), a, def.Flower)], // other triples go here ),","title":"3. Assert: RATT"},{"location":"triply-etl/assert/ratt/#overview","text":"The following assertion functions are available: Assertion Description iri() Create an IRI term. iris() Creates multiple IRI terms. literal() Creates a literal term. literals() Creates multiple literal terms. nestedPairs() Creates a nested node with multiple triples that use that node as their subject term. objects() Asserts multiple triples that share the same subject and predicate terms. pairs() Asserts multiple triples that share the same subject term. quad() Asserts a quadruple. quads() Asserts multiple quadruples. str() Creates a static string. triple() Asserts a triple. triples() Asserts multiple triples. All RATT assertions can be imported from the RATT library in TriplyETL: import { iri, iris, literal, literals, nestedPairs, objects, pairs, quad, quads, triple, triples } from '@triplyetl/etl/ratt'","title":"Overview"},{"location":"triply-etl/assert/ratt/#function-iri-iri","text":"Asserts an IRI term based on a key and an optional IRI prefix: iri(prefix:PrefixedToIri, content:Key|StaticString) iri(content:Key|StaticString)","title":"Function iri() {#iri}"},{"location":"triply-etl/assert/ratt/#parameters","text":"prefix A prefix that is declared with declarePrefix() . content Either a key that contains a string value, or a static string. If the prefix is used, this content is placed after the prefix (sometimes referred to a the 'local name'). If the prefix parameter is not used, the content must specify the full IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#examples","text":"The following asserts an IRI based on a declared prefix ( prefix.ex ) and a key ( name ): triple(iri(prefix.ex, 'name'), a, owl.NamedIndividual), The following asserts an IRI based on a declared prefix ( prefix.ex ) and a static string (see function str() ): triple(iri(prefix.ex, str('bob')), a, owl.NamedIndividual), The following asserts an IRI based on the value stored in key 'url' . Notice that this value must encode a full absolute IRI. fromJson([{ 'url': 'https://example.com/bob' }]), triple(iri('url'), a, owl.NamedIndividual),","title":"Examples"},{"location":"triply-etl/assert/ratt/#see-also","text":"If the same IRI is used in multiple statements, repeating the same assertion multiple times may impose a maintenance burden. In such cases, it is possible to first add the IRI to the record using transformation function addIri() , and refer to that one IRI in multiple statements. Use function iris() to create multiple IRIs in one step.","title":"See also"},{"location":"triply-etl/assert/ratt/#function-iris-iris","text":"Asserts multiple IRIs, one for each entry in an array of strings: iris(prefix:PrefixedToIri, content:Key|Array<StaticString>) iris(content:Key|Array<StaticString>)","title":"Function iris() {#iris}"},{"location":"triply-etl/assert/ratt/#parameters_1","text":"prefix A prefix that is declared with declarePrefix() . content Either a key that contains a array of string values, or an array of static strings. If the prefix is used, this content is placed after the prefix (sometimes referred to a the 'local name'). If the prefix parameter is not used, the content must specify the full IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example","text":"The following code snippet asserts one IRI for each entry in record key 'children' : fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.id, 'parent'), sdo.children, iris(prefix.id, 'children')), This makes the following linked data assertions: id:John sdo:children id:Joe, id:Jane. Or diagrammatically: graph LR john -- sdo:children --> joe john -- sdo:children --> jane john[id:John]:::data joe[id:Joe]:::data jane[id:Jane]:::data classDef data fill:yellow","title":"Example"},{"location":"triply-etl/assert/ratt/#function-literal-literal","text":"Asserts a literal term: literal(lexicalForm, languageTagOrDatatype)","title":"Function literal() {#literal}"},{"location":"triply-etl/assert/ratt/#parameters_2","text":"lexicalForm A static string (see function str() ), or a key that contains a string value. languageTagOrDatatype A static language tag, or a static datatype IRI, or a key that contains either a language tag or a datatype IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#examples_1","text":"The following snippet asserts a language-tagged string: triple('city', sdo.name, literal('name', lang.nl)), The following snippet asserts a typed literal: triple('city', vocab.population, literal('population', xsd.nonNegativeInteger)), Notice that string literals can be asserted directly; the following two statements make the same assertion: triple('city', dct.identifier, literal('id', xsd.string)), triple('city', dct.identifier, 'id'), These assertions combined can result in the following linked data: id:amsterdam sdo:name 'Amsterdam'@nl vocab:population '800000'^^xsd:nonNegativeInteger dct:identifier '0200'.","title":"Examples"},{"location":"triply-etl/assert/ratt/#see-also_1","text":"If the same literal is used in multiple statements, repeating the same assertion multiple times can impose a maintenance burden. In such cases, it is possible to first add the literal to the record with transformation addLiteral() , and refer to that one literal in multiple statements. Use assertion literals() to create multiple literals in one step.","title":"See also"},{"location":"triply-etl/assert/ratt/#function-literals-literals","text":"Asserts multiple literals, one for each given lexical form: literals(lexicalForms, languageTagOrDatatype) When the record contains a key that stores an array, it is possible to create one literal for each value in the array.","title":"Function literals() {#literals}"},{"location":"triply-etl/assert/ratt/#parameters_3","text":"lexicalForms A key that stores an array. languageTagOrDatatype A static language tag, or a static datatype IRI, or a key that stores a language tag or datatype IRI.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-fruit-basket","text":"The following code snippet creates one literal for each value in the array that is stored in the 'contents' key: fromJson([{ id: 123, contents: ['apple', 'pear', 'banana'] }]), triple(iri(prefix.basket, 'id'), rdfs.member, literals('contents', lang.en)), This makes the following linked data assertions: basket:123 rdfs:member 'apple'@en, 'banana'@en, 'pear'@en. Or diagrammatically: graph LR basket -- rdfs:member --> apple basket -- rdfs:member --> banana basket -- rdfs:member --> pear apple[\"'apple'@en\"]:::data banana[\"'banana'@en\"]:::data basket[basket:123]:::data pear[\"'pear'@en\"]:::data classDef data fill:yellow","title":"Example: Fruit basket"},{"location":"triply-etl/assert/ratt/#example-names","text":"String literals can be asserted directly from a key that stores an array of strings. The following code snippet asserts one string literal for each child: fromJson([{ parent: 'John', children: ['Joe', 'Jane'] }]), triple(iri(prefix.id, 'parent'), sdo.children, 'children'), This makes the following linked data assertions: id:John sdo:children 'Jane', 'Joe'. Or diagrammatically: graph LR john -- sdo:children --> jane john -- sdo:children --> joe jane['Jane']:::data joe['Joe']:::data john['John']:::data classDef data fill:yellow Notice that the same could have been achieved with an explicit datatype IRI: triple(iri(prefix.id, 'parent'), sdo.children, literals('children', xsd.string)),","title":"Example: Names"},{"location":"triply-etl/assert/ratt/#function-nestedpairs-nestedpairs","text":"This function creates a nested node and makes multiple assertions about that node. Since linked data is composed of triples, more complex n-ary information must often be asserted with one or more nested nodes. Such nested nodes can be given a name with assertion iri() or transformation addIri() .","title":"Function nestedPairs() {#nestedPairs}"},{"location":"triply-etl/assert/ratt/#parameters_4","text":"subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). nestedNode The nested node. This must be an IRI (see function iri() ). pairs One or more pairs that make assertions about the nested node. Every pair consists of a predicate term and an object term (in that order).","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-unit-of-measure","text":"The following example asserts a value together with a unit of measure. A well-known Skolem IRI or 'blank node' is used to attach the value and unit to: fromJson([{ id: '1', height: 15 }]), addSkolemIri({ prefix: prefix.skolem, key: '_height', }), nestedPairs(iri(prefix.product, 'id'), sdo.height, '_height', [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), This makes the following linked data assertions: product:1 sdo:height [ qudt:unit unit:CentiM; rdf:value 15 ]. Or diagrammatically: graph LR product -- sdo:height --> skolem skolem -- qudt:unit --> centim skolem -- rdf:value --> 15 product[product:1]:::data skolem[_:1]:::data centim[unit:CentiM]:::model 15:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: Unit of measure"},{"location":"triply-etl/assert/ratt/#example-geometry","text":"The following example asserts a GeoSPARQL geometry. The geometry is created as a separate node. fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), nestedpairs(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ), This generates the following linked data, where a well-known Skolem IRI is used for the geometry 'blank node': feature:1 geo:hasGeometry geometry:1. geometry:1 a geo:Geometry; geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt feature[feature:1]:::data geometry[geometry:1]:::data Geometry[geo:Geometry]:::model wkt[\"'Point(1.1 2.2)'^^geo:wktLiteral\"]:::data classDef data fill:yellow classDef model fill:lightblue classDef meta fill:sandybrown Assertions that use nestedpairs() provide a shorter notation for the following sequence of assertions that uses functions triple() and pairs() : fromJson([{ id: '1', geometry: 'Point(1.1 2.2)' }]), triple(iri(prefix.feature, 'id'), geo.hasGeometry, iri(prefix.geometry, 'id')), pairs(iri(prefix.geometry, 'id'), [a, geo.Geometry], [geo.asWKT, literal('geometry', geo.wktLiteral)], ),","title":"Example: Geometry"},{"location":"triply-etl/assert/ratt/#see-also_2","text":"In some cases, it is inconvenient to come up with a naming scheme for intermediate nodes. In such cases, the following options are available: - Use transformation addHashedIri() to create a content-based IRI. - Use transformation addRandomIri() to create a random IRI. - Use transformation addSkolemIri() to create a random Skolem IRI.","title":"See also"},{"location":"triply-etl/assert/ratt/#function-objects-objects","text":"Asserts multiple triples that share the same subject term and predicate term. This function provides a shorthand notation for assertions that can also be made with multiple uses of the triple() assertion function. The notational convenience of this middleware is similar to predicate-object lists in TriG, Turtle, and SPARQL.","title":"Function objects() {#objects}"},{"location":"triply-etl/assert/ratt/#parameters_5","text":"subject A subject term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). predicate A predicate term. This must be an IRI (see function iri() ). objects An array of object terms. This must be either an IRI (see function iri() ) or a literal (see function literal ). Every distinct object term in the array results in a distinct triple assertion.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-alternative-labels","text":"The following snippet asserts multiple alternative labels for a city: fromJson([{ name: 'Ney York', alt1: 'The Big Apple', alt2: 'The Capital of the World', alt3: 'The City of Dreams', alt4: 'The City That Never Sleeps', }]), objects(iri(prefix.city, 'name'), skos.altLabel, ['alt1', 'alt2', 'alt3', 'alt4'] ), This results in the following 4 linked data assertions: city:New%20York skos:altLabel 'The Big Apple'@en. 'The Capital of the World'@en, 'The City of Dreams'@en, 'The City That Never Sleeps'@en. Or diagrammatically: graph LR newYork -- skos:altLabel --> a & b & c & d newYork[city:New%20York]:::data a[\"'The Big Apple'@en\"]:::data b[\"'The Capital of the World'@en\"]:::data c[\"'The City of Dreams'@en\"]:::data d[\"'The City That Never Sleeps'@en\"]:::data classDef data fill:yellow","title":"Example: Alternative labels"},{"location":"triply-etl/assert/ratt/#function-pairs-pairs","text":"Asserts multiple triples that share the same subject term. This function provides a shorthand notation for assertions that can also be made with multiple uses of assertion triple() . The notational convenience of this middleware is similar to predicate lists in TriG, Turtle, and SPARQL.","title":"Function pairs() {#pairs}"},{"location":"triply-etl/assert/ratt/#parameters_6","text":"subject The subject term of the asserted triples. pairs Zero or more pairs. Each pair is an array with a predicate term and an object term (in that order). Every distinct element in the pairs array results in a distinct triple assertion.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-alternative-and-preferred-label","text":"The following snippet asserts a preferred label and an alternative label for cities: fromJson([ { name: 'London', alt: 'Home of the Big Ben' }, { name: 'Ney York', alt: 'The Big Apple' }, ]), pairs(iri(prefix.city, 'name'), [skos.prefLabel, literal('name', lang.en)], [skos.altLabel, literal('alt', lang.en)], ), This results in the following 4 linked data assertions: city:London skos:prefLabel 'London'@en; skos:altLabel 'Home of the Big Ben'@en. city:New%20York skos:prefLabel 'New York'@en; skos:altLabel 'The Big Apple'@en. Or diagrammatically: graph LR london -- skos:altLabel --> a london -- skos:prefLabel --> b newYork -- skos:altLabel --> c newYork -- skos:prefLabel --> d london[city:London]:::data newYork[city:New%20York]:::data a[\"'Home of the Big Ben'@en\"]:::data b[\"'London'@en\"]:::data c[\"'The Big Apple'@en\"]:::data d[\"'New York'@en\"]:::data classDef data fill:yellow","title":"Example: Alternative and preferred label"},{"location":"triply-etl/assert/ratt/#function-quad-quad","text":"Asserts a quadruple or 'quad', i.e. a statement that consists of a subject term, a predicate term, an object term, and a graph name. A quadruple is a triple with a graph name as its fourth parameter.","title":"Function quad() {#quad}"},{"location":"triply-etl/assert/ratt/#parameters_7","text":"subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). object An object term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). graph A graph name. This must be an IRI (see function iri() ).","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-data-and-metadata","text":"An ETL may distinguish between data and metadata assertions. Both may be placed into distinct graphs. The following snippet makes one assertion in a metadata graph and one assertion in a data graph. quad(iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata), quad(iri(prefix.flower, '_id'), a, def.Flower, graph.data),","title":"Example: Data and metadata"},{"location":"triply-etl/assert/ratt/#see-also_3","text":"Use function quads() to make multiple quadruple assertions.","title":"See also"},{"location":"triply-etl/assert/ratt/#function-quads-quads","text":"Asserts multiple quadruples or 'quads', i.e. statements that consists of a subject term, a predicate term, an object term, and a graph name. A quadruple is a triple with a graph name as its fourth parameter.","title":"Function quads() {#quads}"},{"location":"triply-etl/assert/ratt/#parameters_8","text":"subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). object An object term. This must be either an IRI (see function iri() ) or a literal (see function literal() ). graph A graph name. This must be an IRI (see function iri() ).","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-data-and-metadata_1","text":"An ETL can distinguish between data and metadata assertions. Both may be placed into distinct graphs. The following snippet makes assertions in a metadata graph and assertions in a data graph. quads( [iri(prefix.dataset, 'flowers'), a, dcat.Dataset, graph.metadata], ..., ), quads( [iri(prefix.flower, '_id'), a, def.Flower, graph.data], ..., ),","title":"Example: Data and metadata"},{"location":"triply-etl/assert/ratt/#see-also_4","text":"Use function quad() for asserting a single quadruple.","title":"See also"},{"location":"triply-etl/assert/ratt/#function-str-str","text":"Asserts a static string value.","title":"Function str() {#str}"},{"location":"triply-etl/assert/ratt/#when-to-use","text":"Strings in assertions are typically used to denote keys in the Record. For example, the string 'abc' in the following code snippet indicates that the value of key 'abc' should be used as the local name of the IRI in the subject position. The value of key 'abc' should also be used as the lexical form of the literal in the object position: triple(iri(prefix.id, 'abc'), rdfs.label, 'abc'), But sometimes we want to assert a static string, i.e. the actual string value 'abc' instead of the string value stored in a key with that name. In such cases the string function str() can be used. The following code snippet asserts the IRI id:abc and literal 'abc' : triple(iri(prefix.id, str('abc')), rdfs.label, str('abc')),","title":"When to use"},{"location":"triply-etl/assert/ratt/#function-triple-triple","text":"Asserts a triple, i.e. a statement that consists of a subject term, a predicate term, and an object term. A triple is a sequence of three terms: subject, predicate, and object. A triple asserts a factual statement, claiming that the thing denoted by the subject term and the thing denotes by the object term are related to one another according to the relationship denoted by the predicate term. A triple is the smallest unit of meaning in linked data.","title":"Function triple() {#triple}"},{"location":"triply-etl/assert/ratt/#parameters_9","text":"subject A subject term. This must be an IRI (see function iri() ). predicate A predicate term. This must be an IRI (see function iri() ). object An object term. This must be either an IRI (see function iri() ) or a literal (see function literal() ).","title":"Parameters"},{"location":"triply-etl/assert/ratt/#example-1","text":"The following triple asserts that something is a person. Notice that: - the subject term is an IRI that is constructed out of an IRI prefix ( prefix.person ) and a key that contains the IRI local name ( 'id' ), - and the predicate and object terms are IRIs that are imported from the vocabulary module. triple(iri(prefix.person, 'id'), a, foaf.Person),","title":"Example 1"},{"location":"triply-etl/assert/ratt/#example-2","text":"The following triple asserts that something has an age that is derived from the 'age' key in the record. Notice that: the subject term is an IRI that is stored in the '_person' key of the record (possibly created with transformation function addIri() ), the predicate term is an IRI ( foaf.age ) that is imported from the vocabulary module, and the object term is a typed literal with a datatype IRI that is imported from the vocabulary module. triple('_person', foaf.age, literal('age', xsd.nonNegativeInteger)),","title":"Example 2"},{"location":"triply-etl/assert/ratt/#function-triples-triples","text":"Asserts multiple triples in the same named graph: triples(graph, triples)","title":"Function triples() {#triples}"},{"location":"triply-etl/assert/ratt/#parameters_10","text":"graph A graph name. This must be an IRI (see function iri() ). triples An array of triples. Every triple is represented by an array of 3 terms: subject, predicate, and object.","title":"Parameters"},{"location":"triply-etl/assert/ratt/#when-to-use_1","text":"It is common for multiple statements to occur in the same graph. In such cases, it is suboptimal to repeat the graph name for multiple uses of the quad() function. In such cases, it is shorter to use the triples() function, where the graph name only needs to be specified once.","title":"When to use"},{"location":"triply-etl/assert/ratt/#example_1","text":"Suppose that we want to distinguish between data and metadata assertions. We can do so by asserting them in distinct graphs. The following makes multiple metadata assertions in the metadata graph, followed by multiple data assertions in the data graph. triples(graph.metadata, [iri(prefix.dataset, str('flowers')), a, dcat.Dataset], ... ), triples(graph.data, [iri(prefix.flower, '_id'), a, def.Flower], ... ),","title":"Example"},{"location":"triply-etl/assert/ratt/#see-also_5","text":"The triples() function is conceptually similar to graph notation in the TriG standard. In TriG, the graph name is specified up front, and the triples within that graph are specified immediately afterwards: graph:flowers { id:123 a def:Flower. # other triples go here } Notice the correspondence with the following code snippet that uses the triples() function: triples(iri(prefix.ex, 'myGraph'), [iri(prefix.ex, 'id'), a, def.Flower)], // other triples go here ),","title":"See also"},{"location":"triply-etl/changelog/","text":"The current version of TriplyETL is 2.0.10 You can use this changelog to perform a safe update from an older version of TriplyETL to a newer one. See the documentation for Upgrading TriplyETL repositories for the advised approach, and how the changelog factors into that. Changelog for TriplyETL 2.0.7 through 2.0.10 Release dates: 2023-06-17 through 2023-06-28 Bugfixes: - Installation on Windows. - Fixed warnings for a dependency (async-saxophone for XML processing), which did not yet officially support Node.js 18. Changelog for TriplyETL 2.0.6 Release date: 2023-06-07 [Added] Support for the PREMIS vocabulary Support was added for the PREMIS 3.0.0 vocabulary. This vocabulary is published by the Library of Congress and can be used to publish metadata about the preservation of digital objects. See the PREMIS documentation for more information. The vocabulary can be imported from the 'vocab' module: import { premis } from '@triplyetl/etl/vocab' The following code snippet uses the vocabulary to assert that a PREMIS file is stored in a PREMIS storage location: pairs(iri(id, 'some-file'), [a, premis.File], [premis.storedAt, iri(id, 'some-location')], ), triple(iri(id, 'some-location'), a, premis.StorageLocation), See the documentation about external vocabulary declarations for more information. [Added] New debug function logMemory() A new debug function logMemory() is added. This function prints an overview of the current memory usage of TriplyETL. This allows users to detect fluctuations in memory consumption inside their pipelines. See the debug functions documentation page for more information. [Added] Support for the ListIdentifiers verb in the OAI-PMH extractor The fromOai() extractor already supported the ListRecords verb. This release adds support for the ListIdentifiers verb. The latter allows used to stream through the headers of all records in an OAI-PMG collection, without requiring the full record (i.e. body) to be retrieved as well. See the fromOai() documentation for more information. Changelog for TriplyETL 2.0.5 Release date: 2023-05-25 [Changed] New default engine for SPARQL Construct The default engine for evaluating SPARQL Construct queries (function construct() ) has changed from Comunica to Speedy. Speedy is a new SPARQL implementation that is developed by Triply; Comunica is an open source engine that is developed by the open source community. Since SPARQL is a standardized query language, this change should not cause a difference in behavior for your ETL pipelines. In the unexpected case where an ETL pipeline is negatively affected by this change, the old situation can be restored by explicitly configuring the Comunica engine: import { construct } from '@triplyetl/etl/sparql' construct(Source.TriplyDb.query('my-query'), { sparqlEngine: 'comunica' }), The benefit of switching to the Speedy engine is that this engine is expected to be faster for most queries. Overall, this change will therefore result in speed improvements for your TriplyETL pipelines. [Added] New CLI tool for comparing graphs The new CLI tool compare allows graph comparison to be performed from the command-line. This uses the same algorithm that is used by the compareGraphs() validator function. Bug fixes This release includes the following bug fixes: fromXlsx() did not remove trailing whitespace in cell values. When a SHACL result was printed, an incorrect message about a faulty SHACL model would be shown. Some RDF processors did not handle empty RDF inputs correctly. Changelog for TriplyETL 2.0.4 Release date: 2023-05-11 [Enhanced] Better output for checking graph isomorphism Before this release, when two graphs were not isomorph and their difference consisted of a mapping from blank nodes onto blank nodes exclusively, an empty difference message was communicated. From this release onwards, the difference message is non-empty, and specifically indicates the difference between the non-isomorphic graphs. The following snippet should emit a non-empty difference: import { Etl, Source, compareGraphs, loadRdf } from \"@triplyetl/etl/generic\" export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf( Source.string('base <https://triply.cc/> <g> { []<p><o> }'), { contentType: 'application/trig' } ), compareGraphs( Source.string('base <https://triply.cc/> <g> { [] <p><o>. []<p><o>. }'), { contentType: 'application/trig' } ), ) return etl } Notice that the two TriG snippets are not isomorphic: graph LR _:1 -- p --> o. and graph LR _:2a -- p --> o. _:2b -- p --> o. It is possible to map _:2a and _:2b onto _:1 , but there is no mapping that works the other way round. Changelog for TriplyETL 2.0.3 Release date: 2023-05-10 Bug fixes This release includes the following bug fixes: Error location information is not shown in TriplyETL Runner. Issue when a URL data source ( Source.url() ) includes an HTTP body. Changelog for TriplyETL 2.0.2 Release date: 2023-05-09 Bug fixes This release fixes bugs related to the recent switch from CommonJS to ESM: Dynamic import bug on Windows. Error reporting issues due to ESM imports. Changelog for TriplyETL 2.0.1 Release date: 2023-05-03 [Added] Timeout flag for TriplyETL Runner The TriplyETL Runner is the CLI tool that is used to run ETL pipelines. Starting with this version, you can specify a --timeout flag when using the TriplyETL Runner. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. See the TriplyETL Runner documentation page for more information. Changelog for TriplyETL 2.0.0 Release date: 2023-05-01 [Changed] Modules infrastructure moves from CommonJS to ESM Before this release, TriplyETL used CommonJS modules to modularize its functionality into different components. Starting in this release, ECMAScript Modules (ESM) are used to modularize TriplyETL functionality into different modules. ESM is a more modern approach for modularizing ECMAScript (JavaScript, TypeScript, and Node.js) code. While CommonJS imports are evaluated at runtime, ESM imports are evaluated at compile time. TriplyETL users benefit from this change, since error messages related to module imports will be detected much earlier in the development process. All documentation examples were update to use ESM syntax for module imports, for example: import { logRecord } from '@triplyetl/etl/debug' [Changed] Debug functions move to a new module Before this release, debug functions like logRecord() and startTrace() were part of the RATT module. Since debug functions are generic / not RATT-specific, they were moved into a new module. Function are imported from this new module in the following way: import { logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug' [Enhanced] Better error messages when things go wrong This release introduces a new approach for communicating errors back to the user. When TriplyETL functionality detects an error condition, a unified 'trace middleware' is now used to retrieve information from the environment in which the error occurred. This information is then printed to the error output stream for communication with the user. Bug fixes The following bug fixes are included in this release: Incorrect behavior of the _switch() control function . The fromOai() extractor now communicates clearer when the accessed OAI-PMH endpoint encounters any issues. When a key with a NULL value was accessed, the name of the key is now included in the error message. Start of the changelog TriplyETL 1.0.0 was released on 2023-03-20.","title":"TriplyETL: Changelog"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-207-through-2010","text":"Release dates: 2023-06-17 through 2023-06-28 Bugfixes: - Installation on Windows. - Fixed warnings for a dependency (async-saxophone for XML processing), which did not yet officially support Node.js 18.","title":"Changelog for TriplyETL 2.0.7 through 2.0.10"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-206","text":"Release date: 2023-06-07","title":"Changelog for TriplyETL 2.0.6"},{"location":"triply-etl/changelog/#added-support-for-the-premis-vocabulary","text":"Support was added for the PREMIS 3.0.0 vocabulary. This vocabulary is published by the Library of Congress and can be used to publish metadata about the preservation of digital objects. See the PREMIS documentation for more information. The vocabulary can be imported from the 'vocab' module: import { premis } from '@triplyetl/etl/vocab' The following code snippet uses the vocabulary to assert that a PREMIS file is stored in a PREMIS storage location: pairs(iri(id, 'some-file'), [a, premis.File], [premis.storedAt, iri(id, 'some-location')], ), triple(iri(id, 'some-location'), a, premis.StorageLocation), See the documentation about external vocabulary declarations for more information.","title":"[Added] Support for the PREMIS vocabulary"},{"location":"triply-etl/changelog/#added-new-debug-function-logmemory","text":"A new debug function logMemory() is added. This function prints an overview of the current memory usage of TriplyETL. This allows users to detect fluctuations in memory consumption inside their pipelines. See the debug functions documentation page for more information.","title":"[Added] New debug function logMemory()"},{"location":"triply-etl/changelog/#added-support-for-the-listidentifiers-verb-in-the-oai-pmh-extractor","text":"The fromOai() extractor already supported the ListRecords verb. This release adds support for the ListIdentifiers verb. The latter allows used to stream through the headers of all records in an OAI-PMG collection, without requiring the full record (i.e. body) to be retrieved as well. See the fromOai() documentation for more information.","title":"[Added] Support for the ListIdentifiers verb in the OAI-PMH extractor"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-205","text":"Release date: 2023-05-25","title":"Changelog for TriplyETL 2.0.5"},{"location":"triply-etl/changelog/#changed-new-default-engine-for-sparql-construct","text":"The default engine for evaluating SPARQL Construct queries (function construct() ) has changed from Comunica to Speedy. Speedy is a new SPARQL implementation that is developed by Triply; Comunica is an open source engine that is developed by the open source community. Since SPARQL is a standardized query language, this change should not cause a difference in behavior for your ETL pipelines. In the unexpected case where an ETL pipeline is negatively affected by this change, the old situation can be restored by explicitly configuring the Comunica engine: import { construct } from '@triplyetl/etl/sparql' construct(Source.TriplyDb.query('my-query'), { sparqlEngine: 'comunica' }), The benefit of switching to the Speedy engine is that this engine is expected to be faster for most queries. Overall, this change will therefore result in speed improvements for your TriplyETL pipelines.","title":"[Changed] New default engine for SPARQL Construct"},{"location":"triply-etl/changelog/#added-new-cli-tool-for-comparing-graphs","text":"The new CLI tool compare allows graph comparison to be performed from the command-line. This uses the same algorithm that is used by the compareGraphs() validator function.","title":"[Added] New CLI tool for comparing graphs"},{"location":"triply-etl/changelog/#bug-fixes","text":"This release includes the following bug fixes: fromXlsx() did not remove trailing whitespace in cell values. When a SHACL result was printed, an incorrect message about a faulty SHACL model would be shown. Some RDF processors did not handle empty RDF inputs correctly.","title":"Bug fixes"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-204","text":"Release date: 2023-05-11","title":"Changelog for TriplyETL 2.0.4"},{"location":"triply-etl/changelog/#enhanced-better-output-for-checking-graph-isomorphism","text":"Before this release, when two graphs were not isomorph and their difference consisted of a mapping from blank nodes onto blank nodes exclusively, an empty difference message was communicated. From this release onwards, the difference message is non-empty, and specifically indicates the difference between the non-isomorphic graphs. The following snippet should emit a non-empty difference: import { Etl, Source, compareGraphs, loadRdf } from \"@triplyetl/etl/generic\" export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf( Source.string('base <https://triply.cc/> <g> { []<p><o> }'), { contentType: 'application/trig' } ), compareGraphs( Source.string('base <https://triply.cc/> <g> { [] <p><o>. []<p><o>. }'), { contentType: 'application/trig' } ), ) return etl } Notice that the two TriG snippets are not isomorphic: graph LR _:1 -- p --> o. and graph LR _:2a -- p --> o. _:2b -- p --> o. It is possible to map _:2a and _:2b onto _:1 , but there is no mapping that works the other way round.","title":"[Enhanced] Better output for checking graph isomorphism"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-203","text":"Release date: 2023-05-10","title":"Changelog for TriplyETL 2.0.3"},{"location":"triply-etl/changelog/#bug-fixes_1","text":"This release includes the following bug fixes: Error location information is not shown in TriplyETL Runner. Issue when a URL data source ( Source.url() ) includes an HTTP body.","title":"Bug fixes"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-202","text":"Release date: 2023-05-09","title":"Changelog for TriplyETL 2.0.2"},{"location":"triply-etl/changelog/#bug-fixes_2","text":"This release fixes bugs related to the recent switch from CommonJS to ESM: Dynamic import bug on Windows. Error reporting issues due to ESM imports.","title":"Bug fixes"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-201","text":"Release date: 2023-05-03","title":"Changelog for TriplyETL 2.0.1"},{"location":"triply-etl/changelog/#added-timeout-flag-for-triplyetl-runner","text":"The TriplyETL Runner is the CLI tool that is used to run ETL pipelines. Starting with this version, you can specify a --timeout flag when using the TriplyETL Runner. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. See the TriplyETL Runner documentation page for more information.","title":"[Added] Timeout flag for TriplyETL Runner"},{"location":"triply-etl/changelog/#changelog-for-triplyetl-200","text":"Release date: 2023-05-01","title":"Changelog for TriplyETL 2.0.0"},{"location":"triply-etl/changelog/#changed-modules-infrastructure-moves-from-commonjs-to-esm","text":"Before this release, TriplyETL used CommonJS modules to modularize its functionality into different components. Starting in this release, ECMAScript Modules (ESM) are used to modularize TriplyETL functionality into different modules. ESM is a more modern approach for modularizing ECMAScript (JavaScript, TypeScript, and Node.js) code. While CommonJS imports are evaluated at runtime, ESM imports are evaluated at compile time. TriplyETL users benefit from this change, since error messages related to module imports will be detected much earlier in the development process. All documentation examples were update to use ESM syntax for module imports, for example: import { logRecord } from '@triplyetl/etl/debug'","title":"[Changed] Modules infrastructure moves from CommonJS to ESM"},{"location":"triply-etl/changelog/#changed-debug-functions-move-to-a-new-module","text":"Before this release, debug functions like logRecord() and startTrace() were part of the RATT module. Since debug functions are generic / not RATT-specific, they were moved into a new module. Function are imported from this new module in the following way: import { logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug'","title":"[Changed] Debug functions move to a new module"},{"location":"triply-etl/changelog/#enhanced-better-error-messages-when-things-go-wrong","text":"This release introduces a new approach for communicating errors back to the user. When TriplyETL functionality detects an error condition, a unified 'trace middleware' is now used to retrieve information from the environment in which the error occurred. This information is then printed to the error output stream for communication with the user.","title":"[Enhanced] Better error messages when things go wrong"},{"location":"triply-etl/changelog/#bug-fixes_3","text":"The following bug fixes are included in this release: Incorrect behavior of the _switch() control function . The fromOai() extractor now communicates clearer when the accessed OAI-PMH endpoint encounters any issues. When a key with a NULL value was accessed, the name of the key is now included in the error message.","title":"Bug fixes"},{"location":"triply-etl/changelog/#start-of-the-changelog","text":"TriplyETL 1.0.0 was released on 2023-03-20.","title":"Start of the changelog"},{"location":"triply-etl/cli/","text":"Command Line Interface (CLI) TriplyETL allows you to manually perform various tasks in a terminal application (a Command-Line Interface or CLI). Installing dependencies must be repeated when dependencies were changed. Transpiling to JavaScript must be repeated when one or more TypeScript files are changed. TriplyETL Runner allows you to manually run local TriplyETL projects in your terminal. TriplyETL Tools explains how you can perform common ETL tasks. Installing dependencies When you work on an existing TriplyETL project, you sometimes pull in changes made by your team members. Such changes are typically obtained by running the following Git command: git pull This command prints a list of files that were changed by your team members. If this list includes changes to the file package.json , this means that one or more dependencies were changed. In order to effectuate these changes in your local copy of the TriplyETL project, you must run the following command: npm i Transpiling to JavaScript When you make changes to one or more TypeScript files, the corresponding JavaScript files will have become outdated. If you now use the TriplyETL Runner , it will use one or more outdated JavaScript files, and will not take into account your most recent changes to the TypeScript files. In order to keep your JavaScript files up-to-date relative to your TypeScript files, you must run the following command after making changes to TypeScript files: npm run build If you edit your TypeScript files repeatedly, having to run this extra command may get tedious. In such cases, you can run the following command to automatically perform the transpile step in the background: npm run dev Notice that this prevents you from using the terminal application for new commands. It is typical to open a new terminal application window, and run the npx etl command from there. TriplyETL Runner The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. We assume that you have a local TriplyETL project in which you can successfully run the npx etl command. Follow the Getting Started instructions for TriplyETL Runner if this is not yet the case. Run the following command to run the ETL pipeline: npx etl This command implicitly uses the file lib/main.js , which is the transpiled JavaScript file that corresponds to the TypeScript file src/main.ts . The following command has the same behavior, but makes explicit which file is used: npx etl lib/main.js Some TriplyETL projects have multiple top-level scripts. In such cases, it is possible to run each of these scripts individually as follows: npx etl lib/some-script.js Output summary TriplyETL Runner will start processing data. Depending on the size of the data source, the Runner may take more or less time to finish. When the Runner finishes successfully, it will print the following summary: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Etl: #Error 0 | #Warning 0 | #Info 0 \u2502 \u2502 #Statements 2 \u2502 \u2502 #Records 2 \u2502 \u2502 Started at 2023-06-18 10:05:20 \u2502 \u2502 Runtime 0 sec \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 This summary includes the following information: - \"#Error\" shows the number of errors encountered. With default settings, this number is at most 1, since the Runner will immediately stop after an error occurs. - \"#Warning\" shows the number of warnings encountered. With default settings, this includes warnings emitted by the SHACL Validator . - \"#Info\" shows the number of informational messages. With default settings, this includes informational messages emitted by the SHACL Validator . - \"#Statements\" shows the number of triples or quads that was generated. This number is equal to or higher than the number of statements that is uploaded to the triple store. The reason for this is that TriplyETL processes records in parallel. If the same statement is generated for two records, the number of statements with be incremented by 2, but only 1 unique statement will be uploaded to the triple store. - \"#Records\" shows the number of records that was processed. - \"Started at\" shows the date and time at which the Runner started. - \"Runtime\" shows the wall time duration of the run. Limit the number of records When developing a pipeline, it is almost never necessary to process all records from the source data. Instead, it is common to run the ETL for a small number of example record, which results in quick feedback. The --head flag indicates the maximum number of records that is processed by the Runner: npx etl --head 1 npx etl --head 10 These commands run the ETL for the first record (if one is available) and for the first 10 records (if these are available). Specify a range of records When developing a pipeline over a large source data collection, it is often standard practice to use the first 10 or 100 records most of the time. The benefit of this approach is that the feedback loop between making changes and receiving feedback is short. A downside of this approach is that the ETL may be overly optimized towards these first few records. For example, if a value is missing in the first 1.000 records, then transformations that are necessary for when the value is present will not be developed initially. An alternative is to run the entire ETL, but that takes a long time. To avoid the downsides of using --head , TriplyETL also supports the --from-record-id flag. This flag specifies the number of records that are skipped. This allows us to specify an arbitrary consecutive range of records. For example, the following processes the 1.001-st until and including the 1.010-th record: npx etl --from-record-id 1000 --head 10 Process a specific record When the --head flag is set to 1, the --from-record-id flag specifies the index of a single specific record that is processed. This is useful when a record is known to be problematic, for instance during debugging. The following command runs TriplyETL for the 27th record: npx etl --from-record-id 26 --head 1 Set a timeout For large ETL pipelines, it is sometimes useful to specify a maximum duration for which the TriplyETL Runner is allowed to run. In such cases, the --timeout flag can be used. The --timeout option accepts human-readable duration strings, such as '1h 30m 5s', '1hr', '1 hour', or '3hrs'. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. As a result, the Runner will upload all linked data (graphs) that was produced up to that point, and it will write a performance log. For TriplyETLs that run in a CI/CD environment, the timeout must be set lower than the CI/CD timeout, in order for the Runner to be able to perform the termination step. Verbose mode When TriplyETL is run normally, the following information is displayed: The number of added triples. The runtime of the script. An error message, if any occurred. It is possible to also show the following additional information by specifying the --verbose flag: In case of an error, the first 20 values from the last processed record. In case of an error, the full stack trace. The following example shows how the --verbose flag can be used: npx etl --verbose Secure verbose mode Verbose mode may perform a reset of your current terminal session. If this happens you lose visible access to the commands that were run prior to the last TriplyETL invocation. This destructive behavior of verbose mode can be disabled by setting the following environment variable: export CI=true This fixes the reset issue, but also makes the output less colorful. TriplyETL Tools TriplyETL Tools is a collection of small tools that can be used to run isolated tasks from your terminal application. TriplyETL Tools can be used when you are inside a TriplyETL project. If you do not have an ETL project yet, use the TriplyETL Generator first to create one. The following command prints an overview of the supported tools: npx tools The following tools are supported: Tool Description compare Compare the contents of two RDF files create-token Create a new TriplyDB API Token | print-token | Print the currently set TriplyDB API Token, if any | | validate | Validate a data file against a SHACL shapes file | For each tool, the following command prints more information on how to use it: npx tools {name} --help Compare The compare tool checks whether two RDF files encode the same linked data: - If the two files contain the same data, the command succeeds and does not print any output. - If the two files do not contain the same data, the command exits with an error code, and the difference between the two files is printed. The compare tools is invoked over the two RDF files one.ttl and two.ttl as follows: npx tools compare one.ttl two.ttl This tool can be used to compare two RDF files that contain multiple graphs, for example: npx tools compare one.trig two.trig This tool uses the graph isomorphism property as defined in the RDF 1.1 standard: link Create TriplyDB API Token This tool creates a new TriplyDB API Token from the command-line. This command can be used as follows: npx tools create-token The command will ask a couple of questions in order to create the TriplyDB API Token: - The hostname of the TriplyDB instance - The name of the token - Your TriplyDB account e-mail - Your TriplyDB account password The command exists in case a TriplyDB API Token is already configured. Print TriplyDB API Token This tool prints the currently configured TriplyDB API Token, if any. This command can be used as follows: npx tools print-token This command is useful when there are issues with configuring a TriplyDB API Token. Validate This tool validates the content of one data file against the SHACL shapes in another file. The resulting SHACL validation report is printed to standard output. The command can be used as follows: $ npx tools validate -d data.trig -s model.trig See this section to learn more about the SHACL validation report.","title":"TriplyETL: Command-Line Interface (CLI)"},{"location":"triply-etl/cli/#command-line-interface-cli","text":"TriplyETL allows you to manually perform various tasks in a terminal application (a Command-Line Interface or CLI). Installing dependencies must be repeated when dependencies were changed. Transpiling to JavaScript must be repeated when one or more TypeScript files are changed. TriplyETL Runner allows you to manually run local TriplyETL projects in your terminal. TriplyETL Tools explains how you can perform common ETL tasks.","title":"Command Line Interface (CLI)"},{"location":"triply-etl/cli/#installing-dependencies","text":"When you work on an existing TriplyETL project, you sometimes pull in changes made by your team members. Such changes are typically obtained by running the following Git command: git pull This command prints a list of files that were changed by your team members. If this list includes changes to the file package.json , this means that one or more dependencies were changed. In order to effectuate these changes in your local copy of the TriplyETL project, you must run the following command: npm i","title":"Installing dependencies"},{"location":"triply-etl/cli/#transpiling-to-javascript","text":"When you make changes to one or more TypeScript files, the corresponding JavaScript files will have become outdated. If you now use the TriplyETL Runner , it will use one or more outdated JavaScript files, and will not take into account your most recent changes to the TypeScript files. In order to keep your JavaScript files up-to-date relative to your TypeScript files, you must run the following command after making changes to TypeScript files: npm run build If you edit your TypeScript files repeatedly, having to run this extra command may get tedious. In such cases, you can run the following command to automatically perform the transpile step in the background: npm run dev Notice that this prevents you from using the terminal application for new commands. It is typical to open a new terminal application window, and run the npx etl command from there.","title":"Transpiling to JavaScript"},{"location":"triply-etl/cli/#triplyetl-runner","text":"The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. We assume that you have a local TriplyETL project in which you can successfully run the npx etl command. Follow the Getting Started instructions for TriplyETL Runner if this is not yet the case. Run the following command to run the ETL pipeline: npx etl This command implicitly uses the file lib/main.js , which is the transpiled JavaScript file that corresponds to the TypeScript file src/main.ts . The following command has the same behavior, but makes explicit which file is used: npx etl lib/main.js Some TriplyETL projects have multiple top-level scripts. In such cases, it is possible to run each of these scripts individually as follows: npx etl lib/some-script.js","title":"TriplyETL Runner"},{"location":"triply-etl/cli/#output-summary","text":"TriplyETL Runner will start processing data. Depending on the size of the data source, the Runner may take more or less time to finish. When the Runner finishes successfully, it will print the following summary: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Etl: #Error 0 | #Warning 0 | #Info 0 \u2502 \u2502 #Statements 2 \u2502 \u2502 #Records 2 \u2502 \u2502 Started at 2023-06-18 10:05:20 \u2502 \u2502 Runtime 0 sec \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 This summary includes the following information: - \"#Error\" shows the number of errors encountered. With default settings, this number is at most 1, since the Runner will immediately stop after an error occurs. - \"#Warning\" shows the number of warnings encountered. With default settings, this includes warnings emitted by the SHACL Validator . - \"#Info\" shows the number of informational messages. With default settings, this includes informational messages emitted by the SHACL Validator . - \"#Statements\" shows the number of triples or quads that was generated. This number is equal to or higher than the number of statements that is uploaded to the triple store. The reason for this is that TriplyETL processes records in parallel. If the same statement is generated for two records, the number of statements with be incremented by 2, but only 1 unique statement will be uploaded to the triple store. - \"#Records\" shows the number of records that was processed. - \"Started at\" shows the date and time at which the Runner started. - \"Runtime\" shows the wall time duration of the run.","title":"Output summary"},{"location":"triply-etl/cli/#limit-the-number-of-records","text":"When developing a pipeline, it is almost never necessary to process all records from the source data. Instead, it is common to run the ETL for a small number of example record, which results in quick feedback. The --head flag indicates the maximum number of records that is processed by the Runner: npx etl --head 1 npx etl --head 10 These commands run the ETL for the first record (if one is available) and for the first 10 records (if these are available).","title":"Limit the number of records"},{"location":"triply-etl/cli/#specify-a-range-of-records","text":"When developing a pipeline over a large source data collection, it is often standard practice to use the first 10 or 100 records most of the time. The benefit of this approach is that the feedback loop between making changes and receiving feedback is short. A downside of this approach is that the ETL may be overly optimized towards these first few records. For example, if a value is missing in the first 1.000 records, then transformations that are necessary for when the value is present will not be developed initially. An alternative is to run the entire ETL, but that takes a long time. To avoid the downsides of using --head , TriplyETL also supports the --from-record-id flag. This flag specifies the number of records that are skipped. This allows us to specify an arbitrary consecutive range of records. For example, the following processes the 1.001-st until and including the 1.010-th record: npx etl --from-record-id 1000 --head 10","title":"Specify a range of records"},{"location":"triply-etl/cli/#process-a-specific-record","text":"When the --head flag is set to 1, the --from-record-id flag specifies the index of a single specific record that is processed. This is useful when a record is known to be problematic, for instance during debugging. The following command runs TriplyETL for the 27th record: npx etl --from-record-id 26 --head 1","title":"Process a specific record"},{"location":"triply-etl/cli/#set-a-timeout","text":"For large ETL pipelines, it is sometimes useful to specify a maximum duration for which the TriplyETL Runner is allowed to run. In such cases, the --timeout flag can be used. The --timeout option accepts human-readable duration strings, such as '1h 30m 5s', '1hr', '1 hour', or '3hrs'. When the indicated timeout is reached before the pipeline finishes, the TriplyETL Runner will gracefully terminate the ETL by acting as if there are no more incoming records. As a result, the Runner will upload all linked data (graphs) that was produced up to that point, and it will write a performance log. For TriplyETLs that run in a CI/CD environment, the timeout must be set lower than the CI/CD timeout, in order for the Runner to be able to perform the termination step.","title":"Set a timeout"},{"location":"triply-etl/cli/#verbose-mode","text":"When TriplyETL is run normally, the following information is displayed: The number of added triples. The runtime of the script. An error message, if any occurred. It is possible to also show the following additional information by specifying the --verbose flag: In case of an error, the first 20 values from the last processed record. In case of an error, the full stack trace. The following example shows how the --verbose flag can be used: npx etl --verbose","title":"Verbose mode"},{"location":"triply-etl/cli/#secure-verbose-mode","text":"Verbose mode may perform a reset of your current terminal session. If this happens you lose visible access to the commands that were run prior to the last TriplyETL invocation. This destructive behavior of verbose mode can be disabled by setting the following environment variable: export CI=true This fixes the reset issue, but also makes the output less colorful.","title":"Secure verbose mode"},{"location":"triply-etl/cli/#triplyetl-tools","text":"TriplyETL Tools is a collection of small tools that can be used to run isolated tasks from your terminal application. TriplyETL Tools can be used when you are inside a TriplyETL project. If you do not have an ETL project yet, use the TriplyETL Generator first to create one. The following command prints an overview of the supported tools: npx tools The following tools are supported: Tool Description compare Compare the contents of two RDF files create-token Create a new TriplyDB API Token | print-token | Print the currently set TriplyDB API Token, if any | | validate | Validate a data file against a SHACL shapes file | For each tool, the following command prints more information on how to use it: npx tools {name} --help","title":"TriplyETL Tools"},{"location":"triply-etl/cli/#compare","text":"The compare tool checks whether two RDF files encode the same linked data: - If the two files contain the same data, the command succeeds and does not print any output. - If the two files do not contain the same data, the command exits with an error code, and the difference between the two files is printed. The compare tools is invoked over the two RDF files one.ttl and two.ttl as follows: npx tools compare one.ttl two.ttl This tool can be used to compare two RDF files that contain multiple graphs, for example: npx tools compare one.trig two.trig This tool uses the graph isomorphism property as defined in the RDF 1.1 standard: link","title":"Compare"},{"location":"triply-etl/cli/#create-triplydb-api-token","text":"This tool creates a new TriplyDB API Token from the command-line. This command can be used as follows: npx tools create-token The command will ask a couple of questions in order to create the TriplyDB API Token: - The hostname of the TriplyDB instance - The name of the token - Your TriplyDB account e-mail - Your TriplyDB account password The command exists in case a TriplyDB API Token is already configured.","title":"Create TriplyDB API Token"},{"location":"triply-etl/cli/#print-triplydb-api-token","text":"This tool prints the currently configured TriplyDB API Token, if any. This command can be used as follows: npx tools print-token This command is useful when there are issues with configuring a TriplyDB API Token.","title":"Print TriplyDB API Token"},{"location":"triply-etl/cli/#validate","text":"This tool validates the content of one data file against the SHACL shapes in another file. The resulting SHACL validation report is printed to standard output. The command can be used as follows: $ npx tools validate -d data.trig -s model.trig See this section to learn more about the SHACL validation report.","title":"Validate"},{"location":"triply-etl/control/","text":"This page documents how you can use control structures in your ETL configuration. Process data conditionally ( when() ) Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values that denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when() function allows part of a TriplyETL configuration to run when certain conditions are met. The first parameter is used to determine whether or not the remaining parameters should be called: when('{condition}', '{statement-1}', '{statement-2}', '{statement-3}', // etc ), Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value. Missing values If a value is sometimes completely missing from a source data record, the when() conditional function can be used. The following code snippet assets a triple if and only if a value for the 'zipcode' key is present in the Record: when(context => context.isNotEmpty('zipcode'), triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ), Since checking for the presence or absence of a single record is very common, the above can also be written as follows: when('zipcode', triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ), The empty string In many source data formats, the empty string is used to signify a missing value, this particular string is treated in a special way by when() . A key whose value is the empty string is treated in the same way as a key that is altogether absent. The following code snippet will not print the record to standard output, because the 'zipcode' key is considered empty: fromJson([{ zipcode: '' }]), when('zipcode', logRecord(), ), Notice that it is almost never useful to store the empty string in linked data. So the treatment of empty strings as NULL values is the correct default behavior. NULL values ( when() and whenNotEqual() ) If a key contains specific values that are indended to represent NULL values, then these must be specifically identified the first when() parameter. The following code snippet identifies the value 9999 for the 'created' key as denoting a NULL values. This means that the year 9999 is used in the source system whenever the actual year of creation was unknown. when(context => context.getNumber('created') != 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Since checking the value of one specific key is very common, the above can be written as follows, using the more specific whenNotEqual function: whenNotEqual('created', 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Notice that the use of whenNotEqual() makes the configuration easier to read. The same shorthand notation works when there are multiple NULL values in the source data. The following code snippet only asserts a triple if the year of creation is neither 9999 nor -1. Notice that the array can contain any number of potential NULL values: whenNotEqual('created', [-1, 9999], triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Iterating over lists of objects ( forEach() ) In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want to make an assertion for every element in a list. TriplyETL provides the forEach() function for this purpose. The following code snippet asserts the name for each country in the example data: forEach('data.countries', triple(iri(prefix.id, 'id'), rdfs.label, 'name'), ), Notice the following details: - forEach() uses the path expression 'data.countries' to identify the list. - Inside the forEach() function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'. country:de rdfs:label 'Germany'. Notice that forEach() only works for lists whose elements areobjects*. See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach() iterates over are themselves (sub)records. This implies that all functions that work for full records also work for the (sub)records inside forEach() . The (sub)records inside an forEach() function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, (sub)records inside forEach() also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root ) Index key ( $index ) {#index-key} Each (sub)record that is made available in forEach() contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" } ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: forEach('countries', triple(iri(prefix.id, '$index'), rdfs.label, 'name'), ), This results in the following assertions: country:0 rdfs:label 'The Netherlands'. country:1 rdfs:label 'Germany'. country:2 rdfs:label 'Italy'. Parent key ( $parent ) {#parent-key} When forEach() iterates through a list of elements, it makes the enclosingparent* record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach() . For example, the parent record in the following call is the record that directly contains the \"data\" key: forEach('data.countries', // etc ), The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: forEach('data.countries', logRecord(), ), For our example source data, this emits the following 2 records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section . Root key ( $root ) {#root-key} Sometimes it may be necessary to access a part of the original record that is outside of the scope of the forEach() call. Every (sub)record inside a forEach() call contains the \"$root\" key. The value of the root key provides a link to the full record. Because the $root key is part of the linked-to record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach() calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach() call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: forEach('data.countries', forEach('labels', logRecord(), ), ), The following record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" } Specify multiple conditions ( ifElse() ) The ifElse() function in TriplyETL allows us to specify multiple conditions based on which other functions are run. Every condition is specified with an if key. In case the condition is true, the functions specified in the then key are run. If none of the if conditions are true, the functions specified in an else key, if present, are run. Parameters The first parameter must be an { if: ..., then: ... } object. The non-first parameters are either additional { if: ..., then: ... } objects or a final { else: ... } object. Each if key specifies a condition that is either true or false. Conditions are either a key name or a function that takes the Etl Context and returns a Boolean value. Specifying a key name is identical to specifying the following function: ctx => ctx.getString('KEY') The then and else keys take either one function, or an array of zero or more functions. Example 1 The following code snippet uses different conditions to determine the age category that a person belongs to: fromJson([ { id: 'johndoe', age: 12 }, { id: 'janedoe', age: 32 }, // ... ]), addIri({ prefix: prefix.person, content: 'id', key: '_person', }), ifElse({ if: ctx => ctx.getNumber('age') < 12, then: triple('_person', a, def.Child), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 12 && age < 20 }, then: triple('_person', a, def.Teenager), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 20 && age < 65 }, then: triple('_person', a, def.Adult), }, { else: triple('_person', a, def.Senior), }), Example 2 The following snippet either asserts data about persons or data about organizations, and uses an ifElse to make the conditional determination on which assertion to make: fromJson([ { first: 'John', last: 'Doe' }, { name: 'Triply' }, ]), ifElse({ if: 'name', then: pairs(iri(prefix.id, 'name'), [a, sdo.Organization], [sdo.name, 'name'], ), }, { else: [ concat({ content: ['first', 'last'], separator: '-', key: 'name', }), pairs(iri(prefix.id, 'name'), [a, sdo.Person], [sdo.givenName, 'first'], [sdo.familyName, 'last'], ), ], }), Switch between different cases ( _switch() ) The function _switch() allows us to switch between different cases, based on the value of a specified key. Parameters key The key parameter whose value is compared against the specified values. cases One or more cases. Each case is represented by a pair. The first element of the pair is the value that is checked for equivalence with the value in key . The second element is either one function or a list of functions. Whenever the value in key is equal to the value in one of the cases, the corresponding function(s) are run. Notice that we must write _switch() because switch is a reserved keyword. An error is emitted if the value for key does not match any of the cases. Example 1 When an ETL uses multiple data sources, we can use a _switch() to run a dedicated sub-ETL for each data source. Suppose we have two tabular data sources: file.episodes and file.people . We can use the following _switch() statement to run different sub-ETLs: _switch(key.fileName, [file.episodes, etl_episodes], [file.people, etl_people], ), Example 2 When ETLs transform different kinds of entities, it can be useful to run a sub-ETL based on the type of entity. For example, if the current Etl Record represents a person, we want to assert their age. But if the current Etl Record represents a location, we want to assert its latitude and longitude: const etl_location = [ triple('iri', sdo.latitude, literal('lat', xsd.double)), triple('iri', sdo.longitude, literal('long', xsd.double)), ] const etl_person = [ triple('iri', sdo.age, literal('age', xsd.nonNegativeInteger)), ] etl.run( _switch('type', ['location', etl_location], ['person', etl_person], ), )","title":"TriplyETL: Control Structures"},{"location":"triply-etl/control/#process-data-conditionally-when","text":"Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values that denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when() function allows part of a TriplyETL configuration to run when certain conditions are met. The first parameter is used to determine whether or not the remaining parameters should be called: when('{condition}', '{statement-1}', '{statement-2}', '{statement-3}', // etc ), Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value.","title":"Process data conditionally (when())"},{"location":"triply-etl/control/#missing-values","text":"If a value is sometimes completely missing from a source data record, the when() conditional function can be used. The following code snippet assets a triple if and only if a value for the 'zipcode' key is present in the Record: when(context => context.isNotEmpty('zipcode'), triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ), Since checking for the presence or absence of a single record is very common, the above can also be written as follows: when('zipcode', triple(iri(prefix.id, 'id'), def.zipcode, 'zipcode'), ),","title":"Missing values"},{"location":"triply-etl/control/#the-empty-string","text":"In many source data formats, the empty string is used to signify a missing value, this particular string is treated in a special way by when() . A key whose value is the empty string is treated in the same way as a key that is altogether absent. The following code snippet will not print the record to standard output, because the 'zipcode' key is considered empty: fromJson([{ zipcode: '' }]), when('zipcode', logRecord(), ), Notice that it is almost never useful to store the empty string in linked data. So the treatment of empty strings as NULL values is the correct default behavior.","title":"The empty string"},{"location":"triply-etl/control/#null-values-when-and-whennotequal","text":"If a key contains specific values that are indended to represent NULL values, then these must be specifically identified the first when() parameter. The following code snippet identifies the value 9999 for the 'created' key as denoting a NULL values. This means that the year 9999 is used in the source system whenever the actual year of creation was unknown. when(context => context.getNumber('created') != 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Since checking the value of one specific key is very common, the above can be written as follows, using the more specific whenNotEqual function: whenNotEqual('created', 9999, triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ), Notice that the use of whenNotEqual() makes the configuration easier to read. The same shorthand notation works when there are multiple NULL values in the source data. The following code snippet only asserts a triple if the year of creation is neither 9999 nor -1. Notice that the array can contain any number of potential NULL values: whenNotEqual('created', [-1, 9999], triple(iri(prefix.id, 'id'), dct.created, literal('created', xsd.gYear)), ),","title":"NULL values (when() and whenNotEqual())"},{"location":"triply-etl/control/#iterating-over-lists-of-objects-foreach","text":"In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want to make an assertion for every element in a list. TriplyETL provides the forEach() function for this purpose. The following code snippet asserts the name for each country in the example data: forEach('data.countries', triple(iri(prefix.id, 'id'), rdfs.label, 'name'), ), Notice the following details: - forEach() uses the path expression 'data.countries' to identify the list. - Inside the forEach() function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'. country:de rdfs:label 'Germany'. Notice that forEach() only works for lists whose elements areobjects*. See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach() iterates over are themselves (sub)records. This implies that all functions that work for full records also work for the (sub)records inside forEach() . The (sub)records inside an forEach() function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, (sub)records inside forEach() also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root )","title":"Iterating over lists of objects (forEach())"},{"location":"triply-etl/control/#index-key-index-index-key","text":"Each (sub)record that is made available in forEach() contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" } ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: forEach('countries', triple(iri(prefix.id, '$index'), rdfs.label, 'name'), ), This results in the following assertions: country:0 rdfs:label 'The Netherlands'. country:1 rdfs:label 'Germany'. country:2 rdfs:label 'Italy'.","title":"Index key ($index) {#index-key}"},{"location":"triply-etl/control/#parent-key-parent-parent-key","text":"When forEach() iterates through a list of elements, it makes the enclosingparent* record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach() . For example, the parent record in the following call is the record that directly contains the \"data\" key: forEach('data.countries', // etc ), The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: forEach('data.countries', logRecord(), ), For our example source data, this emits the following 2 records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section .","title":"Parent key ($parent) {#parent-key}"},{"location":"triply-etl/control/#root-key-root-root-key","text":"Sometimes it may be necessary to access a part of the original record that is outside of the scope of the forEach() call. Every (sub)record inside a forEach() call contains the \"$root\" key. The value of the root key provides a link to the full record. Because the $root key is part of the linked-to record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach() calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach() call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: forEach('data.countries', forEach('labels', logRecord(), ), ), The following record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" }","title":"Root key ($root) {#root-key}"},{"location":"triply-etl/control/#specify-multiple-conditions-ifelse","text":"The ifElse() function in TriplyETL allows us to specify multiple conditions based on which other functions are run. Every condition is specified with an if key. In case the condition is true, the functions specified in the then key are run. If none of the if conditions are true, the functions specified in an else key, if present, are run.","title":"Specify multiple conditions (ifElse())"},{"location":"triply-etl/control/#parameters","text":"The first parameter must be an { if: ..., then: ... } object. The non-first parameters are either additional { if: ..., then: ... } objects or a final { else: ... } object. Each if key specifies a condition that is either true or false. Conditions are either a key name or a function that takes the Etl Context and returns a Boolean value. Specifying a key name is identical to specifying the following function: ctx => ctx.getString('KEY') The then and else keys take either one function, or an array of zero or more functions.","title":"Parameters"},{"location":"triply-etl/control/#example-1","text":"The following code snippet uses different conditions to determine the age category that a person belongs to: fromJson([ { id: 'johndoe', age: 12 }, { id: 'janedoe', age: 32 }, // ... ]), addIri({ prefix: prefix.person, content: 'id', key: '_person', }), ifElse({ if: ctx => ctx.getNumber('age') < 12, then: triple('_person', a, def.Child), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 12 && age < 20 }, then: triple('_person', a, def.Teenager), }, { if: ctx => { const age = ctx.getNumber('age') return age >= 20 && age < 65 }, then: triple('_person', a, def.Adult), }, { else: triple('_person', a, def.Senior), }),","title":"Example 1"},{"location":"triply-etl/control/#example-2","text":"The following snippet either asserts data about persons or data about organizations, and uses an ifElse to make the conditional determination on which assertion to make: fromJson([ { first: 'John', last: 'Doe' }, { name: 'Triply' }, ]), ifElse({ if: 'name', then: pairs(iri(prefix.id, 'name'), [a, sdo.Organization], [sdo.name, 'name'], ), }, { else: [ concat({ content: ['first', 'last'], separator: '-', key: 'name', }), pairs(iri(prefix.id, 'name'), [a, sdo.Person], [sdo.givenName, 'first'], [sdo.familyName, 'last'], ), ], }),","title":"Example 2"},{"location":"triply-etl/control/#switch-between-different-cases-_switch","text":"The function _switch() allows us to switch between different cases, based on the value of a specified key.","title":"Switch between different cases (_switch())"},{"location":"triply-etl/control/#parameters_1","text":"key The key parameter whose value is compared against the specified values. cases One or more cases. Each case is represented by a pair. The first element of the pair is the value that is checked for equivalence with the value in key . The second element is either one function or a list of functions. Whenever the value in key is equal to the value in one of the cases, the corresponding function(s) are run. Notice that we must write _switch() because switch is a reserved keyword. An error is emitted if the value for key does not match any of the cases.","title":"Parameters"},{"location":"triply-etl/control/#example-1_1","text":"When an ETL uses multiple data sources, we can use a _switch() to run a dedicated sub-ETL for each data source. Suppose we have two tabular data sources: file.episodes and file.people . We can use the following _switch() statement to run different sub-ETLs: _switch(key.fileName, [file.episodes, etl_episodes], [file.people, etl_people], ),","title":"Example 1"},{"location":"triply-etl/control/#example-2_1","text":"When ETLs transform different kinds of entities, it can be useful to run a sub-ETL based on the type of entity. For example, if the current Etl Record represents a person, we want to assert their age. But if the current Etl Record represents a location, we want to assert its latitude and longitude: const etl_location = [ triple('iri', sdo.latitude, literal('lat', xsd.double)), triple('iri', sdo.longitude, literal('long', xsd.double)), ] const etl_person = [ triple('iri', sdo.age, literal('age', xsd.nonNegativeInteger)), ] etl.run( _switch('type', ['location', etl_location], ['person', etl_person], ), )","title":"Example 2"},{"location":"triply-etl/debug/","text":"TriplyETL includes functions that can be used during debugging. These debug function allow you to inspect in a detailed way how data flows through your pipeline. This allows you to find problems more quickly, and allows you to determine whether data is handled correctly by your TriplyETL configuration. Overview The following debug function are available: Function Description logMemory() Prints the current memory consumption. logQuads() Prints the contents of the internal store to standard output. logQuery() Prints a query string to standard output. logRecord() Prints the record in its current state to standard output. traceEnd() Ends a trace of the record and internal store. traceStart() Starts a trace of the record and internal store. These functions can be imported from the debug module: import { logMemory, logQuads, logQuery, logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug' Function logMemory() {#logMemory} This function prints information about the current memory consumption. It includes the following fields: | Field name | Meaning | Use case | | --- | --- | | CallCount | The number of times that a specific use of logMemory() can been invoked. | Find a location in your ETL script that is visited many times, e.g. because it occurs inside a (nested) loop. | | RecordId | The numeric identifier of the record that is currently processed. | Find a specific record that causes memory consumption to increase. | | Heap used | The number of megabytes that are currently used on the heap. | Find places in your ETL where an unexpected amount of memory is used. | | Heap total | The number of megabytes that are currently allocated on the heap. | Find places in your ETL where memory reallocation occurs. | The following code snippet prints the memory consumption of TriplyETL for each record (first call), and for each member of key 'a' (second call): fromJson([{ a: [{ b: 1 }, { b: 2 }] }, { a: [] }, { a: [] }]), logMemory(), forEach('a', logMemory()), This prints the following messages to standard output: Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 2 | Heap (MB) used: 92 / total: 122 Info CallCount: 3 | RecordId: 2 | Heap (MB) used: 92 / total: 122 Function logQuads() {#logQuads} This function prints the current contents of the internal store to standard output. The following snippet asserts one triple into the default graph of the internal store, and then prints the contents of the internal store: fromJson([{}]), triple(rdfs.Class, a, rdfs.Class), logQuads(), This results in the following output: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix sdo: <https://schema.org/>. <https://triplydb.com/graph/default> { rdfs:Class a rdfs:Class } Function logQuery() {#logQuery} This function prints a query string to standard output. This is specifically useful when the query string is stored in an external system, e.g. a SPARQL query string that is stored on a TriplyDB server: logQuery(Source.TriplyDb.query('my-account', 'my-query')), Depending on the query string that is stored in 'my-query' , this could result in the following output: select * { ?s ?p ?o. } limit 10 Function logRecord() {#logRecord} This function prints the current state of the record to standard output. The record is a generic representation of the data that is extracted from one of the data sources (see the Record documentation page for more information). The following snippet prints the inline JSON record to standard output: fromJson([{ a: 1 }]), logRecord(), This emits the following: { \"a\": 1, \"$recordId\": 1, \"$environment\": \"Development\" } Use when writing a new ETL When writing a new ETL, logRecord() is often used as the first function to invoke immediately after extracting the record. For example: fromJson(Source.url('https://example.com/some/api/call')), logRecord(), Since this prints a full overview of what is available in the data source, this forms a good starting point for writing the rest of the ETL configurations. Observe the effects of transformations Another common use case for logRecord() is to observe the record at different moments in time. This is specifically useful to observe the effects of transformation functions , since these are the functions that modify the record. The following snippet logs the record directly before and directly after the transformation function split() is called. fromJson([{ a: '1, 2, 3' }]), logRecord(), split({ content: 'a', separator: ',', key: 'b' }), logRecord(), This makes it easy to observe the result of applying the transformation function: Running lib/main.js { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\", \"b\": [ \"1\", \"2\", \"3\" ] } Log a specific key Since records can be quite long, in some cases it may be easier to print only a specific key. The following code snippet only prints the key that was added by the transformation function: fromJson([{ a: '1, 2, 3' }]), split({ content: 'a', separator: ',', key: 'b' }), logRecord({ key: 'b' }), This results in the following output: [ \"1\", \"2\", \"3\" ] Functions traceStart() and traceEnd() {#trace} Sometimes you are interested to find one specific record based on a certain value of a key and/or to see the changes in this record made by specific middlewares. For these purposes, trace middleware can be used. Below, there is an example of how this middleware can be used: fromJson([ { a: 1, b: 1 }, // first dummy record { a: 2, b: 2 }, // second dummy record ]), change({key:'a', type:'number', change: (val) => val +100}), // change the 'a' key traceStart(), change({key:'b', type:'number', change: (val) => val +100}), // change the 'b' key traceEnd(), The result would be: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Record trace information \u2502 \u2502 { \u2502 \u2502 \"a\": 101, \u2502 \u2502 \"b\": 1 \u2502 \u2502 \"b\": 101 \u2502 \u2502 } \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Quads trace information (unchanged) \u2502 \u2502 empty \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 To rerun the traced middlewares for this record use the following command: > npx etl lib/{script-name} --trace .trace-1650542307095 In your terminal the line with \"b\": 1 will be red colored, showing the previous state of this key-value and the line with \"b\": 101 will be green colored, showing the new state. Also you can rerun the trace information for this specific record by running: npx etl lib/{script-name} --trace .trace-1650542307095","title":"TriplyETL: Debug"},{"location":"triply-etl/debug/#overview","text":"The following debug function are available: Function Description logMemory() Prints the current memory consumption. logQuads() Prints the contents of the internal store to standard output. logQuery() Prints a query string to standard output. logRecord() Prints the record in its current state to standard output. traceEnd() Ends a trace of the record and internal store. traceStart() Starts a trace of the record and internal store. These functions can be imported from the debug module: import { logMemory, logQuads, logQuery, logRecord, traceEnd, traceStart } from '@triplyetl/etl/debug'","title":"Overview"},{"location":"triply-etl/debug/#function-logmemory-logmemory","text":"This function prints information about the current memory consumption. It includes the following fields: | Field name | Meaning | Use case | | --- | --- | | CallCount | The number of times that a specific use of logMemory() can been invoked. | Find a location in your ETL script that is visited many times, e.g. because it occurs inside a (nested) loop. | | RecordId | The numeric identifier of the record that is currently processed. | Find a specific record that causes memory consumption to increase. | | Heap used | The number of megabytes that are currently used on the heap. | Find places in your ETL where an unexpected amount of memory is used. | | Heap total | The number of megabytes that are currently allocated on the heap. | Find places in your ETL where memory reallocation occurs. | The following code snippet prints the memory consumption of TriplyETL for each record (first call), and for each member of key 'a' (second call): fromJson([{ a: [{ b: 1 }, { b: 2 }] }, { a: [] }, { a: [] }]), logMemory(), forEach('a', logMemory()), This prints the following messages to standard output: Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 1 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 1 | Heap (MB) used: 92 / total: 122 Info CallCount: 2 | RecordId: 2 | Heap (MB) used: 92 / total: 122 Info CallCount: 3 | RecordId: 2 | Heap (MB) used: 92 / total: 122","title":"Function logMemory() {#logMemory}"},{"location":"triply-etl/debug/#function-logquads-logquads","text":"This function prints the current contents of the internal store to standard output. The following snippet asserts one triple into the default graph of the internal store, and then prints the contents of the internal store: fromJson([{}]), triple(rdfs.Class, a, rdfs.Class), logQuads(), This results in the following output: @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>. @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>. @prefix sdo: <https://schema.org/>. <https://triplydb.com/graph/default> { rdfs:Class a rdfs:Class }","title":"Function logQuads() {#logQuads}"},{"location":"triply-etl/debug/#function-logquery-logquery","text":"This function prints a query string to standard output. This is specifically useful when the query string is stored in an external system, e.g. a SPARQL query string that is stored on a TriplyDB server: logQuery(Source.TriplyDb.query('my-account', 'my-query')), Depending on the query string that is stored in 'my-query' , this could result in the following output: select * { ?s ?p ?o. } limit 10","title":"Function logQuery() {#logQuery}"},{"location":"triply-etl/debug/#function-logrecord-logrecord","text":"This function prints the current state of the record to standard output. The record is a generic representation of the data that is extracted from one of the data sources (see the Record documentation page for more information). The following snippet prints the inline JSON record to standard output: fromJson([{ a: 1 }]), logRecord(), This emits the following: { \"a\": 1, \"$recordId\": 1, \"$environment\": \"Development\" }","title":"Function logRecord() {#logRecord}"},{"location":"triply-etl/debug/#use-when-writing-a-new-etl","text":"When writing a new ETL, logRecord() is often used as the first function to invoke immediately after extracting the record. For example: fromJson(Source.url('https://example.com/some/api/call')), logRecord(), Since this prints a full overview of what is available in the data source, this forms a good starting point for writing the rest of the ETL configurations.","title":"Use when writing a new ETL"},{"location":"triply-etl/debug/#observe-the-effects-of-transformations","text":"Another common use case for logRecord() is to observe the record at different moments in time. This is specifically useful to observe the effects of transformation functions , since these are the functions that modify the record. The following snippet logs the record directly before and directly after the transformation function split() is called. fromJson([{ a: '1, 2, 3' }]), logRecord(), split({ content: 'a', separator: ',', key: 'b' }), logRecord(), This makes it easy to observe the result of applying the transformation function: Running lib/main.js { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"a\": \"1, 2, 3\", \"$recordId\": 1, \"$environment\": \"Development\", \"b\": [ \"1\", \"2\", \"3\" ] }","title":"Observe the effects of transformations"},{"location":"triply-etl/debug/#log-a-specific-key","text":"Since records can be quite long, in some cases it may be easier to print only a specific key. The following code snippet only prints the key that was added by the transformation function: fromJson([{ a: '1, 2, 3' }]), split({ content: 'a', separator: ',', key: 'b' }), logRecord({ key: 'b' }), This results in the following output: [ \"1\", \"2\", \"3\" ]","title":"Log a specific key"},{"location":"triply-etl/debug/#functions-tracestart-and-traceend-trace","text":"Sometimes you are interested to find one specific record based on a certain value of a key and/or to see the changes in this record made by specific middlewares. For these purposes, trace middleware can be used. Below, there is an example of how this middleware can be used: fromJson([ { a: 1, b: 1 }, // first dummy record { a: 2, b: 2 }, // second dummy record ]), change({key:'a', type:'number', change: (val) => val +100}), // change the 'a' key traceStart(), change({key:'b', type:'number', change: (val) => val +100}), // change the 'b' key traceEnd(), The result would be: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Record trace information \u2502 \u2502 { \u2502 \u2502 \"a\": 101, \u2502 \u2502 \"b\": 1 \u2502 \u2502 \"b\": 101 \u2502 \u2502 } \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Quads trace information (unchanged) \u2502 \u2502 empty \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 To rerun the traced middlewares for this record use the following command: > npx etl lib/{script-name} --trace .trace-1650542307095 In your terminal the line with \"b\": 1 will be red colored, showing the previous state of this key-value and the line with \"b\": 101 will be green colored, showing the new state. Also you can rerun the trace information for this specific record by running: npx etl lib/{script-name} --trace .trace-1650542307095","title":"Functions traceStart() and traceEnd() {#trace}"},{"location":"triply-etl/declare/","text":"This page documents how you can declare prefixes, graph names, vocabulary terms, and constants that you can use in the rest of your ETL configuration. Prefix declarations {#declarePrefix} Linked data uses IRIs for uniquely identifying most data items. Since IRIs can be long and complex, it is a best practice to declare short aliases that can be used to abbreviate them. Such aliases are introduced in prefix declarations. The function for declaring prefixes can be imported from the generic TriplyETL library: import { declarePrefix } from '@triplyetl/etl/generic' The following function introduces ALIAS as shorthandnotation for IRI_PREFIX : const base = declarePrefix('https://example.com/') Once an alias has been declared, future declarations can make use of that alias to extend it: const id = declarePrefix(base('id/')) Notice that it is common practice to end every IRI prefix in a forward slash. It is common to make declarations for the full IRI strategy in one place, with an intent to reuse them through the ETL configuration. To distinguish prefix declarations from other declarations, it is best practice to put all prefix declaration that will be used in transformations and assertions into an single object called prefix : const base = declarePrefix('https://example.com/') const id = declarePrefix(base('id/')) const prefix = { person: declarePrefix(id('person/')), def: declarePrefix(base('model/def/')), vehicle: declarePrefix(id('vehicle/')), } Notice that base and id are not intended to be used in transformations or assertions, but are only used to declare other prefixes that are used. With the above declarations in place, the following IRI assertion can be made: iri(prefix.person, 'name') iri(prefix.person, str('John')), iri(prefix.vehicle, 'id') iri(prefix.vehicle, str('123')), See assertion functions iri() and str() for more information. External prefix declarations In linked data, it is common to reuse existing vocabularies and datasets. TriplyETL allows you to use popular namespaces from predefined prefix declarations. Popular namespaces are imported from the vocabulary library: import { prefix } from '@triplyetl/etl/vocab' For example, you can use the prefix declaration for DBpedia resources as follows: iri(prefix.dbr, 'cityName') This may create IRIs like the following: http://dbpedia.org/resource/Amsterdam http://dbpedia.org/resource/Berlin You can use the prefix declaration for XML Schema Datatypes as follows: literal('cityName', xsd.string) This may create literals like the following: 'Amsterdam'^^xsd:string 'Berlin'^^xsd:string Vocabulary declarations {#vocabulary} Vocabularies are collections of IRIs that have the same namespace. The namespace can be declared with a prefix (see Prefix declarations ). We use the following prefix declaration as the namespace for our vocabulary: const base = declarePrefix('https://example.com/') const prefix = { def: declarePrefix(base('model/def/')), } Individual terms in the vocabulary can be declared by using the declaration of the namespace as a function: prefix.def('Person') prefix.def('Vehicle') prefix.def('knows') prefix.def('owns') These are equivalent to the following full IRIs: https://example.com/model/def/Person https://example.com/model/def/Vehicle https://example.com/model/def/knows https://example.com/model/def/owns It is best practice to place IRI terms that belong to the same vocabulary or namespace in an object: const def = { Person: prefix.def('Person'), Vehicle: prefix.def('Vehicle'), knows: prefix.def('knows'), owns: prefix.def('owns'), } With the above declarations in place, we can now make the following assertions: pairs(iri(prefix.person, 'name'), [a, def.Person], [def.owns, iri(prefix.vehicle, 'id')], ), This results in the following linked data: person:John a def:Person; def:owns vehicle:123. Or diagrammatically: graph LR john -- a --> Person john -- def:owns --> vehicle Person[def:Person]:::model john[person:John]:::data vehicle[vehicle:123]:::data classDef data fill:yellow classDef model fill:lightblue External vocabularies {#external-vocabularies} In linked data, it is common to reuse existing vocabularies. Popular vocabularies can be imported from the TriplyETL vocabulary library: import { a, foaf, owl, premis } from '@triplyetl/etl/vocab' This allows you to make the following assertions: triple(foaf.Person, a, owl.Class), This results in the following linked data: foaf:Person a owl:Class. Notice that the notation in TriplyETL comes very close to the notation in Turtle/TriG/SPARQL that is familiar to linked data users. The following code snippet uses the specialized PREMIS 3.0.0 vocabulary. This vocabulary is published by the Library of Congress and is used to publish metadata about the preservation of digital objects. The following code snippet asserts that a PREMIS file is stored in a PREMIS storage location: pairs(iri(id, 'some-file'), [a, premis.File], [premis.storedAt, iri(id, 'some-location')], ), triple(iri(id, 'some-location'), a, premis.StorageLocation), Custom abbreviations The custom abbreviation a is available in the popular Turtle/TriG/SPARQL languages. TriplyETL allows you to introduce this custom abbreviation from the vocabulary library: import { a } from '@triplyetl/etl/vocab' In Turtle/TriG syntax this abbreviation is only allowed to be used in the predicate position. This restriction is not enforced in TriplyETL, where you can use the a abbreviation in the subject, predicate, object, and even graph position. You can introduce your own custom abbreviations as needed. For example, the following code snippet introduces is_a as a custom abbreviation for the IRI rdfs:subClassOf : import { foaf, rdfs } from '@triplyetl/etl/vocab' const is_a = rdfs.subClassOf This allows you to write the following assertion: triple(foaf.Person, is_a, foaf.Agent), This may make assertions more readable for users from certain domains. For example, \"is a\" is a commonly use phrase in many other modeling languages to denote the subsumption relation. Instance declarations The same approach that is used for vocabulary declarations can also be used for instance declarations. The following example introduces constants for individual persons: const person = { jane: prefix.person('Jane'), john: prefix.person('John'), mary: prefix.person('Mary'), } Instance declarations are used in assertions similar to how vocabulary declarations as used: triple(person.john, def.knows, person.mary), Graph name declarations A linked dataset contains one or more graphs. Each graph can be given a name. It is common practice to declare a fixed set of graph names that will be used throughout the TriplyETL configuration. The following code snippet declares graph names for graphs that store metadata, model, and instances: import { declarePrefix } from '@triplyetl/etl/generic' const id = declarePrefix('https://example.com/id/') const prefix = { graph: declarePrefix(id('graph/')), } const graph = { metadata: prefix.graph('metadata'), model: prefix.graph('model'), instances: prefix.graph('instances'), } The declared graph names can now be used in assertions: triples(graph.metadata, ['_dataset', a, dcat.Dataset], ['_dataset', rdfs.label, str('My Dataset')], ), See assertion function triples() for more information. Language declarations Commonly used language tags can be imported in the following way: import { lang } from '@triplyetl/etl/vocab' These language declarations can be used to add language-tagged strings to the Record: addLiteral({ content: 'label', languageTag: lang.fr, key: '_languageTaggedString', }), Or they can be used to directly assert language-tagged strings in the Internal Store: triple('_city', rdfs.label, literal('label', lang.fr)), See transformation function addLiteral() and assertion function literal() for more information. Geospatial declarations IRIs that denote commonly used coordinate reference systems can be imported from the epsg object: import { geojsonToWkt } from '@triplyetl/etl/ratt' import { epsg } from '@triplyetl/etl/vocab' Such IRIs that denote coordinate reference systems can be used in several geospatial functions, for example in transformation function geojsonToWkt() : geojsonToWkt({ content: 'geojson', crs: epsg[28992], key: '_wkt', }),","title":"TriplyETL: Declare"},{"location":"triply-etl/declare/#prefix-declarations-declareprefix","text":"Linked data uses IRIs for uniquely identifying most data items. Since IRIs can be long and complex, it is a best practice to declare short aliases that can be used to abbreviate them. Such aliases are introduced in prefix declarations. The function for declaring prefixes can be imported from the generic TriplyETL library: import { declarePrefix } from '@triplyetl/etl/generic' The following function introduces ALIAS as shorthandnotation for IRI_PREFIX : const base = declarePrefix('https://example.com/') Once an alias has been declared, future declarations can make use of that alias to extend it: const id = declarePrefix(base('id/')) Notice that it is common practice to end every IRI prefix in a forward slash. It is common to make declarations for the full IRI strategy in one place, with an intent to reuse them through the ETL configuration. To distinguish prefix declarations from other declarations, it is best practice to put all prefix declaration that will be used in transformations and assertions into an single object called prefix : const base = declarePrefix('https://example.com/') const id = declarePrefix(base('id/')) const prefix = { person: declarePrefix(id('person/')), def: declarePrefix(base('model/def/')), vehicle: declarePrefix(id('vehicle/')), } Notice that base and id are not intended to be used in transformations or assertions, but are only used to declare other prefixes that are used. With the above declarations in place, the following IRI assertion can be made: iri(prefix.person, 'name') iri(prefix.person, str('John')), iri(prefix.vehicle, 'id') iri(prefix.vehicle, str('123')), See assertion functions iri() and str() for more information.","title":"Prefix declarations {#declarePrefix}"},{"location":"triply-etl/declare/#external-prefix-declarations","text":"In linked data, it is common to reuse existing vocabularies and datasets. TriplyETL allows you to use popular namespaces from predefined prefix declarations. Popular namespaces are imported from the vocabulary library: import { prefix } from '@triplyetl/etl/vocab' For example, you can use the prefix declaration for DBpedia resources as follows: iri(prefix.dbr, 'cityName') This may create IRIs like the following: http://dbpedia.org/resource/Amsterdam http://dbpedia.org/resource/Berlin You can use the prefix declaration for XML Schema Datatypes as follows: literal('cityName', xsd.string) This may create literals like the following: 'Amsterdam'^^xsd:string 'Berlin'^^xsd:string","title":"External prefix declarations"},{"location":"triply-etl/declare/#vocabulary-declarations-vocabulary","text":"Vocabularies are collections of IRIs that have the same namespace. The namespace can be declared with a prefix (see Prefix declarations ). We use the following prefix declaration as the namespace for our vocabulary: const base = declarePrefix('https://example.com/') const prefix = { def: declarePrefix(base('model/def/')), } Individual terms in the vocabulary can be declared by using the declaration of the namespace as a function: prefix.def('Person') prefix.def('Vehicle') prefix.def('knows') prefix.def('owns') These are equivalent to the following full IRIs: https://example.com/model/def/Person https://example.com/model/def/Vehicle https://example.com/model/def/knows https://example.com/model/def/owns It is best practice to place IRI terms that belong to the same vocabulary or namespace in an object: const def = { Person: prefix.def('Person'), Vehicle: prefix.def('Vehicle'), knows: prefix.def('knows'), owns: prefix.def('owns'), } With the above declarations in place, we can now make the following assertions: pairs(iri(prefix.person, 'name'), [a, def.Person], [def.owns, iri(prefix.vehicle, 'id')], ), This results in the following linked data: person:John a def:Person; def:owns vehicle:123. Or diagrammatically: graph LR john -- a --> Person john -- def:owns --> vehicle Person[def:Person]:::model john[person:John]:::data vehicle[vehicle:123]:::data classDef data fill:yellow classDef model fill:lightblue","title":"Vocabulary declarations {#vocabulary}"},{"location":"triply-etl/declare/#external-vocabularies-external-vocabularies","text":"In linked data, it is common to reuse existing vocabularies. Popular vocabularies can be imported from the TriplyETL vocabulary library: import { a, foaf, owl, premis } from '@triplyetl/etl/vocab' This allows you to make the following assertions: triple(foaf.Person, a, owl.Class), This results in the following linked data: foaf:Person a owl:Class. Notice that the notation in TriplyETL comes very close to the notation in Turtle/TriG/SPARQL that is familiar to linked data users. The following code snippet uses the specialized PREMIS 3.0.0 vocabulary. This vocabulary is published by the Library of Congress and is used to publish metadata about the preservation of digital objects. The following code snippet asserts that a PREMIS file is stored in a PREMIS storage location: pairs(iri(id, 'some-file'), [a, premis.File], [premis.storedAt, iri(id, 'some-location')], ), triple(iri(id, 'some-location'), a, premis.StorageLocation),","title":"External vocabularies {#external-vocabularies}"},{"location":"triply-etl/declare/#custom-abbreviations","text":"The custom abbreviation a is available in the popular Turtle/TriG/SPARQL languages. TriplyETL allows you to introduce this custom abbreviation from the vocabulary library: import { a } from '@triplyetl/etl/vocab' In Turtle/TriG syntax this abbreviation is only allowed to be used in the predicate position. This restriction is not enforced in TriplyETL, where you can use the a abbreviation in the subject, predicate, object, and even graph position. You can introduce your own custom abbreviations as needed. For example, the following code snippet introduces is_a as a custom abbreviation for the IRI rdfs:subClassOf : import { foaf, rdfs } from '@triplyetl/etl/vocab' const is_a = rdfs.subClassOf This allows you to write the following assertion: triple(foaf.Person, is_a, foaf.Agent), This may make assertions more readable for users from certain domains. For example, \"is a\" is a commonly use phrase in many other modeling languages to denote the subsumption relation.","title":"Custom abbreviations"},{"location":"triply-etl/declare/#instance-declarations","text":"The same approach that is used for vocabulary declarations can also be used for instance declarations. The following example introduces constants for individual persons: const person = { jane: prefix.person('Jane'), john: prefix.person('John'), mary: prefix.person('Mary'), } Instance declarations are used in assertions similar to how vocabulary declarations as used: triple(person.john, def.knows, person.mary),","title":"Instance declarations"},{"location":"triply-etl/declare/#graph-name-declarations","text":"A linked dataset contains one or more graphs. Each graph can be given a name. It is common practice to declare a fixed set of graph names that will be used throughout the TriplyETL configuration. The following code snippet declares graph names for graphs that store metadata, model, and instances: import { declarePrefix } from '@triplyetl/etl/generic' const id = declarePrefix('https://example.com/id/') const prefix = { graph: declarePrefix(id('graph/')), } const graph = { metadata: prefix.graph('metadata'), model: prefix.graph('model'), instances: prefix.graph('instances'), } The declared graph names can now be used in assertions: triples(graph.metadata, ['_dataset', a, dcat.Dataset], ['_dataset', rdfs.label, str('My Dataset')], ), See assertion function triples() for more information.","title":"Graph name declarations"},{"location":"triply-etl/declare/#language-declarations","text":"Commonly used language tags can be imported in the following way: import { lang } from '@triplyetl/etl/vocab' These language declarations can be used to add language-tagged strings to the Record: addLiteral({ content: 'label', languageTag: lang.fr, key: '_languageTaggedString', }), Or they can be used to directly assert language-tagged strings in the Internal Store: triple('_city', rdfs.label, literal('label', lang.fr)), See transformation function addLiteral() and assertion function literal() for more information.","title":"Language declarations"},{"location":"triply-etl/declare/#geospatial-declarations","text":"IRIs that denote commonly used coordinate reference systems can be imported from the epsg object: import { geojsonToWkt } from '@triplyetl/etl/ratt' import { epsg } from '@triplyetl/etl/vocab' Such IRIs that denote coordinate reference systems can be used in several geospatial functions, for example in transformation function geojsonToWkt() : geojsonToWkt({ content: 'geojson', crs: epsg[28992], key: '_wkt', }),","title":"Geospatial declarations"},{"location":"triply-etl/enrich/","text":"The Enrich step uses linked data that is asserted in the Internal Store to derive new linked data. graph LR source -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> tdb linkStyle 3 stroke:red,stroke-width:3px; ld[Internal Store] record[Record] source[Data Sources] tdb[(Triple Store)] TriplyETL supports the following enrichment approaches: 4A. SHACL Rules are able to apply SPARQL Ask and Construct queries to the internal store. 4B. SPARQL Update allows linked data to be added to and deleted from the internal store. See also If you do not have linked data in your internal store yet, then first perform one of the following steps: - 1. Extract allows you to load linked data into your internal store directly, using the loadRdf() function. - 3. Assert uses entries from your record to make linked data assertions into your internal store.","title":"4. TriplyETL: Enrich"},{"location":"triply-etl/enrich/#see-also","text":"If you do not have linked data in your internal store yet, then first perform one of the following steps: - 1. Extract allows you to load linked data into your internal store directly, using the loadRdf() function. - 3. Assert uses entries from your record to make linked data assertions into your internal store.","title":"See also"},{"location":"triply-etl/enrich/shacl/","text":"SHACL Rules allow new data to be asserted based on existing data patterns. This makes them a great approach for data enrichment. Since SHACL Rules can be defined as part of the data model, it is one of the best approaches for creating and maintaining business rules in complex domains. SHACL Rules are applied to the linked data that is currently present in the internal store. The order in which rules are evaluated can be specified in terms of dynamic preconditions, or in terms of a predefined order. The SHACL Rules engine in TriplyETL processes rules iteratively. This supports rules whose execution depends on the outcome of other rules. Triply observes that this iterative functionality is often necessary for complex production systems in which SHACL Rules are applied. See the enrichment step overview page for other enrichment approaches. Prerequisites SHACL Rules can be used when the following preconditions are met: You have a data model that has one or more SHACL Rules. You have some linked data in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add linked data to the internal store. The function for executing SHACL Rules is imported as follows: import { executeRules } from '@triplyetl/etl/shacl' TriplyETL supports two kinds of SHACL Rules: Triple Rules and SPARQL Rules. The rest of this page describes two complete examples of the use of SHACL Rules: the first one uses Triple Rules, and the second uses SPARQL Rules. A complete example with Triple Rules This section describes a complete example that makes use of Triple Rules. These are SHACL Rules that assert exactly one triple. Every rule that can be implemented with Triple Rules can also be implemented with SPARQL Rules, but Triple Rules sometimes simpler to use, since they do not require knowledge of the SPARQL language. Step 1: Load instance data {#stepA1} We first need to load some instance data, so that we can apply a rule and enrich the loaded data with some new data. We use linked data assertions that state that John is a person, who has a child (Mary), and whose gender is male: base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. Applying our knowledge of the world, we can deduce from that instance data that John is also a father. This can also be expressed in linked data: base <https://triplydb.com/> <john> a <Father>. When we make this deduction, we are applying a (possibly implicit) rule. When we try to make the rule that we have applied explicit, we discover that a rule has the following two components: The condition is are the criteria that must be met in order for the rule to become applicable. In our example, we must have instance data about a person. That person must have at least one child. That person must be male. Notice that the condition can be made arbitrarily complex: we can add more criteria like age, nationality, etc. if we wanted to. The assertion is the new data that we can add to our internal store. In our example, this is the assertion that John is a father. We can show this principle in a diagram, where condition and assertion contain the two components of the rule: graph subgraph Condition id:john -- a --> sdo:Person id:john -- sdo:children --> id:mary id:john -- sdo:gender --> sdo:Male end subgraph Assertion id:john -- a --> def:Father end Step 2. Formulate the SHACL rule {#stepA2} In Step 1 we applied a rule to the instance John. But our dataset may contain information about many other people too: people with or without children, people with different genders, etc. Suppose our dataset contains information about Peter, who has two children and has the male gender. We can apply the same rule to deduce that Peter is also a father. When we apply the same rule to an arbitrary number of instances, we are applying a principle called 'generalization'. We replace information about instance like 'John' and 'Peter' with a generic class such as 'Person'. When we think about it, the generalized rule that we have applied to John and Peter, and that we can apply to any number of individuals, is as follows: Persons with at least one child and the male gender, are fathers. We can formalize this general rule in the following SHACL snippet: base <https://triplydb.com/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:TripleRule; sh:condition [ sh:property [ sh:path sdo:children; sh:minCount 1 ] ]; sh:condition [ sh:property [ sh:path sdo:gender; sh:hasValue sdo:Male ] ]; sh:subject sh:this; sh:predicate rdf:type; sh:object <Father> ]. Notice the following details: - The rule only applies to persons, i.e. instance of the class sdo:Person . This is expressed by the sh:targetClass property. - The first condition of the rule is that the person must have at least one child. This is expressed by sh:condition and sh:minCount . - The second condition of the rule is that the gender of the person is male. This is expressed by sh:condition and sh:hasValue . - The assertion is that the person is a father. Since we use a Triple Rule, this is expressed by the properties sh:subject , sh:predicate , and sh:object . - Notice that the term sh:this is used to refer to individuals for whom all conditions are met (in our example: John). Step 3: Write and run the script We can store the instance data (i.e., the first linked data snippet in Step 1 ) in a file called instances.trig , and we can store the model (i.e., the linked data snippet in Step 2 ) in a file called model.trig . We then need the following TriplyETL script to (1) load the instance data, (2) execute the rules specified in the model, and (3) print the contents of the internal store to standard output: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('instances.trig')), executeRules(Source.file('rules.trig')), logQuads(), ) return etl } When we run this script (command npx etl ), the following linked data is logged to standard output: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the SHACL rule in the data model. A complete example with SPARQL Rules This section describes a complete example that makes use of SPARQL Rules. These are SHACL Rules that assert an arbitrary number of triples. Every rule that can be implemented with Triple Rules, can also be implemented with SPARQL Rules. We use the same instance data as in Step 1 of the Triple Rules example . We formalize the general rule that was formalized in Step 2 of the Triple Rules example as follows: base <https://triplydb.com/> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:SPARQLRule; sh:construct ''' base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this sdo:children []; sdo:gender sdo:Male. }''' ]. Notice the following details: - The two conditions are specified in the Where-clause. - The assertion is specified in the Construct-clause. - The SPARQL Construct query uses triple quoted literals notation ( '''...''' ), which allows newlines to appear without escaping in linked data. - The SPARQL variable $this has special meaning: it binds to the targets of the SHACL node shape (i.e. instances of sdo:Person ). The TriplyETL script is identical to the one used in the Triple Rules example: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('instances.trig')), executeRules(Source.file('rules.trig')), logQuads(), ) return etl } When we run this script (command npx etl ), the linked data results are also the same: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the SHACL rule in the data model. Another example with SPARQL Rules In this example, we start out with a data source that is not linked data: { \"age\": 20, \"name\": \"ann\" }, { \"age\": 12, \"name\": \"peter\" }, We use the fromJson() extractor and specify the source data inline . We use RATT assertion function pairs() to add linked data to the internal store. import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, declarePrefix, fromJson } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { executeRules } from '@triplyetl/etl/shacl' import { a, foaf } from '@triplyetl/etl/vocab' const id = declarePrefix('https://triplydb.com/') export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { age: 20, name: 'ann' }, { age: 12, name: 'peter' }, ]), pairs(iri(id, 'name'), [a, foaf.Person], [foaf.age, 'age'], ), executeRules(Source.file('model.trig')), logQuads(), ) return etl } The model (file model.trig ) makes use of a SPARQL Rule, to assert that persons who are at least 18 years old are adults: base <https://triplydb.com/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass foaf:Person; sh:rule [ a sh:SPARQLRule; sh:prefixes <>; sh:construct ''' construct { $this a ex:Adult. } where { $this foaf:age ?age. filter(?age >= 18) }''' ]. Notice that the SPARQL query string (the value of sh:construct ) does not declare the ex: and foaf: prefixes. Instead, the rule refers to generic declarations (with property sh:prefixes ) that occur later in the model.trig file: <> sh:declare [ sh:namespace <https://triplydb.com/>; sh:prefix 'ex' ], [ sh:namespace <http://xmlns.com/foaf/0.1/>; sh:prefix 'foaf' ]. This notation allows the same prefix declarations to be reused by an arbitrary number of SPARQL Rules.","title":"4. Enrich: SHACL Rules"},{"location":"triply-etl/enrich/shacl/#prerequisites","text":"SHACL Rules can be used when the following preconditions are met: You have a data model that has one or more SHACL Rules. You have some linked data in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add linked data to the internal store. The function for executing SHACL Rules is imported as follows: import { executeRules } from '@triplyetl/etl/shacl' TriplyETL supports two kinds of SHACL Rules: Triple Rules and SPARQL Rules. The rest of this page describes two complete examples of the use of SHACL Rules: the first one uses Triple Rules, and the second uses SPARQL Rules.","title":"Prerequisites"},{"location":"triply-etl/enrich/shacl/#a-complete-example-with-triple-rules","text":"This section describes a complete example that makes use of Triple Rules. These are SHACL Rules that assert exactly one triple. Every rule that can be implemented with Triple Rules can also be implemented with SPARQL Rules, but Triple Rules sometimes simpler to use, since they do not require knowledge of the SPARQL language.","title":"A complete example with Triple Rules"},{"location":"triply-etl/enrich/shacl/#step-1-load-instance-data-stepa1","text":"We first need to load some instance data, so that we can apply a rule and enrich the loaded data with some new data. We use linked data assertions that state that John is a person, who has a child (Mary), and whose gender is male: base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. Applying our knowledge of the world, we can deduce from that instance data that John is also a father. This can also be expressed in linked data: base <https://triplydb.com/> <john> a <Father>. When we make this deduction, we are applying a (possibly implicit) rule. When we try to make the rule that we have applied explicit, we discover that a rule has the following two components: The condition is are the criteria that must be met in order for the rule to become applicable. In our example, we must have instance data about a person. That person must have at least one child. That person must be male. Notice that the condition can be made arbitrarily complex: we can add more criteria like age, nationality, etc. if we wanted to. The assertion is the new data that we can add to our internal store. In our example, this is the assertion that John is a father. We can show this principle in a diagram, where condition and assertion contain the two components of the rule: graph subgraph Condition id:john -- a --> sdo:Person id:john -- sdo:children --> id:mary id:john -- sdo:gender --> sdo:Male end subgraph Assertion id:john -- a --> def:Father end","title":"Step 1: Load instance data {#stepA1}"},{"location":"triply-etl/enrich/shacl/#step-2-formulate-the-shacl-rule-stepa2","text":"In Step 1 we applied a rule to the instance John. But our dataset may contain information about many other people too: people with or without children, people with different genders, etc. Suppose our dataset contains information about Peter, who has two children and has the male gender. We can apply the same rule to deduce that Peter is also a father. When we apply the same rule to an arbitrary number of instances, we are applying a principle called 'generalization'. We replace information about instance like 'John' and 'Peter' with a generic class such as 'Person'. When we think about it, the generalized rule that we have applied to John and Peter, and that we can apply to any number of individuals, is as follows: Persons with at least one child and the male gender, are fathers. We can formalize this general rule in the following SHACL snippet: base <https://triplydb.com/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:TripleRule; sh:condition [ sh:property [ sh:path sdo:children; sh:minCount 1 ] ]; sh:condition [ sh:property [ sh:path sdo:gender; sh:hasValue sdo:Male ] ]; sh:subject sh:this; sh:predicate rdf:type; sh:object <Father> ]. Notice the following details: - The rule only applies to persons, i.e. instance of the class sdo:Person . This is expressed by the sh:targetClass property. - The first condition of the rule is that the person must have at least one child. This is expressed by sh:condition and sh:minCount . - The second condition of the rule is that the gender of the person is male. This is expressed by sh:condition and sh:hasValue . - The assertion is that the person is a father. Since we use a Triple Rule, this is expressed by the properties sh:subject , sh:predicate , and sh:object . - Notice that the term sh:this is used to refer to individuals for whom all conditions are met (in our example: John).","title":"Step 2. Formulate the SHACL rule {#stepA2}"},{"location":"triply-etl/enrich/shacl/#step-3-write-and-run-the-script","text":"We can store the instance data (i.e., the first linked data snippet in Step 1 ) in a file called instances.trig , and we can store the model (i.e., the linked data snippet in Step 2 ) in a file called model.trig . We then need the following TriplyETL script to (1) load the instance data, (2) execute the rules specified in the model, and (3) print the contents of the internal store to standard output: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('instances.trig')), executeRules(Source.file('rules.trig')), logQuads(), ) return etl } When we run this script (command npx etl ), the following linked data is logged to standard output: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the SHACL rule in the data model.","title":"Step 3: Write and run the script"},{"location":"triply-etl/enrich/shacl/#a-complete-example-with-sparql-rules","text":"This section describes a complete example that makes use of SPARQL Rules. These are SHACL Rules that assert an arbitrary number of triples. Every rule that can be implemented with Triple Rules, can also be implemented with SPARQL Rules. We use the same instance data as in Step 1 of the Triple Rules example . We formalize the general rule that was formalized in Step 2 of the Triple Rules example as follows: base <https://triplydb.com/> prefix sdo: <https://schema.org/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass sdo:Person; sh:rule [ a sh:SPARQLRule; sh:construct ''' base <https://triplydb.com/> prefix sdo: <https://schema.org/> construct { $this a <Father>. } where { $this sdo:children []; sdo:gender sdo:Male. }''' ]. Notice the following details: - The two conditions are specified in the Where-clause. - The assertion is specified in the Construct-clause. - The SPARQL Construct query uses triple quoted literals notation ( '''...''' ), which allows newlines to appear without escaping in linked data. - The SPARQL variable $this has special meaning: it binds to the targets of the SHACL node shape (i.e. instances of sdo:Person ). The TriplyETL script is identical to the one used in the Triple Rules example: import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, loadRdf } from '@triplyetl/etl/generic' import { executeRules } from '@triplyetl/etl/shacl' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( loadRdf(Source.file('instances.trig')), executeRules(Source.file('rules.trig')), logQuads(), ) return etl } When we run this script (command npx etl ), the linked data results are also the same: <john> a sdo:Person, <Father>; sdo:children <mary>; sdo:gender sdo:Male. Notice that the fatherhood assertion was correctly added to the internal store, based on the SHACL rule in the data model.","title":"A complete example with SPARQL Rules"},{"location":"triply-etl/enrich/shacl/#another-example-with-sparql-rules","text":"In this example, we start out with a data source that is not linked data: { \"age\": 20, \"name\": \"ann\" }, { \"age\": 12, \"name\": \"peter\" }, We use the fromJson() extractor and specify the source data inline . We use RATT assertion function pairs() to add linked data to the internal store. import { logQuads } from '@triplyetl/etl/debug' import { Etl, Source, declarePrefix, fromJson } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { executeRules } from '@triplyetl/etl/shacl' import { a, foaf } from '@triplyetl/etl/vocab' const id = declarePrefix('https://triplydb.com/') export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { age: 20, name: 'ann' }, { age: 12, name: 'peter' }, ]), pairs(iri(id, 'name'), [a, foaf.Person], [foaf.age, 'age'], ), executeRules(Source.file('model.trig')), logQuads(), ) return etl } The model (file model.trig ) makes use of a SPARQL Rule, to assert that persons who are at least 18 years old are adults: base <https://triplydb.com/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix sh: <http://www.w3.org/ns/shacl#> <Person> sh:targetClass foaf:Person; sh:rule [ a sh:SPARQLRule; sh:prefixes <>; sh:construct ''' construct { $this a ex:Adult. } where { $this foaf:age ?age. filter(?age >= 18) }''' ]. Notice that the SPARQL query string (the value of sh:construct ) does not declare the ex: and foaf: prefixes. Instead, the rule refers to generic declarations (with property sh:prefixes ) that occur later in the model.trig file: <> sh:declare [ sh:namespace <https://triplydb.com/>; sh:prefix 'ex' ], [ sh:namespace <http://xmlns.com/foaf/0.1/>; sh:prefix 'foaf' ]. This notation allows the same prefix declarations to be reused by an arbitrary number of SPARQL Rules.","title":"Another example with SPARQL Rules"},{"location":"triply-etl/enrich/sparql/","text":"SPARQL Update is a powerful feature that allows you to modify and enrich linked data in the internal store. With SPARQL Update, you can generate new linked data based on existing linked data, thereby enhancing the content of the store. Support for SPARQL Update is currently experimental. In the meantime, you can use SHACL Rules to implement the Enrich Step of your pipeline. Prerequisites The function for executing SPARQL Update requests is imported as follows: import { update } from '@triplyetl/etl/sparql' Add data to the internal store SPARQL Update can be used to add linked data to the internal store. The following example adds one triple: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> insert data { <john> <knows> <mary>. }`), logQuads(), ) return etl } Debug function logQuads() prints the content of the internal store to standard output: base <https://triplydb.com/> <john> <knows> <mary>. Using prefix declarations Notice that the SPARQL Update function takes a plain string. Any typos you make in this string will only result in errors at runtime, when the query string is interpreted and executed. One of the more difficult things to get right in a SPARQL string are the prefix declarations. We can use the prefix object to insert the correct IRI prefixes. The following example asserts three triples, and uses the prefix object to insert the IRI prefix for Schema.org: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { prefix } from '@triplyetl/etl/vocab' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> prefix sdo: <${prefix.sdo('').value}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), logQuads(), ) return etl } This prints the following linked data to standard output: base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. Remove data from the internal store While there are not many uses cases for removing data from the internal store, this is an operation that is supported by the SPARQL Update standard. The following function call removes the parent/child relationship assertion that was added to the internal store earlier: update(` prefix sdo: <${prefix.sdo('').value}> delete data { <john> sdo:children <mary>. }`), You can use the debug function logQuads() before and after this function call, to see the effects on the internal store. Conditionally adding/removing data SPARQL Update can be used to conditionally add and/or remove linked data to/from the internal store. It uses the following keywords for this: where is the condition that must be met inside the internal store. Conditions can be specified in a generic way by using SPARQL variables. The bindings for these variables are shared with the other two components. delete is the pattern that is removed from the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the delete pattern are instantiated before deletion is performed. Deletion is performed before insertion. insert is the pattern that is added to the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the insert pattern are instantiated before insertion is performed. Insertion is performed after deletion. We can use this powerful combination of a where condition and a delete and insert follow-up to implement rules. For example, we may want to formalize the following rule: Persons with at least one child and the male gender, are fathers. At the same time, we may be restricted in the information we are allowed to publish in our linked dataset: After fatherhood has been determined, any specific information about parent/child relationships must be removed from the internal store. The rule can be formalized as follows: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { prefix } from '@triplyetl/etl/vocab' const baseIri = 'https://triplydb.com/' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <${baseIri}> prefix sdo: <${prefix.sdo('').value}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), update(` base <${baseIri}> prefix sdo: <${prefix.sdo('').value}> delete { $person sdo:children ?child. } insert { $person a <Father>. } where { $person a sdo:Person; sdo:children ?child; sdo:gender sdo:Male. }`), logQuads(), ) return etl }","title":"4. Enrich: SPARQL Update"},{"location":"triply-etl/enrich/sparql/#prerequisites","text":"The function for executing SPARQL Update requests is imported as follows: import { update } from '@triplyetl/etl/sparql'","title":"Prerequisites"},{"location":"triply-etl/enrich/sparql/#add-data-to-the-internal-store","text":"SPARQL Update can be used to add linked data to the internal store. The following example adds one triple: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> insert data { <john> <knows> <mary>. }`), logQuads(), ) return etl } Debug function logQuads() prints the content of the internal store to standard output: base <https://triplydb.com/> <john> <knows> <mary>.","title":"Add data to the internal store"},{"location":"triply-etl/enrich/sparql/#using-prefix-declarations","text":"Notice that the SPARQL Update function takes a plain string. Any typos you make in this string will only result in errors at runtime, when the query string is interpreted and executed. One of the more difficult things to get right in a SPARQL string are the prefix declarations. We can use the prefix object to insert the correct IRI prefixes. The following example asserts three triples, and uses the prefix object to insert the IRI prefix for Schema.org: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { prefix } from '@triplyetl/etl/vocab' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <https://triplydb.com/> prefix sdo: <${prefix.sdo('').value}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), logQuads(), ) return etl } This prints the following linked data to standard output: base <https://triplydb.com/> prefix sdo: <https://schema.org/> <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male.","title":"Using prefix declarations"},{"location":"triply-etl/enrich/sparql/#remove-data-from-the-internal-store","text":"While there are not many uses cases for removing data from the internal store, this is an operation that is supported by the SPARQL Update standard. The following function call removes the parent/child relationship assertion that was added to the internal store earlier: update(` prefix sdo: <${prefix.sdo('').value}> delete data { <john> sdo:children <mary>. }`), You can use the debug function logQuads() before and after this function call, to see the effects on the internal store.","title":"Remove data from the internal store"},{"location":"triply-etl/enrich/sparql/#conditionally-addingremoving-data","text":"SPARQL Update can be used to conditionally add and/or remove linked data to/from the internal store. It uses the following keywords for this: where is the condition that must be met inside the internal store. Conditions can be specified in a generic way by using SPARQL variables. The bindings for these variables are shared with the other two components. delete is the pattern that is removed from the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the delete pattern are instantiated before deletion is performed. Deletion is performed before insertion. insert is the pattern that is added to the internal store. This requires that the where condition is satisfied in the internal store. Any bindings for variables that are shared between the where condition and the insert pattern are instantiated before insertion is performed. Insertion is performed after deletion. We can use this powerful combination of a where condition and a delete and insert follow-up to implement rules. For example, we may want to formalize the following rule: Persons with at least one child and the male gender, are fathers. At the same time, we may be restricted in the information we are allowed to publish in our linked dataset: After fatherhood has been determined, any specific information about parent/child relationships must be removed from the internal store. The rule can be formalized as follows: import { logQuads } from '@triplyetl/etl/debug' import { Etl } from '@triplyetl/etl/generic' import { update } from '@triplyetl/etl/sparql' import { prefix } from '@triplyetl/etl/vocab' const baseIri = 'https://triplydb.com/' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( update(` base <${baseIri}> prefix sdo: <${prefix.sdo('').value}> insert data { <john> a sdo:Person; sdo:children <mary>; sdo:gender sdo:Male. }`), update(` base <${baseIri}> prefix sdo: <${prefix.sdo('').value}> delete { $person sdo:children ?child. } insert { $person a <Father>. } where { $person a sdo:Person; sdo:children ?child; sdo:gender sdo:Male. }`), logQuads(), ) return etl }","title":"Conditionally adding/removing data"},{"location":"triply-etl/extract/","text":"The Extract step is the first step in any TriplyETL pipeline. It extracts data in different formats and from different source types. Examples of data formats are 'Microsoft Excel' and 'JSON'. Examples of source types are 'file' or 'URL'. Source data are represented in a uniform Record. graph LR source -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> tdb linkStyle 0 stroke:red,stroke-width:3px; ld[Internal Store] record[Record] source[Data Sources] tdb[(Triple Store)] The following pages cover the Extract step in detail: 1A. Data Formats gives an overview of the data formats that are supported by TriplyETL. 1B. Source Types given an overview of the source types that are supported by TriplyETL 1C. Record documents the basic structure of every record in TriplyETL. Next steps The Extract step results in a stream of records. The basic structure of every Record in TriplyETL is the same. It does not matter which data format or which source type is used. Once a stream of Records is generated, the following steps document how data from those records can be used: 2. Transform are applied to the Record to change its contents. 3. Assert use data from the Record to generate linked data in the Internal Store.","title":"1. TriplyETL: Extract"},{"location":"triply-etl/extract/#next-steps","text":"The Extract step results in a stream of records. The basic structure of every Record in TriplyETL is the same. It does not matter which data format or which source type is used. Once a stream of Records is generated, the following steps document how data from those records can be used: 2. Transform are applied to the Record to change its contents. 3. Assert use data from the Record to generate linked data in the Internal Store.","title":"Next steps"},{"location":"triply-etl/extract/formats/","text":"Overview TriplyETL supports the following data formats: Extractor Format Full name fromCsv() CSV Comma-Separated Values fromJson() JSON JavaScript Object Notation fromOai() OAI-PMH Open Archives Initiative Protocol for Metadata Harvesting fromPostgres() PostgreSQL Query & Postgres API Options PostgreSQL Query & Postgres API Options fromShapefile() ESRI ESRI Shapefiles fromTsv() TSV Tab-Separated Values fromXlsx() XLSX Microsoft Excel fromXml() XML XML Markup Language loadRdf() RDF Resource Description Format All extractors can be imported from the generic library in TriplyETL: import { fromCsv, fromJson, fromOai, fromPostgres, fromShapefile, fromTsv, fromXlsx, fromXml, loadRdf, Source } from '@triplyetl/etl/generic' Notice that you also need to import Source , since every extractor requires a source. Extractor fromCsv() {#fromCsv} CSV or Comma Separated Values (file name extension .csv ) is a popular format for storing tabular source data. TriplyETL has a dedicated fromCsv() extractor for this data format. The following code snippet extracts records from a local CSV file: fromCsv(Source.file('data.csv')), The following code snippet extracts records from an online CSV file, that is hosted at the specified URL: fromCsv(Source.url('https://somewhere.com/data.csv')), The following code snippet extracts records from a TriplyDB Asset . The asset is store in the data with name 'some-data' , under an account with name 'some-account' . The name of the asset is 'example.csv' : fromCsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.csv' } ) ), Standards-compliance The fromCsv() extractor implements the official CSV standard: IETF RFC 4180 . Some CSV files do not follow the standard precisely. In order to process such CSV files, the default behavior of the extractor can be changed through an optional options parameter. See the CSV Parse for Node.js documentation for all the available options. Configure the encoding According to the official CSV standard, CSV sources are allowed to use any encoding. Since the CSV format does not allow the used encoding to be specified in the format itself, a non-standard encoding must always be configured manually. By default, TriplyETL assumes that CSV sources use the UTF-8 encoding. If another encoding is used, this must be explicitly specified by using the optional options parameter. The following snippet configures that the CSV source uses the ISO Latin-1 encoding: fromCsv( Source.TriplyDb.asset('my-dataset', { name: 'example.csv' }), { encoding: 'latin1' } ), The following encodings are currently supported: Value Encoding Standard Alternative values 'ascii' US-ASCII ANSI 'latin1' Latin-1 ISO-8859-1 binary 'utf8' UTF-8 Unicode 'utf16le' UTF-16 Little Endian Unicode 'ucs2' , 'ucs-2' , 'utf16-le' Read the CSV Parse for Node.js documentation for more information. Use a different separator Some CSV files only deviate in their use of a different separator character. For example, some CSV files use the semi-colon ( ; ) or the at-sign ( @ ) for this. The following snippet extracts records for a CSV file that uses the semi-colon ( ; ) as the separator character: fromCsv(Source.file('example.csv'), { separator: ';' }), CSV with tab separators is not TSV Notice that the popular Tab-Separate Values (TSV) format is not the same as the standardized CSV format with a tab separator character. If you want to process standards-conforming TSV source data, use the fromTsv() extractor instead. Record representation TriplyETL treats every row in a CSV source as one record. The columns are emitted as keys and the cells are emitted as values. All values are of type string . Empty cells (i.e. those containing the empty string) are treated as denoting a null value and are therefore excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys and values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following CSV snippet: ID,Name,Age 1,\"Doe, John\",32 2,\"D., Jane \", which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice that: - All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the CSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. - The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. - The \"Age\" key is missing from the second record, since the corresponding CSV cell contains the empty string, which is considered to denote an empty value. Extractor fromJson() {#fromJson} JSON (JavaScript Object Notation) is a popular open standard for interchanging tree-shaped data. TriplyETL has a dedicated fromJson() extractor for this format. The following code snippet connects to a JSON source that is stored as a TriplyDB asset : fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json.gz' } ) ), The following example uses an in-line specified JSON source: fromJson([{ a: 'a', b: 'b', c: 'c' }]), TriplyETL supports the IETF RFC 8259 standard for JSON. Nested keys Since JSON is a tree-shaped format, it is able to store values in a nested structure. This requires a sequence or 'path' of keys to be specified. We use the following example data: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"country.id\": \"nl\", \"name\": \"The Netherlands\" }, { \"country.id\": \"de\", \"name\": \"Germany\" } ] } } Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the JSON example in the previous section, TriplyETL can access the \"name\" key inside the \"title\" key, which itself is nested inside the \"metadata\" key. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which also accesses the \"name\" key, but nested inside the \"countries\" and then \"data\" keys. (The use of the [0] index is explained in the next section.) [1] metadata.title.name [2] data.countries[0].name Path expressions can be used as string keys in many places in TriplyETL. For example, we can assert the title of a dataset in the following way: triple('_dataset', dct.title, 'metadata.title.name'), This asserts the following linked data: dataset:my-dataset dct:title 'Data about countries.'. Dealing with dots in keys In the previous section we saw that dots are used to separate keys in paths. However, sometimes a dot can occur as a regular character inside a key. In such cases, we need to apply additional escaping of the key name to avoid naming conflicts. The example data from the previous section contains the following key: \"country.id\" Notice that the dot is here part of the key name. We can refer to these keys as follows: triple('_country', dct.id, 'data.countries[0].[\"country.id\"]'), Notice the use of additional escaping: [\"...\"] Accessing lists by index Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. TriplyETL is able to access specific elements from lists based on theirindex* or position. Following the standard practice in Computer Science, TriplyETL refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of thefirst* country as follows: triple( iri(prefix.id, 'data.countries[0].[\"country.id\"]'), rdfs.label, 'data.countries[0].name' ), This results in the following linked data: id:nl rdfs:label 'The Netherlands'. We can also assert the name of the second country. Notice that only the index is different ( 1 instead of 0 ): triple( iri(prefix.id, 'data.countries[1].[\"country.id\"]'), rdfs.label, 'data.countries[1].name' ), This results in the following linked data: id:de rdfs:label 'Germany'. Extractor fromOai() {#fromOai} In GLAM domains (Galleries, Libraries, Archives, Museums), the Open Archives Initiative (OAI), Protocol for Metadata Harvesting (PMH) is a popular protocol and format for publishing data collections. TriplyETL includes the fromOai() extractor to tap into these data collections. The fromOai() extractor ensures a continuous stream of data records. Under the hood, the extractor uses resumption tokens to iterate over large collections. An OAI-PMH endpoint can be configured by specifying its URL (parameter url ). Since one OAI-PMH endpoint typically publishes multiple datasets, it is also common to specify the set parameter. The following code snippet connects to an example dataset that is published in an example OAI-PMH endpoint: fromOai({ set: 'some-dataset', url: 'https://somewhere.com/webapioai/oai.ashx' }), TriplyETL supports the official OAI-PMH standard. The OAI-PMH standard defines 6 'verbs'. These different sub-APIs that together component the OAI-PMH API. Extractor fromOai() currently supports the following two verbs: ListIdentifiers and ListRecords . Verb ListIdentifiers {#ListIdentifiers} This 'verb' or sub-API streams through the headers of all records. It does not returns the actual (body) content of each record (see ListRecords ). This verb can be used to look for header properties like set membership, datestamp, and deletion status. The following code snippet streams through the headers of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListIdentifiers' }), logRecord(), Verb ListRecords {#ListRecords} This 'verb' or sub-API streams through all records and retrieves them in full. This API is used to harvest records. The following code snippet streams through the records of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListRecords' }), logRecord(), Extractor fromTsv() {#fromTsv} TSV or Tab-Separated Values (file name extension .tsv ) is a popular format for tabular source data. TriplyETL has a fromTsv() extractor to support this format. The following code snippet extracts records for TSV file that is stored as a TriplyDB Asset : fromTsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.tsv.gz' } ) ), TriplyETL supports the IANA standard definition of the TSV format. Record representation TriplyETL treats every row in a TSV source as one record. The columns are emitted as keys and the cells are emitted as values. All values are of type string . Cells that contain the empty string are treated as denoting an empty value and are excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys or values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following TSV snippet: ID Name Age 1 Doe, John 32 2 D., Jane which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice that: - All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the TSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. - The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. - The \"Age\" key is missing from the second record, since the corresponding TSV cell contains the empty string, which is considered to denote an empty value. Extractor fromXlsx() {#fromXlsx} XLSX or Office Open XML Workbook (file name extension .xlsx ) is a popular format for storing tabular source data. This is the standard file format for Microsoft Excel. TriplyETL has a dedicated fromXlsx() extractor for such sources. The following code snippet shows how a TriplyDB assets is used to process records from an XLSX source: fromXlsx( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.xlsx' } ) ), The fromXlsx() extractor emits one record per row in the source file. Sheets It is common for XLSX files to have multiple sheets. By default the fromXlsx() extractor enumerates all rows from all sheets as records. If only some sheets should be used, this can be specified as a configuration option. The following code snippet only emits records/rows from the 'people' and 'projects' sheets in the XLSX source file 'example.xlsx' . Rows from other sheets in the same XLSX file are not emitted: fromXlsx(Source.file('example.xlsx'), { sheetNames: ['people', 'projects'] }), Record representation TriplyETL treats every row in XLSX sheet as one record. The columns are emitted as keys and the cells are emitted as values. Unlike other tabular formats like CSV and TSV , values in XLSX can have different types. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be emitted as the following two TriplyETL records: { \"$recordId\": 1, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": 32 } { \"$recordId\": 2, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"2\", \"Name\": \"D., Jane\", } Notice the following: - The value for the \"Age\" key is a number. - The special keys $recordId , $environment , and $fileName are documented in the section on Special Keys . - The special key $sheetName is unique to the fromXslx() extractor and is documented in the next subsection. Special key $sheetName {#sheetName} For every record emitted by the fromXlsx() extractor. the $sheetName special key contains the name of the Excel sheet from which that record originates. The presence of the sheet name allows the TriplyETL configuration to be adjusted for different sheet. For example, an Excel spreadsheet may contain a 'companies' sheet and a 'persons' sheet. The name of the sheet may be used to determine which class should be asserted. The following snippet uses transformation translateAll() to map sheet names to class IRIs: fromXlsx(Source.file('example.xlsx')), translateAll({ content: '$sheetName', table: { 'companies': sdo.Organization, 'persons': sdo.Person, }, key: '_class', }), triple(iri(prefix.id, '$recordId'), a, '_class'), Extractor fromPostgres() {#fromPostgres} PostgreSQL or Postgres is an open-source relational database system. Postgres supports both SQL (relational) and JSON (non-relational) querying. TriplyETL has a fromPostgres() extractor to retrieve data from a Postgres database. This can be done via Postgres connectors or via URL using the following code snippet: fromPostgres(Query, Options), Retrieving data via Postgres connectors The first option for retrieving Postgres data is by using Postgres connectors. Below is an example using a publicly available database from RNA central . fromPostgres( 'SELECT * FROM rnc_database', { host: 'hh-pgsql-public.ebi.ac.uk', port: 5432, database: 'pfmegrnargs', user: 'reader', password: 'NWDMCE5xdipIjRrp', } ), Retrieving data via URL The second option is by simply using a database URL. Below we are using the same example database as above, but this time instead of adding Postgres connectors, we are accessing it via URL. fromPostgres( 'select * from rnc_database', { url: 'postgres://reader:NWDMCE5xdipIjRrp@hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs' } ), Extractor fromXml() {#fromXml} XML or Extensible Markup Language is a popular open format for tree-shaped source data. The following snippets connects to an XML file that is made available as a TriplyDB asset : fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: 'first-element' } ), Notice that the fromXml() extractor requires a selectors option. This specifies the subtrees in the XML that should be treated as individual records. In the above snippet the records are the subtrees that occur between the <first-element> opening tag and the </first-element> closing tag. If a deeper path must be specified, sequential tags in the path must be separated by a dot: fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: 'first-element.second-element.third-element' } ), It is common for large XML sources to contain different kinds of records. Different kinds of records often occur under different paths. It is therefore possible to specify multiple paths, all of which will be used for extract records from the XML source. The following code snippet extracts records for three different paths in the same XML source: fromXml( Source.TriplyDb.asset('my-dataset', { name: 'my-data.xml' }), { selectors: [ 'first-element.second-element.third-element', 'first-element.second-element.alt-element', 'first-element.second-element.other-element', ] } ), TriplyETL supports the W3C XML standard. Nested keys Since XML can store tree-shaped data, it can have nested keys and indexed array. See the following subsections of the JSON documentation for how to extract data from such tree structures: Nested keys Dealing with dots in keys Accessing lists by index Function loadRdf() {#loadRdf} Resource Description Framework (RDF) is the standardized and open format for linked data. Data in the RDF format is treated in a different way than other formats. RDF data does not appear in clearly structured records, it no longer needs to be asserted (since it already is linked data), and transformations can be performed with linked data standards such as SHACL Rules. For this reason, RDF sources are directly loaded into the Internal Store, and the Record step is completely skipped. The following code snippet loads RDF from the specified TriplyDB dataset into the Internal Store: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), The following code snippet loads RDF from a SPARQL Construct that is stored in TriplyDB: loadRdf(Source.TriplyDb.query('Triply', 'network-query')), The following code snippet loads RDF from a SPARQL Construct query that is stored in TriplyDB: loadRdf(Source.TriplyDb.query('Triply', 'network-query')), Loading RDF from an HTML page With loadRdf() extractor, it is also possible to extract data from web pages / HTML, which contain Schema in JSON-LD. This is possible because most websites contain linked data annotations that use Schema.org. Such LD is enclosed in tag ... It is possible to load such linked data with TriplyETL. Schema markup is how Google can serve up rich results (also called rich snippets and rich cards). The schema is included in HTML in the following way: The Script Type: What format your structured data will take (JSON-LD) The Context: Where the language you\u2019re using comes from (schema.org) The Type: What kind of thing is the search engine looking at (an image) The Property: What kind of quality will you be describing when it comes to this type (the license document) The Value: What you\u2019re actually telling the search engines about this property (the URL where the license can be found) Example taken from Wikipedia: The Wikipedia page of the first programmer in history (https://en.wikipedia.org/wiki/Ada_Lovelace) contains the following linked data: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"name\": \"Ada Lovelace\", \"url\": \"https://en.wikipedia.org/wiki/Ada_Lovelace\", \"sameAs\": \"http://www.wikidata.org/entity/Q7259\", \"mainEntity\": \"http://www.wikidata.org/entity/Q7259\", \"author\": { \"@type\": \"Organization\", \"name\": \"Contributors to Wikimedia projects\" }, \"publisher\": { \"@type\": \"Organization\", \"name\": \"Wikimedia Foundation, Inc.\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.wikimedia.org/static/images/wmf-hor-googpub.png\" } }, \"datePublished\": \"2001-05-20T14:57:05Z\", \"dateModified\": \"2023-03-17T21:28:23Z\", \"image\": \"https://upload.wikimedia.org/wikipedia/commons/0/0b/Ada_Byron_daguerreotype_by_Antoine_Claudet_1843_or_1850.jpg\", \"headline\": \"1815-1852 British mathematician, considered the first computer programmer\" } This data can be loaded with TriplyETL in the following way: import { Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default () => { const etl = new Etl() etl.use( loadRdf(Source.url('https://en.wikipedia.org/wiki/Ada_Lovelace', { contentType: 'text/html' })), ) return etl }","title":"1. Extract: Data Formats"},{"location":"triply-etl/extract/formats/#overview","text":"TriplyETL supports the following data formats: Extractor Format Full name fromCsv() CSV Comma-Separated Values fromJson() JSON JavaScript Object Notation fromOai() OAI-PMH Open Archives Initiative Protocol for Metadata Harvesting fromPostgres() PostgreSQL Query & Postgres API Options PostgreSQL Query & Postgres API Options fromShapefile() ESRI ESRI Shapefiles fromTsv() TSV Tab-Separated Values fromXlsx() XLSX Microsoft Excel fromXml() XML XML Markup Language loadRdf() RDF Resource Description Format All extractors can be imported from the generic library in TriplyETL: import { fromCsv, fromJson, fromOai, fromPostgres, fromShapefile, fromTsv, fromXlsx, fromXml, loadRdf, Source } from '@triplyetl/etl/generic' Notice that you also need to import Source , since every extractor requires a source.","title":"Overview"},{"location":"triply-etl/extract/formats/#extractor-fromcsv-fromcsv","text":"CSV or Comma Separated Values (file name extension .csv ) is a popular format for storing tabular source data. TriplyETL has a dedicated fromCsv() extractor for this data format. The following code snippet extracts records from a local CSV file: fromCsv(Source.file('data.csv')), The following code snippet extracts records from an online CSV file, that is hosted at the specified URL: fromCsv(Source.url('https://somewhere.com/data.csv')), The following code snippet extracts records from a TriplyDB Asset . The asset is store in the data with name 'some-data' , under an account with name 'some-account' . The name of the asset is 'example.csv' : fromCsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.csv' } ) ),","title":"Extractor fromCsv() {#fromCsv}"},{"location":"triply-etl/extract/formats/#standards-compliance","text":"The fromCsv() extractor implements the official CSV standard: IETF RFC 4180 . Some CSV files do not follow the standard precisely. In order to process such CSV files, the default behavior of the extractor can be changed through an optional options parameter. See the CSV Parse for Node.js documentation for all the available options.","title":"Standards-compliance"},{"location":"triply-etl/extract/formats/#configure-the-encoding","text":"According to the official CSV standard, CSV sources are allowed to use any encoding. Since the CSV format does not allow the used encoding to be specified in the format itself, a non-standard encoding must always be configured manually. By default, TriplyETL assumes that CSV sources use the UTF-8 encoding. If another encoding is used, this must be explicitly specified by using the optional options parameter. The following snippet configures that the CSV source uses the ISO Latin-1 encoding: fromCsv( Source.TriplyDb.asset('my-dataset', { name: 'example.csv' }), { encoding: 'latin1' } ), The following encodings are currently supported: Value Encoding Standard Alternative values 'ascii' US-ASCII ANSI 'latin1' Latin-1 ISO-8859-1 binary 'utf8' UTF-8 Unicode 'utf16le' UTF-16 Little Endian Unicode 'ucs2' , 'ucs-2' , 'utf16-le' Read the CSV Parse for Node.js documentation for more information.","title":"Configure the encoding"},{"location":"triply-etl/extract/formats/#use-a-different-separator","text":"Some CSV files only deviate in their use of a different separator character. For example, some CSV files use the semi-colon ( ; ) or the at-sign ( @ ) for this. The following snippet extracts records for a CSV file that uses the semi-colon ( ; ) as the separator character: fromCsv(Source.file('example.csv'), { separator: ';' }),","title":"Use a different separator"},{"location":"triply-etl/extract/formats/#csv-with-tab-separators-is-not-tsv","text":"Notice that the popular Tab-Separate Values (TSV) format is not the same as the standardized CSV format with a tab separator character. If you want to process standards-conforming TSV source data, use the fromTsv() extractor instead.","title":"CSV with tab separators is not TSV"},{"location":"triply-etl/extract/formats/#record-representation","text":"TriplyETL treats every row in a CSV source as one record. The columns are emitted as keys and the cells are emitted as values. All values are of type string . Empty cells (i.e. those containing the empty string) are treated as denoting a null value and are therefore excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys and values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following CSV snippet: ID,Name,Age 1,\"Doe, John\",32 2,\"D., Jane \", which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice that: - All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the CSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. - The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. - The \"Age\" key is missing from the second record, since the corresponding CSV cell contains the empty string, which is considered to denote an empty value.","title":"Record representation"},{"location":"triply-etl/extract/formats/#extractor-fromjson-fromjson","text":"JSON (JavaScript Object Notation) is a popular open standard for interchanging tree-shaped data. TriplyETL has a dedicated fromJson() extractor for this format. The following code snippet connects to a JSON source that is stored as a TriplyDB asset : fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json.gz' } ) ), The following example uses an in-line specified JSON source: fromJson([{ a: 'a', b: 'b', c: 'c' }]), TriplyETL supports the IETF RFC 8259 standard for JSON.","title":"Extractor fromJson() {#fromJson}"},{"location":"triply-etl/extract/formats/#nested-keys","text":"Since JSON is a tree-shaped format, it is able to store values in a nested structure. This requires a sequence or 'path' of keys to be specified. We use the following example data: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"country.id\": \"nl\", \"name\": \"The Netherlands\" }, { \"country.id\": \"de\", \"name\": \"Germany\" } ] } } Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the JSON example in the previous section, TriplyETL can access the \"name\" key inside the \"title\" key, which itself is nested inside the \"metadata\" key. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which also accesses the \"name\" key, but nested inside the \"countries\" and then \"data\" keys. (The use of the [0] index is explained in the next section.) [1] metadata.title.name [2] data.countries[0].name Path expressions can be used as string keys in many places in TriplyETL. For example, we can assert the title of a dataset in the following way: triple('_dataset', dct.title, 'metadata.title.name'), This asserts the following linked data: dataset:my-dataset dct:title 'Data about countries.'.","title":"Nested keys"},{"location":"triply-etl/extract/formats/#dealing-with-dots-in-keys","text":"In the previous section we saw that dots are used to separate keys in paths. However, sometimes a dot can occur as a regular character inside a key. In such cases, we need to apply additional escaping of the key name to avoid naming conflicts. The example data from the previous section contains the following key: \"country.id\" Notice that the dot is here part of the key name. We can refer to these keys as follows: triple('_country', dct.id, 'data.countries[0].[\"country.id\"]'), Notice the use of additional escaping: [\"...\"]","title":"Dealing with dots in keys"},{"location":"triply-etl/extract/formats/#accessing-lists-by-index","text":"Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. TriplyETL is able to access specific elements from lists based on theirindex* or position. Following the standard practice in Computer Science, TriplyETL refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of thefirst* country as follows: triple( iri(prefix.id, 'data.countries[0].[\"country.id\"]'), rdfs.label, 'data.countries[0].name' ), This results in the following linked data: id:nl rdfs:label 'The Netherlands'. We can also assert the name of the second country. Notice that only the index is different ( 1 instead of 0 ): triple( iri(prefix.id, 'data.countries[1].[\"country.id\"]'), rdfs.label, 'data.countries[1].name' ), This results in the following linked data: id:de rdfs:label 'Germany'.","title":"Accessing lists by index"},{"location":"triply-etl/extract/formats/#extractor-fromoai-fromoai","text":"In GLAM domains (Galleries, Libraries, Archives, Museums), the Open Archives Initiative (OAI), Protocol for Metadata Harvesting (PMH) is a popular protocol and format for publishing data collections. TriplyETL includes the fromOai() extractor to tap into these data collections. The fromOai() extractor ensures a continuous stream of data records. Under the hood, the extractor uses resumption tokens to iterate over large collections. An OAI-PMH endpoint can be configured by specifying its URL (parameter url ). Since one OAI-PMH endpoint typically publishes multiple datasets, it is also common to specify the set parameter. The following code snippet connects to an example dataset that is published in an example OAI-PMH endpoint: fromOai({ set: 'some-dataset', url: 'https://somewhere.com/webapioai/oai.ashx' }), TriplyETL supports the official OAI-PMH standard. The OAI-PMH standard defines 6 'verbs'. These different sub-APIs that together component the OAI-PMH API. Extractor fromOai() currently supports the following two verbs: ListIdentifiers and ListRecords .","title":"Extractor fromOai() {#fromOai}"},{"location":"triply-etl/extract/formats/#verb-listidentifiers-listidentifiers","text":"This 'verb' or sub-API streams through the headers of all records. It does not returns the actual (body) content of each record (see ListRecords ). This verb can be used to look for header properties like set membership, datestamp, and deletion status. The following code snippet streams through the headers of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListIdentifiers' }), logRecord(),","title":"Verb ListIdentifiers {#ListIdentifiers}"},{"location":"triply-etl/extract/formats/#verb-listrecords-listrecords","text":"This 'verb' or sub-API streams through all records and retrieves them in full. This API is used to harvest records. The following code snippet streams through the records of a public OAI-PMH endpoint: fromOai({ metadataPrefix: 'marcxml', set: 'iish.evergreen.biblio', url: 'https://api.socialhistoryservices.org/solr/all/oai', verb: 'ListRecords' }), logRecord(),","title":"Verb ListRecords {#ListRecords}"},{"location":"triply-etl/extract/formats/#extractor-fromtsv-fromtsv","text":"TSV or Tab-Separated Values (file name extension .tsv ) is a popular format for tabular source data. TriplyETL has a fromTsv() extractor to support this format. The following code snippet extracts records for TSV file that is stored as a TriplyDB Asset : fromTsv( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.tsv.gz' } ) ), TriplyETL supports the IANA standard definition of the TSV format.","title":"Extractor fromTsv() {#fromTsv}"},{"location":"triply-etl/extract/formats/#record-representation_1","text":"TriplyETL treats every row in a TSV source as one record. The columns are emitted as keys and the cells are emitted as values. All values are of type string . Cells that contain the empty string are treated as denoting an empty value and are excluded from the record. Any trailing whitespace that appears in headers or cells is removed from the keys or values in the record. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be expressed by the following TSV snippet: ID Name Age 1 Doe, John 32 2 D., Jane which is emitted as the following two TriplyETL records: { \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": \"32\" } { \"ID\": \"2\", \"Name\": \"D., Jane\" } Notice that: - All values have type string , including \"ID\" and \"Age\" . The value for field \"Age\" should probably be considered numeric, but the TSV format cannot express this. A TriplyETL transformation can be used to cast string values to numeric values. - The trailing space in \"D., Jane \" is omitted from the second record, since training whitespace is removed from all keys and values. - The \"Age\" key is missing from the second record, since the corresponding TSV cell contains the empty string, which is considered to denote an empty value.","title":"Record representation"},{"location":"triply-etl/extract/formats/#extractor-fromxlsx-fromxlsx","text":"XLSX or Office Open XML Workbook (file name extension .xlsx ) is a popular format for storing tabular source data. This is the standard file format for Microsoft Excel. TriplyETL has a dedicated fromXlsx() extractor for such sources. The following code snippet shows how a TriplyDB assets is used to process records from an XLSX source: fromXlsx( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.xlsx' } ) ), The fromXlsx() extractor emits one record per row in the source file.","title":"Extractor fromXlsx() {#fromXlsx}"},{"location":"triply-etl/extract/formats/#sheets","text":"It is common for XLSX files to have multiple sheets. By default the fromXlsx() extractor enumerates all rows from all sheets as records. If only some sheets should be used, this can be specified as a configuration option. The following code snippet only emits records/rows from the 'people' and 'projects' sheets in the XLSX source file 'example.xlsx' . Rows from other sheets in the same XLSX file are not emitted: fromXlsx(Source.file('example.xlsx'), { sheetNames: ['people', 'projects'] }),","title":"Sheets"},{"location":"triply-etl/extract/formats/#record-representation_2","text":"TriplyETL treats every row in XLSX sheet as one record. The columns are emitted as keys and the cells are emitted as values. Unlike other tabular formats like CSV and TSV , values in XLSX can have different types. For example, the following table: ID Name Age 1 Doe, John 32 2 D., Jane can be emitted as the following two TriplyETL records: { \"$recordId\": 1, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"1\", \"Name\": \"Doe, John\", \"Age\": 32 } { \"$recordId\": 2, \"$environment\": \"Development\", \"$sheetName\": \"Sheet1\", \"$fileName\": \"static/Untitled 1.xlsx\", \"ID\": \"2\", \"Name\": \"D., Jane\", } Notice the following: - The value for the \"Age\" key is a number. - The special keys $recordId , $environment , and $fileName are documented in the section on Special Keys . - The special key $sheetName is unique to the fromXslx() extractor and is documented in the next subsection.","title":"Record representation"},{"location":"triply-etl/extract/formats/#special-key-sheetname-sheetname","text":"For every record emitted by the fromXlsx() extractor. the $sheetName special key contains the name of the Excel sheet from which that record originates. The presence of the sheet name allows the TriplyETL configuration to be adjusted for different sheet. For example, an Excel spreadsheet may contain a 'companies' sheet and a 'persons' sheet. The name of the sheet may be used to determine which class should be asserted. The following snippet uses transformation translateAll() to map sheet names to class IRIs: fromXlsx(Source.file('example.xlsx')), translateAll({ content: '$sheetName', table: { 'companies': sdo.Organization, 'persons': sdo.Person, }, key: '_class', }), triple(iri(prefix.id, '$recordId'), a, '_class'),","title":"Special key $sheetName {#sheetName}"},{"location":"triply-etl/extract/formats/#extractor-frompostgres-frompostgres","text":"PostgreSQL or Postgres is an open-source relational database system. Postgres supports both SQL (relational) and JSON (non-relational) querying. TriplyETL has a fromPostgres() extractor to retrieve data from a Postgres database. This can be done via Postgres connectors or via URL using the following code snippet: fromPostgres(Query, Options),","title":"Extractor fromPostgres() {#fromPostgres}"},{"location":"triply-etl/extract/formats/#retrieving-data-via-postgres-connectors","text":"The first option for retrieving Postgres data is by using Postgres connectors. Below is an example using a publicly available database from RNA central . fromPostgres( 'SELECT * FROM rnc_database', { host: 'hh-pgsql-public.ebi.ac.uk', port: 5432, database: 'pfmegrnargs', user: 'reader', password: 'NWDMCE5xdipIjRrp', } ),","title":"Retrieving data via Postgres connectors"},{"location":"triply-etl/extract/formats/#retrieving-data-via-url","text":"The second option is by simply using a database URL. Below we are using the same example database as above, but this time instead of adding Postgres connectors, we are accessing it via URL. fromPostgres( 'select * from rnc_database', { url: 'postgres://reader:NWDMCE5xdipIjRrp@hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs' } ),","title":"Retrieving data via URL"},{"location":"triply-etl/extract/formats/#extractor-fromxml-fromxml","text":"XML or Extensible Markup Language is a popular open format for tree-shaped source data. The following snippets connects to an XML file that is made available as a TriplyDB asset : fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: 'first-element' } ), Notice that the fromXml() extractor requires a selectors option. This specifies the subtrees in the XML that should be treated as individual records. In the above snippet the records are the subtrees that occur between the <first-element> opening tag and the </first-element> closing tag. If a deeper path must be specified, sequential tags in the path must be separated by a dot: fromXml( Source.TriplyDb.asset('my-dataset', {name: 'my-data.xml'}), { selectors: 'first-element.second-element.third-element' } ), It is common for large XML sources to contain different kinds of records. Different kinds of records often occur under different paths. It is therefore possible to specify multiple paths, all of which will be used for extract records from the XML source. The following code snippet extracts records for three different paths in the same XML source: fromXml( Source.TriplyDb.asset('my-dataset', { name: 'my-data.xml' }), { selectors: [ 'first-element.second-element.third-element', 'first-element.second-element.alt-element', 'first-element.second-element.other-element', ] } ), TriplyETL supports the W3C XML standard.","title":"Extractor fromXml() {#fromXml}"},{"location":"triply-etl/extract/formats/#nested-keys_1","text":"Since XML can store tree-shaped data, it can have nested keys and indexed array. See the following subsections of the JSON documentation for how to extract data from such tree structures: Nested keys Dealing with dots in keys Accessing lists by index","title":"Nested keys"},{"location":"triply-etl/extract/formats/#function-loadrdf-loadrdf","text":"Resource Description Framework (RDF) is the standardized and open format for linked data. Data in the RDF format is treated in a different way than other formats. RDF data does not appear in clearly structured records, it no longer needs to be asserted (since it already is linked data), and transformations can be performed with linked data standards such as SHACL Rules. For this reason, RDF sources are directly loaded into the Internal Store, and the Record step is completely skipped. The following code snippet loads RDF from the specified TriplyDB dataset into the Internal Store: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), The following code snippet loads RDF from a SPARQL Construct that is stored in TriplyDB: loadRdf(Source.TriplyDb.query('Triply', 'network-query')), The following code snippet loads RDF from a SPARQL Construct query that is stored in TriplyDB: loadRdf(Source.TriplyDb.query('Triply', 'network-query')),","title":"Function loadRdf() {#loadRdf}"},{"location":"triply-etl/extract/formats/#loading-rdf-from-an-html-page","text":"With loadRdf() extractor, it is also possible to extract data from web pages / HTML, which contain Schema in JSON-LD. This is possible because most websites contain linked data annotations that use Schema.org. Such LD is enclosed in tag ... It is possible to load such linked data with TriplyETL. Schema markup is how Google can serve up rich results (also called rich snippets and rich cards). The schema is included in HTML in the following way: The Script Type: What format your structured data will take (JSON-LD) The Context: Where the language you\u2019re using comes from (schema.org) The Type: What kind of thing is the search engine looking at (an image) The Property: What kind of quality will you be describing when it comes to this type (the license document) The Value: What you\u2019re actually telling the search engines about this property (the URL where the license can be found) Example taken from Wikipedia: The Wikipedia page of the first programmer in history (https://en.wikipedia.org/wiki/Ada_Lovelace) contains the following linked data: { \"@context\": \"https://schema.org\", \"@type\": \"Article\", \"name\": \"Ada Lovelace\", \"url\": \"https://en.wikipedia.org/wiki/Ada_Lovelace\", \"sameAs\": \"http://www.wikidata.org/entity/Q7259\", \"mainEntity\": \"http://www.wikidata.org/entity/Q7259\", \"author\": { \"@type\": \"Organization\", \"name\": \"Contributors to Wikimedia projects\" }, \"publisher\": { \"@type\": \"Organization\", \"name\": \"Wikimedia Foundation, Inc.\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://www.wikimedia.org/static/images/wmf-hor-googpub.png\" } }, \"datePublished\": \"2001-05-20T14:57:05Z\", \"dateModified\": \"2023-03-17T21:28:23Z\", \"image\": \"https://upload.wikimedia.org/wikipedia/commons/0/0b/Ada_Byron_daguerreotype_by_Antoine_Claudet_1843_or_1850.jpg\", \"headline\": \"1815-1852 British mathematician, considered the first computer programmer\" } This data can be loaded with TriplyETL in the following way: import { Etl, loadRdf, Source } from '@triplyetl/etl/generic' export default () => { const etl = new Etl() etl.use( loadRdf(Source.url('https://en.wikipedia.org/wiki/Ada_Lovelace', { contentType: 'text/html' })), ) return etl }","title":"Loading RDF from an HTML page"},{"location":"triply-etl/extract/record/","text":"When a TriplyETL is connected to one of more data sources, a stream of Records will be generated. Records use a generic representation that is independent of the format used in the data sources. The generic Record We illustrate the representation of the generic Record with the following code snippet. This snippet uses extractor fromJson() to extract data from inline JSON source data: import { Etl, fromJson, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two inline records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key for more information). Now suppose that we change the source system. We no longer use inline JSON, but a local XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use extractor fromXml() and the local file source type: import { Etl, fromXml, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary Record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source extractor needs to be changed, and all transformations and assertions remain as they were. Special keys Records in TriplyETL contain several special keys. These special keys start with a dollar sign character ( $ ). The special keys contain values that are inserted during the Extract step. These special keys can be used in the same way as regular keys in your TriplyETL configuration. We now discuss these special keys in detail. Special key $recordId The special key $recordId assigns a unique number to every record that is processed in one single run of a TriplyETL pipeline. If the source data does not change, multiple runs of the TriplyETL pipeline will always generate the same record IDs. However, if source data changes, multiple runs of the TriplyETL pipeline may generate different record IDs for the same record. Use case: Unique identifiers The first main use case of the $recordId key is to create IRIs that are unique within one single run of a TriplyETL pipeline. Suppose the following table is our source data: First name Last name John Doe Jane Doe John Doe We need to create an IRI for every person in this table. Notice that the table contains no unique properties: there are two different persons with the same first and last name. This means that we cannot use the keys \"First name\" and \"Last name\" in our record in order to create our IRIs. Luckily, the source connector adds the $recordId for us: { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 1 } { \"First name\": \"Jane\", \"Last name\": \"Doe\", \"$recordId\": 2 } { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 3 } This allows us to make the following assertion: pairs(iri(prefix.id, '$recordId'), [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), Which results in the following linked data: id:1 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. id:2 a sdo:Person; sdo:givenName 'Jane'; sdo:familyName 'Doe'. id:3 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. Notice that the use of the $recordId results in a correct single run of the TriplyETL pipeline. But if the source data changes, the IRIs may change as well. For example, if the first and second row in the source table are swapped, the IRI that denotes \"Jane Doe\" will change from id:2 to id:1 . Use case: Debugging When you are debugging the configuration of a TriplyETL pipeline, it is sometimes useful to perform a specific actions for a specific record. Assuming the stream of records is stable during the debugging effort, the $recordId key can be used to perform such a debugging action; for example: whenEqual('$recordId', 908, logRecord()), Do note that it is generally better to run the TriplyETL for a specific record using the --from-record-id 908 --head 1 command line flags (see CLI ). Special key $environment The TriplyETL record contains special key $environment . Its value denotes the DTAP environment that the pipeline is currently running in. This is one of the followin values: \"Development\", \"Test\", \"Acceptance\", or \"Production\". See the Automation tutorial for more information. Special key $sheetName The special key $sheetName only occurs in records that original from data source that use the Microsoft Excel format. In such records, this special key contains the name of the sheet from which the record originats. See the documentation for the Microsoft Excel format for more information about this special key.","title":"1. Extract: Record"},{"location":"triply-etl/extract/record/#the-generic-record","text":"We illustrate the representation of the generic Record with the following code snippet. This snippet uses extractor fromJson() to extract data from inline JSON source data: import { Etl, fromJson, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two inline records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key for more information). Now suppose that we change the source system. We no longer use inline JSON, but a local XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use extractor fromXml() and the local file source type: import { Etl, fromXml, logRecord } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary Record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source extractor needs to be changed, and all transformations and assertions remain as they were.","title":"The generic Record"},{"location":"triply-etl/extract/record/#special-keys","text":"Records in TriplyETL contain several special keys. These special keys start with a dollar sign character ( $ ). The special keys contain values that are inserted during the Extract step. These special keys can be used in the same way as regular keys in your TriplyETL configuration. We now discuss these special keys in detail.","title":"Special keys"},{"location":"triply-etl/extract/record/#special-key-recordid","text":"The special key $recordId assigns a unique number to every record that is processed in one single run of a TriplyETL pipeline. If the source data does not change, multiple runs of the TriplyETL pipeline will always generate the same record IDs. However, if source data changes, multiple runs of the TriplyETL pipeline may generate different record IDs for the same record.","title":"Special key $recordId"},{"location":"triply-etl/extract/record/#use-case-unique-identifiers","text":"The first main use case of the $recordId key is to create IRIs that are unique within one single run of a TriplyETL pipeline. Suppose the following table is our source data: First name Last name John Doe Jane Doe John Doe We need to create an IRI for every person in this table. Notice that the table contains no unique properties: there are two different persons with the same first and last name. This means that we cannot use the keys \"First name\" and \"Last name\" in our record in order to create our IRIs. Luckily, the source connector adds the $recordId for us: { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 1 } { \"First name\": \"Jane\", \"Last name\": \"Doe\", \"$recordId\": 2 } { \"First name\": \"John\", \"Last name\": \"Doe\", \"$recordId\": 3 } This allows us to make the following assertion: pairs(iri(prefix.id, '$recordId'), [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), Which results in the following linked data: id:1 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. id:2 a sdo:Person; sdo:givenName 'Jane'; sdo:familyName 'Doe'. id:3 a sdo:Person; sdo:givenName 'John'; sdo:familyName 'Doe'. Notice that the use of the $recordId results in a correct single run of the TriplyETL pipeline. But if the source data changes, the IRIs may change as well. For example, if the first and second row in the source table are swapped, the IRI that denotes \"Jane Doe\" will change from id:2 to id:1 .","title":"Use case: Unique identifiers"},{"location":"triply-etl/extract/record/#use-case-debugging","text":"When you are debugging the configuration of a TriplyETL pipeline, it is sometimes useful to perform a specific actions for a specific record. Assuming the stream of records is stable during the debugging effort, the $recordId key can be used to perform such a debugging action; for example: whenEqual('$recordId', 908, logRecord()), Do note that it is generally better to run the TriplyETL for a specific record using the --from-record-id 908 --head 1 command line flags (see CLI ).","title":"Use case: Debugging"},{"location":"triply-etl/extract/record/#special-key-environment","text":"The TriplyETL record contains special key $environment . Its value denotes the DTAP environment that the pipeline is currently running in. This is one of the followin values: \"Development\", \"Test\", \"Acceptance\", or \"Production\". See the Automation tutorial for more information.","title":"Special key $environment"},{"location":"triply-etl/extract/record/#special-key-sheetname","text":"The special key $sheetName only occurs in records that original from data source that use the Microsoft Excel format. In such records, this special key contains the name of the sheet from which the record originats. See the documentation for the Microsoft Excel format for more information about this special key.","title":"Special key $sheetName"},{"location":"triply-etl/extract/types/","text":"Overview This page documents the different data source types that can be used in TriplyETL: Source type Description Local files Local files that contain data. Online files Online files that contain data. APIs APIs that return data. TriplyDB assets Files stored in TriplyDB ('Assets'). TriplyDB datasets Linked datasets stored in TriplyDB graphs. TriplyDB queries Saved queries in TriplyDB that return data. Inline JSON A JSON object or an array of JSON objects. Strings A string serialization of data. Local files The following code snippet extracts records from a local file that uses the JSON format : fromJson(Source.file('./static/example.json')), It is possible to specify an arbitrary number of local files by using array notation: fromJson(Source.file([ './static/data-001.json', './static/data-002.json', ..., './static/data-999.json', ])), Notice that local files are not typically used in production systems, since it is difficult to guarantee that all project partners have exactly the same local files on their computer. The risk of using outdated files, and the overhead of securely sharing them with multiple team members, are often sufficient reason to use TriplyDB assets instead. Online files The following code snippet connects to a JSON source that is stored in a publicly accessible location on the Internet: fromJson(Source.url('https://somewhere.com/example.json')), If needed, you can configure details about how the HTTP request should be made made. This can be done with the optional options parameter. All options provided by the node-fetch library can be used. For example, the following requests private data that is accessed using basic authentication with username and password: fromJson(Source.url( 'https://somewhere.com/example.json', { request: { headers: { Authorization: `Basic ${username}:${password}` } } } )), Use in production systems Online files are typically not used in production pipelines, because the availability of many Internet resources is outside of the control of the project team. Internet resources that are not maintained by team members may be subject to content-wise changes, which may affect the production pipeline. If the project team controls the Internet resources, then risks are smaller. But at that point it is even better to upload the online files as TriplyDB asset for additional benefits such as access controls. APIs The URL source type can also be used to extract records from online endpoints and APIs. The following code snippet extracts records from a TriplyDB REST API: fromJson(Source.url('https://api.triplydb.com/datasets')), Raw SPARQL endpoints SPARQL endpoints are online APIs. The following code snippet issues a raw SPARQL query against a public SPARQL endpoint. Since we specified CSV as the result set format (Media Type text/csv ), the result set can be accessed as any other CSV source: fromCsv( Source.url( 'https://dbpedia.org/sparql', { request: { headers: { accept: 'text/csv', 'content-type': 'application/query-string', }, body: 'select * { ?s ?p ?o. } limit 1', method: 'POST', }, } ) ) Use in production systems Raw SPARQL endpoints lack several features that are essential for use in production systems: - secure access control - pagination - reliable retrieval of large result sets - API variables - versioning These features are all supported by TriplyDB queries . It is therefore simpler and safer to use TriplyDB queries. Still, when used outside of production systems, raw SPARQL endpoints can still be used as regular web APIs. TriplyDB assets {#triplydb-assets} Assets are a core feature of TriplyDB. Assets allow arbitrary files to be stored in the context of a linked dataset. A typical use case for assets is to upload (new versions of) source files. The TriplyETL pipeline can pick the latest versions of these source files and publish the resulting linked data in the the same dataset. The following code snippet uses a JSON source that is stored in a TriplyDB asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json' } ) ), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf( Source.TriplyDb.rdf('my-dataset', { name: 'example.json' }) ), As with other source type, multiple assets can be specified: fromCsv([ Source.TriplyDb.asset('my-dataset', { name: 'table1.csv' }), Source.TriplyDb.asset('my-dataset', { name: 'table2.csv' }), ]), Filtering If the asset name is omitted, all assets are returned. This is often unpractical, since only some assets must be processed. For example, if a dataset has PDF and JSON assets, only the latter should be processed by the fromJson() source extractor. For such use cases the filter option can be used instead of the name option. The filter option takes a TypeScript function that maps assets names onto Boolean values (true or false). Only the assets for which the function returns truth are included. The following snippet processes all and only assets whose name ends in .json : fromJson( Source.TriplyDb.asset( 'my-dataset', { filter: name => name.endsWith('json') } ) ), Versioning It is possible to upload new versions of an existing TriplyDB asset. When no specific version is specified, a TriplyETL pipeline will use the latest version automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of an asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json', assetVersion: 2 } ) ), Access Since TriplyDB assets are part of a TriplyDB dataset: - they are accessible under the same access level as the rest of the dataset, and - they are accessible with the same API Token that allows linked data to be published in that dataset. Notice that this makes it easier and safer to deal with source data that is not public. When private data is retrieved from online files or APIs , authorization information must be configured at the HTTP level. This is possible but cumbersome. And, depending on the authentication approach, it is required to create a new API Token and securely configure that in addition to the TriplyDB API Token. Notice that access also is more transparent when TriplyDB assets are used. All and only collaborators that have access to the TriplyDB dataset also have access to the source data. It is clear for all collaborators which source files should be used, and which versions are available. This is more transparent than having to share (multiple versions of) source files over email or by other indirect means. TriplyDB instance {#triplydb-option} By default, assets are loaded from the TriplyDB instance that is associated with the currently used API Token. In some situations it is useful to connect to a linked dataset from a different TriplyDB instance. This can be configured with the triplyDb option. The following snippet loads the OWL vocabulary from TriplyDB.com. Notice that the URL of the API must be specified; this is different from the URL of the web-based GUI. loadRdf( Source.TriplyDb.rdf( 'w3c', 'owl', { triplyDb: { url: 'https://triplydb.com' } } ) ), If an asset is part of a non-public dataset, specifying the URL is insufficient. In such cases an API Token from this other TriplyDB instance must be created and configured using the token option in combination with the url option. Compression Source data is often text-based. This means that such source data can often be compressed to minimize storage space and/or Internet bandwidth. TriplyETL provides automatic support for the GNU zip (file name extension *.gz ) compression format. The following snippet uses a TriplyDB assets that was compressed with GNU zip (file extension *.gz ): fromCsv( Source.TriplyDb.asset('my-dataset', { name: 'example.csv.gz' }) ), TriplyDB datasets Datasets in TriplyDB store linked data in one or more graphs. Such datasets can be loaded as a TriplyETL source. The following snippet loads a dataset from TriplyDB into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), As with other TriplyDB sources, the account name is optional. When omitted, a dataset from the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.rdf('my-dataset')), Graphs option By default, all graphs from a linked dataset are loaded. It is possible to specify a only those graphs that should be loaded. The following snippet only loads the data model, but not the instance data: loadRdf( Source.TriplyDb.rdf( 'my-account', 'my-dataset', { graphs: ['https://example.com/id/graph/model'] } ) ), TriplyDB instance The triplyDb option can be used to specify that a linked dataset from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link TriplyDB queries Saved SPARQL queries in TriplyDB can be used as data sources. SPARQL queries are very powerful data sources, since they allow complex filters to be expressed. There are 4 SPARQL query forms, with different source extractors that can process their results: Query form Source extractor SPARQL Ask fromJson() , fromXml() SPARQL Construct loadRdf() SPARQL Describe loadRdf() SPARQL Select fromCsv() , fromJson() , fromTsv() , fromXml() SPARQL Ask queries {#ask} SPARQL Ask queries can return data in either the JSON or the XML format. This allows them to be processed with the extractors fromCsv() and fromXml() . The following code snippet connects to the XML results of a SPARQL Ask query in TriplyDB: fromXml(Source.TriplyDb.query('my-account', 'my-ask-query')), SPARQL Construct and Describe queries {#construct-describe} SPARQL Construct and Describe queries return data in the RDF format. This allows them to be used with function loadRdf() . The following snippet loads the results of a SPARQL query into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.query('my-account', 'my-construct-query')), SPARQL Select queries {#select} SPARQL Select queries return data in either the CSV, JSON, TSV, or XML format. This allows them to be used with the following four extractors: fromCsv() , fromJson() , fromTsv() , and fromXml() . The following code snippet connects to the table returned by a SPARQL Select query in TriplyDB: fromCsv(Source.TriplyDb.query('my-account', 'my-select-query')), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.query('my-construct-query')), Versioning In production systems, applications must be able to choose whether they want to use the latest version of a query (acceptance mode), or whether they want to use a specific recent version (production mode), or whether they want to use a specific older version (legacy mode). Versioning is supported by TriplyDB saved queries. When no specific version is specified, a TriplyETL pipeline will use the latest version of a query automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of a query: fromJson(Source.TriplyDb.query('my-query', { version: 2 })), Not specifying the version option automatically uses the latest version. API variables In production systems, applications often need to request distinct information based on a limited set of input variables. This is supported in TriplyDB saved queries which API variables. API variables ensure that the query string is parameterized correctly, while adhering to the RDF and SPARQL standards. The following example binds the ?country variable inside the query string to literal 'Holland' . This allows the results for Holland to be returned: fromCsv( Source.TriplyDb.query( 'information-about-countries', { variables: { country: 'Holland' } } ) ), Pagination When a bare SPARQL endpoint is queried as an online API , there are sometimes issues with retrieving the full result set for larger queries. With TriplyDB saved queries, the process of obtaining all results is abstracted away from the user, with the TriplyETL source performing multiple requests in the background as needed. Result graph It is often useful to store the results of SPARQL Construct and Describe queries in a specific graph. For example, when internal data is enriched with external sources, it is often useful to store the external enrichments in a separate graph. Another example is the use of a query that applies RDF(S) and/or OWL reasoning. In such cases the results of the reasoner may be stored in a specific graph. The following snippet stores the results of the specified construct query in a special enrichment graph: loadRdf( Source.TriplyDb.query('my-query', { toGraph: graph.enrichment }) ) This snippet assumes that the graph names have been declared (see Delcarations ). TriplyDB instance The triplyDb option can be used to specify that a query from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link Strings Data in the JSON or RDF formats can be specified with inline strings. The following code snippet loads triples into the Internal Store: loadRdf( Source.string(` prefix person: <https://example.com/id/person/> prefix sdo: <https://schema.org/> person:1 a sdo:Person; sdo:name 'J. Doe'.`), { contentType: 'text/turtle' } ), This loads the following triples: graph LR person:1 -- a --> sdo:Person person:1 -- sdo:name --> J.Doe Notice that we must specify the RDF serialization format that we use. This is necessary because loadRdf() supports a large number of formats, some of which are difficult to autodetect. The following formats are supported: Format contentType value HTML 'text/html' JSON-LD 'application/ld+json' JSON 'application/json' N-Quads 'application/n-quads' N-Triples 'application/n-triples' N3 'text/n3' RDF/XML 'application/rdf+xml' SVG 'image/svg+xml' TriG 'application/trig' Turtle 'text/turtle' XHTML 'application/xhtml+xml' XML 'application/xml' The following example makes RDF source data available to the SHACL validate() function: import { Source } from '@triplyetl/etl/generic' import { validate } from '@triplyetl/etl/shacl' validate(Source.string(` prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://example.com/model/shp/> prefix sdo: <https://schema.org/> shp:Person a sh:NodeShape; sh:property shp:Person_name; sh:targetClass sdo:Person. shp:Person_name a sh:PropertyShape; sh:datatype xsd:string; sh:minLength 1; sh:path sdo:name.`)) This makes the following linked data SHACL specification available: graph LR shp:Person -- a --> sh:NodeShape shp:Person -- sh:property --> shp:Person_name shp:Person -- sh:targetClass --> sdo:Person shp:Person_name -- a --> sh:PropertyShape shp:Person_name -- sh:datatype --> xsd:string shp:Person_name -- sh:minLength --> 1 shp:Person_name -- sh:path --> sdo:name Notice that validate() does not require us to set the content-type, since it only supports N-Quads, N-Triples, TriG and Turtle (and these formats can be detected automatically). The following example makes a string source available to the fromJson() source extractor: fromJson(Source.string(` [ { id: '123', name: 'John' }, { id: '456', name: 'Jane' } ]`)), Notice that the inline JSON source is often a more intuitive specification format for the fromJson() source extractor than its corresponding string source. While inline JSON and string sources are mostly used for small examples, local files are somewhat more widely used. Inline JSON Because TriplyETL configurations are implemented in TypeScript, it is possible to specify JSON data inline with TypeScript Objects. JSON is the only data format that be specified in such a native inline way in TriplyETL. The following code snippet specifies two records using inline TypeScript objects: fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), This results in the following two records: { \"id\": \"123\", \"name\": \"John\" } { \"id\": \"456\", \"name\": \"Jane\" } In documentation, we often use such inline JSON sources since that makes code snippets self-contained, without having to rely on external sources such as files. In production systems this native inline source type is almost never used.","title":"1. Extract: Source types"},{"location":"triply-etl/extract/types/#overview","text":"This page documents the different data source types that can be used in TriplyETL: Source type Description Local files Local files that contain data. Online files Online files that contain data. APIs APIs that return data. TriplyDB assets Files stored in TriplyDB ('Assets'). TriplyDB datasets Linked datasets stored in TriplyDB graphs. TriplyDB queries Saved queries in TriplyDB that return data. Inline JSON A JSON object or an array of JSON objects. Strings A string serialization of data.","title":"Overview"},{"location":"triply-etl/extract/types/#local-files","text":"The following code snippet extracts records from a local file that uses the JSON format : fromJson(Source.file('./static/example.json')), It is possible to specify an arbitrary number of local files by using array notation: fromJson(Source.file([ './static/data-001.json', './static/data-002.json', ..., './static/data-999.json', ])), Notice that local files are not typically used in production systems, since it is difficult to guarantee that all project partners have exactly the same local files on their computer. The risk of using outdated files, and the overhead of securely sharing them with multiple team members, are often sufficient reason to use TriplyDB assets instead.","title":"Local files"},{"location":"triply-etl/extract/types/#online-files","text":"The following code snippet connects to a JSON source that is stored in a publicly accessible location on the Internet: fromJson(Source.url('https://somewhere.com/example.json')), If needed, you can configure details about how the HTTP request should be made made. This can be done with the optional options parameter. All options provided by the node-fetch library can be used. For example, the following requests private data that is accessed using basic authentication with username and password: fromJson(Source.url( 'https://somewhere.com/example.json', { request: { headers: { Authorization: `Basic ${username}:${password}` } } } )),","title":"Online files"},{"location":"triply-etl/extract/types/#use-in-production-systems","text":"Online files are typically not used in production pipelines, because the availability of many Internet resources is outside of the control of the project team. Internet resources that are not maintained by team members may be subject to content-wise changes, which may affect the production pipeline. If the project team controls the Internet resources, then risks are smaller. But at that point it is even better to upload the online files as TriplyDB asset for additional benefits such as access controls.","title":"Use in production systems"},{"location":"triply-etl/extract/types/#apis","text":"The URL source type can also be used to extract records from online endpoints and APIs. The following code snippet extracts records from a TriplyDB REST API: fromJson(Source.url('https://api.triplydb.com/datasets')),","title":"APIs"},{"location":"triply-etl/extract/types/#raw-sparql-endpoints","text":"SPARQL endpoints are online APIs. The following code snippet issues a raw SPARQL query against a public SPARQL endpoint. Since we specified CSV as the result set format (Media Type text/csv ), the result set can be accessed as any other CSV source: fromCsv( Source.url( 'https://dbpedia.org/sparql', { request: { headers: { accept: 'text/csv', 'content-type': 'application/query-string', }, body: 'select * { ?s ?p ?o. } limit 1', method: 'POST', }, } ) )","title":"Raw SPARQL endpoints"},{"location":"triply-etl/extract/types/#use-in-production-systems_1","text":"Raw SPARQL endpoints lack several features that are essential for use in production systems: - secure access control - pagination - reliable retrieval of large result sets - API variables - versioning These features are all supported by TriplyDB queries . It is therefore simpler and safer to use TriplyDB queries. Still, when used outside of production systems, raw SPARQL endpoints can still be used as regular web APIs.","title":"Use in production systems"},{"location":"triply-etl/extract/types/#triplydb-assets-triplydb-assets","text":"Assets are a core feature of TriplyDB. Assets allow arbitrary files to be stored in the context of a linked dataset. A typical use case for assets is to upload (new versions of) source files. The TriplyETL pipeline can pick the latest versions of these source files and publish the resulting linked data in the the same dataset. The following code snippet uses a JSON source that is stored in a TriplyDB asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json' } ) ), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf( Source.TriplyDb.rdf('my-dataset', { name: 'example.json' }) ), As with other source type, multiple assets can be specified: fromCsv([ Source.TriplyDb.asset('my-dataset', { name: 'table1.csv' }), Source.TriplyDb.asset('my-dataset', { name: 'table2.csv' }), ]),","title":"TriplyDB assets {#triplydb-assets}"},{"location":"triply-etl/extract/types/#filtering","text":"If the asset name is omitted, all assets are returned. This is often unpractical, since only some assets must be processed. For example, if a dataset has PDF and JSON assets, only the latter should be processed by the fromJson() source extractor. For such use cases the filter option can be used instead of the name option. The filter option takes a TypeScript function that maps assets names onto Boolean values (true or false). Only the assets for which the function returns truth are included. The following snippet processes all and only assets whose name ends in .json : fromJson( Source.TriplyDb.asset( 'my-dataset', { filter: name => name.endsWith('json') } ) ),","title":"Filtering"},{"location":"triply-etl/extract/types/#versioning","text":"It is possible to upload new versions of an existing TriplyDB asset. When no specific version is specified, a TriplyETL pipeline will use the latest version automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of an asset: fromJson( Source.TriplyDb.asset( 'some-account', 'some-dataset', { name: 'example.json', assetVersion: 2 } ) ),","title":"Versioning"},{"location":"triply-etl/extract/types/#access","text":"Since TriplyDB assets are part of a TriplyDB dataset: - they are accessible under the same access level as the rest of the dataset, and - they are accessible with the same API Token that allows linked data to be published in that dataset. Notice that this makes it easier and safer to deal with source data that is not public. When private data is retrieved from online files or APIs , authorization information must be configured at the HTTP level. This is possible but cumbersome. And, depending on the authentication approach, it is required to create a new API Token and securely configure that in addition to the TriplyDB API Token. Notice that access also is more transparent when TriplyDB assets are used. All and only collaborators that have access to the TriplyDB dataset also have access to the source data. It is clear for all collaborators which source files should be used, and which versions are available. This is more transparent than having to share (multiple versions of) source files over email or by other indirect means.","title":"Access"},{"location":"triply-etl/extract/types/#triplydb-instance-triplydb-option","text":"By default, assets are loaded from the TriplyDB instance that is associated with the currently used API Token. In some situations it is useful to connect to a linked dataset from a different TriplyDB instance. This can be configured with the triplyDb option. The following snippet loads the OWL vocabulary from TriplyDB.com. Notice that the URL of the API must be specified; this is different from the URL of the web-based GUI. loadRdf( Source.TriplyDb.rdf( 'w3c', 'owl', { triplyDb: { url: 'https://triplydb.com' } } ) ), If an asset is part of a non-public dataset, specifying the URL is insufficient. In such cases an API Token from this other TriplyDB instance must be created and configured using the token option in combination with the url option.","title":"TriplyDB instance {#triplydb-option}"},{"location":"triply-etl/extract/types/#compression","text":"Source data is often text-based. This means that such source data can often be compressed to minimize storage space and/or Internet bandwidth. TriplyETL provides automatic support for the GNU zip (file name extension *.gz ) compression format. The following snippet uses a TriplyDB assets that was compressed with GNU zip (file extension *.gz ): fromCsv( Source.TriplyDb.asset('my-dataset', { name: 'example.csv.gz' }) ),","title":"Compression"},{"location":"triply-etl/extract/types/#triplydb-datasets","text":"Datasets in TriplyDB store linked data in one or more graphs. Such datasets can be loaded as a TriplyETL source. The following snippet loads a dataset from TriplyDB into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.rdf('my-account', 'my-dataset')), As with other TriplyDB sources, the account name is optional. When omitted, a dataset from the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.rdf('my-dataset')),","title":"TriplyDB datasets"},{"location":"triply-etl/extract/types/#graphs-option","text":"By default, all graphs from a linked dataset are loaded. It is possible to specify a only those graphs that should be loaded. The following snippet only loads the data model, but not the instance data: loadRdf( Source.TriplyDb.rdf( 'my-account', 'my-dataset', { graphs: ['https://example.com/id/graph/model'] } ) ),","title":"Graphs option"},{"location":"triply-etl/extract/types/#triplydb-instance","text":"The triplyDb option can be used to specify that a linked dataset from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link","title":"TriplyDB instance"},{"location":"triply-etl/extract/types/#triplydb-queries","text":"Saved SPARQL queries in TriplyDB can be used as data sources. SPARQL queries are very powerful data sources, since they allow complex filters to be expressed. There are 4 SPARQL query forms, with different source extractors that can process their results: Query form Source extractor SPARQL Ask fromJson() , fromXml() SPARQL Construct loadRdf() SPARQL Describe loadRdf() SPARQL Select fromCsv() , fromJson() , fromTsv() , fromXml()","title":"TriplyDB queries"},{"location":"triply-etl/extract/types/#sparql-ask-queries-ask","text":"SPARQL Ask queries can return data in either the JSON or the XML format. This allows them to be processed with the extractors fromCsv() and fromXml() . The following code snippet connects to the XML results of a SPARQL Ask query in TriplyDB: fromXml(Source.TriplyDb.query('my-account', 'my-ask-query')),","title":"SPARQL Ask queries {#ask}"},{"location":"triply-etl/extract/types/#sparql-construct-and-describe-queries-construct-describe","text":"SPARQL Construct and Describe queries return data in the RDF format. This allows them to be used with function loadRdf() . The following snippet loads the results of a SPARQL query into the internal RDF store of TriplyETL: loadRdf(Source.TriplyDb.query('my-account', 'my-construct-query')),","title":"SPARQL Construct and Describe queries {#construct-describe}"},{"location":"triply-etl/extract/types/#sparql-select-queries-select","text":"SPARQL Select queries return data in either the CSV, JSON, TSV, or XML format. This allows them to be used with the following four extractors: fromCsv() , fromJson() , fromTsv() , and fromXml() . The following code snippet connects to the table returned by a SPARQL Select query in TriplyDB: fromCsv(Source.TriplyDb.query('my-account', 'my-select-query')), As with other TriplyDB sources, the account name is optional. When omitted, the user account that is associated with the current API Token is used: loadRdf(Source.TriplyDb.query('my-construct-query')),","title":"SPARQL Select queries {#select}"},{"location":"triply-etl/extract/types/#versioning_1","text":"In production systems, applications must be able to choose whether they want to use the latest version of a query (acceptance mode), or whether they want to use a specific recent version (production mode), or whether they want to use a specific older version (legacy mode). Versioning is supported by TriplyDB saved queries. When no specific version is specified, a TriplyETL pipeline will use the latest version of a query automatically. In order to use a specific version, the version option can be set to a version number. The following snippet uses a specific version of a query: fromJson(Source.TriplyDb.query('my-query', { version: 2 })), Not specifying the version option automatically uses the latest version.","title":"Versioning"},{"location":"triply-etl/extract/types/#api-variables","text":"In production systems, applications often need to request distinct information based on a limited set of input variables. This is supported in TriplyDB saved queries which API variables. API variables ensure that the query string is parameterized correctly, while adhering to the RDF and SPARQL standards. The following example binds the ?country variable inside the query string to literal 'Holland' . This allows the results for Holland to be returned: fromCsv( Source.TriplyDb.query( 'information-about-countries', { variables: { country: 'Holland' } } ) ),","title":"API variables"},{"location":"triply-etl/extract/types/#pagination","text":"When a bare SPARQL endpoint is queried as an online API , there are sometimes issues with retrieving the full result set for larger queries. With TriplyDB saved queries, the process of obtaining all results is abstracted away from the user, with the TriplyETL source performing multiple requests in the background as needed.","title":"Pagination"},{"location":"triply-etl/extract/types/#result-graph","text":"It is often useful to store the results of SPARQL Construct and Describe queries in a specific graph. For example, when internal data is enriched with external sources, it is often useful to store the external enrichments in a separate graph. Another example is the use of a query that applies RDF(S) and/or OWL reasoning. In such cases the results of the reasoner may be stored in a specific graph. The following snippet stores the results of the specified construct query in a special enrichment graph: loadRdf( Source.TriplyDb.query('my-query', { toGraph: graph.enrichment }) ) This snippet assumes that the graph names have been declared (see Delcarations ).","title":"Result graph"},{"location":"triply-etl/extract/types/#triplydb-instance_1","text":"The triplyDb option can be used to specify that a query from a different TriplyDB instance should be used. This option works in the same way as for TriplyDB assets: link","title":"TriplyDB instance"},{"location":"triply-etl/extract/types/#strings","text":"Data in the JSON or RDF formats can be specified with inline strings. The following code snippet loads triples into the Internal Store: loadRdf( Source.string(` prefix person: <https://example.com/id/person/> prefix sdo: <https://schema.org/> person:1 a sdo:Person; sdo:name 'J. Doe'.`), { contentType: 'text/turtle' } ), This loads the following triples: graph LR person:1 -- a --> sdo:Person person:1 -- sdo:name --> J.Doe Notice that we must specify the RDF serialization format that we use. This is necessary because loadRdf() supports a large number of formats, some of which are difficult to autodetect. The following formats are supported: Format contentType value HTML 'text/html' JSON-LD 'application/ld+json' JSON 'application/json' N-Quads 'application/n-quads' N-Triples 'application/n-triples' N3 'text/n3' RDF/XML 'application/rdf+xml' SVG 'image/svg+xml' TriG 'application/trig' Turtle 'text/turtle' XHTML 'application/xhtml+xml' XML 'application/xml' The following example makes RDF source data available to the SHACL validate() function: import { Source } from '@triplyetl/etl/generic' import { validate } from '@triplyetl/etl/shacl' validate(Source.string(` prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://example.com/model/shp/> prefix sdo: <https://schema.org/> shp:Person a sh:NodeShape; sh:property shp:Person_name; sh:targetClass sdo:Person. shp:Person_name a sh:PropertyShape; sh:datatype xsd:string; sh:minLength 1; sh:path sdo:name.`)) This makes the following linked data SHACL specification available: graph LR shp:Person -- a --> sh:NodeShape shp:Person -- sh:property --> shp:Person_name shp:Person -- sh:targetClass --> sdo:Person shp:Person_name -- a --> sh:PropertyShape shp:Person_name -- sh:datatype --> xsd:string shp:Person_name -- sh:minLength --> 1 shp:Person_name -- sh:path --> sdo:name Notice that validate() does not require us to set the content-type, since it only supports N-Quads, N-Triples, TriG and Turtle (and these formats can be detected automatically). The following example makes a string source available to the fromJson() source extractor: fromJson(Source.string(` [ { id: '123', name: 'John' }, { id: '456', name: 'Jane' } ]`)), Notice that the inline JSON source is often a more intuitive specification format for the fromJson() source extractor than its corresponding string source. While inline JSON and string sources are mostly used for small examples, local files are somewhat more widely used.","title":"Strings"},{"location":"triply-etl/extract/types/#inline-json","text":"Because TriplyETL configurations are implemented in TypeScript, it is possible to specify JSON data inline with TypeScript Objects. JSON is the only data format that be specified in such a native inline way in TriplyETL. The following code snippet specifies two records using inline TypeScript objects: fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), This results in the following two records: { \"id\": \"123\", \"name\": \"John\" } { \"id\": \"456\", \"name\": \"Jane\" } In documentation, we often use such inline JSON sources since that makes code snippets self-contained, without having to rely on external sources such as files. In production systems this native inline source type is almost never used.","title":"Inline JSON"},{"location":"triply-etl/getting-started/","text":"This page helps you to get started with TriplyETL. You can get started with TriplyETL in any of the following ways: TriplyETL Generator creates a new ETL project based on your answers to a set of question. TriplyETL Runner runs an existing ETL project. TriplyETL Library can be included as a dependency in your TypeScript project. Prerequisites {#prerequisites} In order to use TriplyETL, you must first install the following programs on your computer: Install Git Go to this link and follow the instructions for your operating system (Windows, macOS, or Linux). Run the following commands to set your user name and email in Git: git config --global user.email \"ada@triply.cc\" git config --global user.name \"Ada Lovelace\" This information will be used in the Git version history for the TriplyETL project. This allows your team to keep track of who made which change. Install Node.js Go to nodejs.org and click on \u201c18.x.y LTS (Recommended For Most Users)\u201d. This will download the installer for your operating system. Run the installer on your computer. Alternatively, you can install the Node Version Manager ( nvm ). This allows you to install multiple versions of Node.js on the same computer. See the following links for more information: On Windows You can following the official instructions from Microsoft for installing NVM on Windows. On macOS or Linux You can follow the instructions for installing NVM on any operating system (including macOS and Linux). Find a terminal application You must use a terminal application in order to run commands from the [TriplyETL CLI](/docs/triply-etl/cli). Here are some examples of terminal applications on different operating systems: On Windows Most Windows versions come with some version of PowerShell preinstalled. You can also follow these instructions by Microsoft to update to the latest version of PowerShell. On macOS Most macOS version come with a Terminal application preinstalled. On Linux Most Linux versions come with a preinstalled terminal application. For example, on Ubuntu the GNOME Terminal application is preinstalled. # TriplyETL Generator {#generator} The TriplyETL Generator allows you to create new ETL projects in your terminal application. If a TriplyETL project already exists, use the [TriplyETL Runner](#runner) instead. In order to use TriplyETL Generator, you must have: 1. Satisfied the [prerequisites](#prerequisites). 2. A TriplyETL License Key. Contact [info@triply.cc](mailto:info@triply.cc) to obtain a License Key for your organization. 3. A user account on a TriplyDB server. Contact [info@triply.cc](mailto:info@triply.cc) to set up a TriplyDB server for your organization, or create a free account on . Perform the following steps to use the TriplyETL Generator: 1. Run the following command: ```sh npx triply-etl-generator ``` If you use TriplyETL Generator for the first time, this command automatically downloads and installs the latest version on your computer. If you have used TriplyETL Generator in the past, this command automatically updates your installation to the latest version. 2. When asked, enter the following information: a. TriplyETL License Key b. Project name c. Target folder d. Dataset name e. TriplyDB API Token f. TriplyDB URL g. TriplyDB email h. TriplyDB password Here is an example of a possible run: ```sh ? TriplyETL License Key: [hidden] ? Project name: my-etl ? Target folder: my-etl ? Dataset name: my-etl ? Create a new TriplyDB API Token? Yes ? Your TriplyDB URL: triplydb.com ? Your TriplyDB email: my-account@my-organization.com ? Your TriplyDB password: [hidden] \ud83c\udfc1 Your project my-etl is ready for use in my-etl. ``` 3. Go to the target folder that you have specified: ```sh cd my-etl ``` 4. You can now use the [TriplyETL Runner](/docs/triply-etl/cli#runner) to run the ETL: ```sh npx etl ``` # TriplyETL Runner {#runner} The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. In order to use TriplyETL Runner, you must have: 1. Satisfied the [prerequisites](#prerequisites). 2. A user account on a TriplyDB server. Contact [info@triply.cc](mailto:info@triply.cc) to set up a TriplyDB server for your organization, or create a free account on . Perform the following steps to use the TriplyETL Runner: 1. Create a local copy of an existing ETL project. If you do not have access to an existing TriplyETL project yet, use the [TriplyETL Generator](/docs/triply-etl/getting-started#generator) to create a new one. If you have access to an existing TriplyETL project, use the following command to make a local copy with Git: ```sh git clone ssh://git@git.triply.cc:10072/customers/my-org/my-project.git ``` 2. Once you have created a local copy of an existing ETL project, go into the corresponding directory: ```sh cd my-project ``` 3. Install the dependencies: ```sh npm i ``` 4. Transpile the TypeScript files into JavaScript: ```sh npm run build ``` 5. You can now use the TriplyETL Runner: ```sh npx etl ``` At this point, you should see a first TriplyETL process in your terminal application. If this is not the case, please contact [support@triply.cc](mailto:support@triply.cc) to help you out. Visit the [TriplyETL CLI documentation](/docs/triply-etl/cli#runner) to learn more about how you can use the TriplyETL Runner. # TriplyETL Library {#library} If you are a software developer that is building a software application in TypeScript, you can include the TriplyETL Library in your project. In order to use the TriplyETL Library, you must have: 1. Satisfied the [prerequisites](#prerequisites). 2. A TriplyETL License Key. Contact [info@triply.cc](mailto:info@triply.cc) to obtain a License Key for your organization. 3. A user account on a TriplyDB server. Contact [info@triply.cc](mailto:info@triply.cc) to set up a TriplyDB server for your organization, or create a free account on . Perform the following steps to use the TriplyETL Library: 1. Open the file `.npmrc` in your text editor, or create the file if it does not yet exist. Add the following content: ``` @triplydb:registry=https://git.triply.cc/api/v4/packages/npm/ @triplyetl:registry=https://git.triply.cc/api/v4/packages/npm/ //git.triply.cc/api/v4/packages/npm/:_authToken={LICENSE_KEY} ``` Replace `{LICENSE_KEY}` with your TriplyETL License Key. Contact [support@triply.cc](mailto:support@triply.cc) if you do not have such a license key yet. 2. Run the following command to add the TriplyETL dependency to your `package.json` file: ```sh npm install @triplyetl/etl ``` 3. Open one of the TypeScript files in your software project. When you add the following line to the top of your file, it should be recognized by your TypeScript editor: ```ts import { sdo } from '@triplyetl/etl/vocab' ```","title":"TriplyETL: Getting Started"},{"location":"triply-etl/getting-started/#prerequisites-prerequisites","text":"In order to use TriplyETL, you must first install the following programs on your computer: Install Git Go to this link and follow the instructions for your operating system (Windows, macOS, or Linux). Run the following commands to set your user name and email in Git: git config --global user.email \"ada@triply.cc\" git config --global user.name \"Ada Lovelace\" This information will be used in the Git version history for the TriplyETL project. This allows your team to keep track of who made which change. Install Node.js Go to nodejs.org and click on \u201c18.x.y LTS (Recommended For Most Users)\u201d. This will download the installer for your operating system. Run the installer on your computer. Alternatively, you can install the Node Version Manager ( nvm ). This allows you to install multiple versions of Node.js on the same computer. See the following links for more information: On Windows You can following the official instructions from Microsoft for installing NVM on Windows. On macOS or Linux You can follow the instructions for installing NVM on any operating system (including macOS and Linux). Find a terminal application You must use a terminal application in order to run commands from the [TriplyETL CLI](/docs/triply-etl/cli). Here are some examples of terminal applications on different operating systems: On Windows Most Windows versions come with some version of PowerShell preinstalled. You can also follow these instructions by Microsoft to update to the latest version of PowerShell. On macOS Most macOS version come with a Terminal application preinstalled. On Linux Most Linux versions come with a preinstalled terminal application. For example, on Ubuntu the GNOME Terminal application is preinstalled. # TriplyETL Generator {#generator} The TriplyETL Generator allows you to create new ETL projects in your terminal application. If a TriplyETL project already exists, use the [TriplyETL Runner](#runner) instead. In order to use TriplyETL Generator, you must have: 1. Satisfied the [prerequisites](#prerequisites). 2. A TriplyETL License Key. Contact [info@triply.cc](mailto:info@triply.cc) to obtain a License Key for your organization. 3. A user account on a TriplyDB server. Contact [info@triply.cc](mailto:info@triply.cc) to set up a TriplyDB server for your organization, or create a free account on . Perform the following steps to use the TriplyETL Generator: 1. Run the following command: ```sh npx triply-etl-generator ``` If you use TriplyETL Generator for the first time, this command automatically downloads and installs the latest version on your computer. If you have used TriplyETL Generator in the past, this command automatically updates your installation to the latest version. 2. When asked, enter the following information: a. TriplyETL License Key b. Project name c. Target folder d. Dataset name e. TriplyDB API Token f. TriplyDB URL g. TriplyDB email h. TriplyDB password Here is an example of a possible run: ```sh ? TriplyETL License Key: [hidden] ? Project name: my-etl ? Target folder: my-etl ? Dataset name: my-etl ? Create a new TriplyDB API Token? Yes ? Your TriplyDB URL: triplydb.com ? Your TriplyDB email: my-account@my-organization.com ? Your TriplyDB password: [hidden] \ud83c\udfc1 Your project my-etl is ready for use in my-etl. ``` 3. Go to the target folder that you have specified: ```sh cd my-etl ``` 4. You can now use the [TriplyETL Runner](/docs/triply-etl/cli#runner) to run the ETL: ```sh npx etl ``` # TriplyETL Runner {#runner} The TriplyETL Runner allows you to run a local TriplyETL project in your terminal application. In order to use TriplyETL Runner, you must have: 1. Satisfied the [prerequisites](#prerequisites). 2. A user account on a TriplyDB server. Contact [info@triply.cc](mailto:info@triply.cc) to set up a TriplyDB server for your organization, or create a free account on . Perform the following steps to use the TriplyETL Runner: 1. Create a local copy of an existing ETL project. If you do not have access to an existing TriplyETL project yet, use the [TriplyETL Generator](/docs/triply-etl/getting-started#generator) to create a new one. If you have access to an existing TriplyETL project, use the following command to make a local copy with Git: ```sh git clone ssh://git@git.triply.cc:10072/customers/my-org/my-project.git ``` 2. Once you have created a local copy of an existing ETL project, go into the corresponding directory: ```sh cd my-project ``` 3. Install the dependencies: ```sh npm i ``` 4. Transpile the TypeScript files into JavaScript: ```sh npm run build ``` 5. You can now use the TriplyETL Runner: ```sh npx etl ``` At this point, you should see a first TriplyETL process in your terminal application. If this is not the case, please contact [support@triply.cc](mailto:support@triply.cc) to help you out. Visit the [TriplyETL CLI documentation](/docs/triply-etl/cli#runner) to learn more about how you can use the TriplyETL Runner. # TriplyETL Library {#library} If you are a software developer that is building a software application in TypeScript, you can include the TriplyETL Library in your project. In order to use the TriplyETL Library, you must have: 1. Satisfied the [prerequisites](#prerequisites). 2. A TriplyETL License Key. Contact [info@triply.cc](mailto:info@triply.cc) to obtain a License Key for your organization. 3. A user account on a TriplyDB server. Contact [info@triply.cc](mailto:info@triply.cc) to set up a TriplyDB server for your organization, or create a free account on . Perform the following steps to use the TriplyETL Library: 1. Open the file `.npmrc` in your text editor, or create the file if it does not yet exist. Add the following content: ``` @triplydb:registry=https://git.triply.cc/api/v4/packages/npm/ @triplyetl:registry=https://git.triply.cc/api/v4/packages/npm/ //git.triply.cc/api/v4/packages/npm/:_authToken={LICENSE_KEY} ``` Replace `{LICENSE_KEY}` with your TriplyETL License Key. Contact [support@triply.cc](mailto:support@triply.cc) if you do not have such a license key yet. 2. Run the following command to add the TriplyETL dependency to your `package.json` file: ```sh npm install @triplyetl/etl ``` 3. Open one of the TypeScript files in your software project. When you add the following line to the top of your file, it should be recognized by your TypeScript editor: ```ts import { sdo } from '@triplyetl/etl/vocab' ```","title":"Prerequisites {#prerequisites}"},{"location":"triply-etl/maintenance/","text":"Once a TriplyETL repository is configured, it goes into maintenance mode. Maintenance includes the following tasks: Update the TriplyETL dependency Configure the TriplyETL CI/CD Monitor the TriplyETL CI/CD Update the TriplyETL dependency {#update} New versions of TriplyETL are released regularly. Moving to a new version is generally a good idea, because it allows new features to be used and will include fixes for known/reported bugs. At the same time, updating to a new version may require you to make some changes to your pipeline. It is important to determine an approach for updating your TriplyETL projects that fits your team and organization. The following sections describe how you can make such a determination. Check the current version The following command prints the TriplyETL version that you are currently using: npm list @triplyetl/etl Check for new versions The following command prints the latest TriplyETL version that is available: npm outdated TriplyETL repositories typically include several developer dependencies as well. These developer dependencies make it easier to write and maintain your ETLs. These developer dependencies are not part of TriplyETL, and must therefore be updated independently of TriplyETL. Assess the impact of updating TriplyETL uses the Semantic Versioning approach: {major}.{minor}.{patch} The impact of updating to a new TriplyETL version can therefore be determined as follows: Patch update Only the {patch} number has increased. This means that bugs have been fixed and small enhancements to existing functionalities have been added. In such cases, you should be able to update without having to make any changes to your configuration. Minor update The {minor} number has increased, but the {major} number is still the same. This means that new features have been added and/or existing features have received small enhancements. A minor update will never remove existing functionality, but it may change details of how existing functionalities work (e.g. the settings for an existing function may have undergone minor changes). In such cases, you should be able to update with either no changes or only minor changes to your configuration. The changes you need to make are described in the changelog . Major update The {major} number has increased. This means that features have been removed, or have changed significantly. In such cases, changes to your configuration are almost certainly necessary, and may take some time to implement. Any changes you need to make are described in the changelog . Perform the update Based on the outcome of the previous step, a maintainer of the repository can choose to update a specific dependency. The following command updates the TriplyETL dependency: npm up {package-name} The following command updates all dependencies: npm up","title":"TriplyETL: Maintenance"},{"location":"triply-etl/maintenance/#update-the-triplyetl-dependency-update","text":"New versions of TriplyETL are released regularly. Moving to a new version is generally a good idea, because it allows new features to be used and will include fixes for known/reported bugs. At the same time, updating to a new version may require you to make some changes to your pipeline. It is important to determine an approach for updating your TriplyETL projects that fits your team and organization. The following sections describe how you can make such a determination.","title":"Update the TriplyETL dependency {#update}"},{"location":"triply-etl/maintenance/#check-the-current-version","text":"The following command prints the TriplyETL version that you are currently using: npm list @triplyetl/etl","title":"Check the current version"},{"location":"triply-etl/maintenance/#check-for-new-versions","text":"The following command prints the latest TriplyETL version that is available: npm outdated TriplyETL repositories typically include several developer dependencies as well. These developer dependencies make it easier to write and maintain your ETLs. These developer dependencies are not part of TriplyETL, and must therefore be updated independently of TriplyETL.","title":"Check for new versions"},{"location":"triply-etl/maintenance/#assess-the-impact-of-updating","text":"TriplyETL uses the Semantic Versioning approach: {major}.{minor}.{patch} The impact of updating to a new TriplyETL version can therefore be determined as follows: Patch update Only the {patch} number has increased. This means that bugs have been fixed and small enhancements to existing functionalities have been added. In such cases, you should be able to update without having to make any changes to your configuration. Minor update The {minor} number has increased, but the {major} number is still the same. This means that new features have been added and/or existing features have received small enhancements. A minor update will never remove existing functionality, but it may change details of how existing functionalities work (e.g. the settings for an existing function may have undergone minor changes). In such cases, you should be able to update with either no changes or only minor changes to your configuration. The changes you need to make are described in the changelog . Major update The {major} number has increased. This means that features have been removed, or have changed significantly. In such cases, changes to your configuration are almost certainly necessary, and may take some time to implement. Any changes you need to make are described in the changelog .","title":"Assess the impact of updating"},{"location":"triply-etl/maintenance/#perform-the-update","text":"Based on the outcome of the previous step, a maintainer of the repository can choose to update a specific dependency. The following command updates the TriplyETL dependency: npm up {package-name} The following command updates all dependencies: npm up","title":"Perform the update"},{"location":"triply-etl/publish/","text":"The Publish step makes the linked data that is produced by the TriplyETL pipeline available in a Triple Store for use by others. graph LR source -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> tdb linkStyle 5 stroke:red,stroke-width:3px; ld[Internal Store] record[Record] source[Data Sources] tdb[(Triple Store)] Remote data destinations Destinations are usually online locations in TriplyDB where the output of your pipeline will be published. If no account name is given, pipeline output is uploaded under the user account tied to the currently used API Token. Destination.TriplyDb.rdf('my-dataset') Destination.TriplyDb.rdf('my-account', 'my-dataset') Destination.TriplyDb.rdf('my-account', 'my-dataset', { overwrite: true }) The following options can be specified to configure the destination behavior: overwrite Whether the graphs that are being uploaded by TriplyETL should replace any existing graphs with the same name in the dataset. Graphs appearing in the dataset with a different name than those uploaded by TriplyETL are kept. The default value is false . synchronizeServices Whether active services should be automatically synchronized once new data is uploaded. The default value is false . triplyDb A configuration object describing a TriplyDB instance that is different from the one associated with the current API Token. (See the section on configuring multiple TriplyDB instance for more information.) truncateGraphs Whether to delete all graphs in the dataset before uploading any graphs from TriplyETL. Notice that this will also remove graphs that will not be re-uploaded by TriplyETL. The default value is false . Local data destinations TriplyETL supports publishing RDF output into a local file. This is not often used, because files lack many of the features that TriplyDB destinations support, such as: The ability to browse the data. The ability to query the data. The ability to configure metadata. The ability to configure prefix declarations. Still, there may be cases in which a local file destination is useful, for example when you do not have an active Internet connection: Destination.file('my-file.trig'), Static and Dynamic destinations Destinations can be defined as static objects meaning that you can define destination beforehand. But it might be the case that you want to have multiple destinations for different records. In this case, you would need a dynamic destination, which should change based on certain information inside your source data. You can set static and dynamic destinations: const etl = new Etl({ sources: { someSource: Source.file('source.trig'), }, destinations: { someStaticDestination: Destination.file('static.ttl'), someDynamicDestination: context => Destination.file(context.getString('destination')), }, }) Configuring multiple TriplyDB instances It is possible to use multiple TriplyDB instances in one TriplyETL pipeline. The following example illustrates how the data model is used from the production instance of TriplyDB. const etl = new Etl({ sources: { data_model: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['PRODUCTION_INSTANCE_TOKEN'], url: 'https://api.production.example.com' } } ), instance_data: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['ACCEPTANCE_INSTANCE_TOKEN'], url: 'https://api.acceptance.example.com' } } ), }, }) Direct copying of source data to destination TriplyETL supports copying sources directly to destination locations. This function is useful when you already have linked data that is used as a source, but is also needed at the destination. An example would be the information model. This would be available as a source, and with the copy function it can be uploaded to TriplyDB via TriplyETL. The following example shows the copy function: etl.copySource( Source.file(`${source_location}`), Destination.TriplyDb.rdf(`${destination_name}`) ), The function destination expects that source data is linked data. Copying a source that is not linked data can result in errors. Using TriplyDB.js in TriplyETL All operations that can be performed in a TriplyDB instance can be automated with classes and methods in the TriplyDB.js library. This library is also used by TriplyETL in the background to implement many of the TriplyETL functionalities. Sometimes it is useful to use classes and methods in TriplyDB.js directly. This is done in the following way: // Create the ETL context. const etl = new Etl() // Use the context to access the TriplyDB.js connection. console.log((await etl.triplyDb.getInfo()).name) The above example prints the name of the TriplyDB instance. But any other TriplyDB.js operations can be performed. For example, the user of the current API Token can change their avatar image in TriplyDB: const user = await etl.triplyDb.getUser() await user.setAvatar('my-avatar.png') Setting up Acceptance/Production runs (DTAP) When working on a pipeline it is best to at least run it in the following two modes: Acceptance mode Upload the result of the pipeline to the user account for which the API Token was created. Production mode Upload the result of the pipeline to the organization where the production version of the data is published. Having multiple modes ensures that the production version of a dataset is not accidentally overwritten during development. export function account(): any { switch (Etl.environment) { case 'Development': return undefined case 'Testing': return 'my-org-testing' case 'Acceptance': return 'my-org-acceptance' case 'Production': return 'my-org' } } const etl = new Etl() etl.use( // Your ETL pipeline is configured here. toRdf(Destination.triplyDb.rdf(account(), 'my-dataset')), ) By default, you run the pipeline in Development mode. If you want to run in another mode, you must set the ENV environment variable. You can do this in the .env file of your TriplyETL repository. For example, the following runs the pipeline in Testing mode: ENV=Testing You can also set the ENV variable in the GitLab CI/CD environment. This allows you to automatically run different pipelines, according to the DTAP approach for production systems. Upload prefix declarations At the end of a TriplyETL script, it is common to upload the prefix declarations that are configured for that pipeline. This is often done directly before or after graphs are uploaded (function toRdf() ): import { declarePrefix, toRdf, uploadPrefixes } from '@triplyetl/etl/generic' const prefix = { // Your prefix declarations. } export default async function(): Promise<Etl> { const etl = new Etl({ prefixes: prefix }) etl.run( // You ETL pipeline toRdf({ account: 'my-account', dataset: 'my-dataset' }), uploadPrefixes({ account: 'my-account', dataset: 'my-dataset' }), ) return etl }","title":"6. TriplyETL: Publish"},{"location":"triply-etl/publish/#remote-data-destinations","text":"Destinations are usually online locations in TriplyDB where the output of your pipeline will be published. If no account name is given, pipeline output is uploaded under the user account tied to the currently used API Token. Destination.TriplyDb.rdf('my-dataset') Destination.TriplyDb.rdf('my-account', 'my-dataset') Destination.TriplyDb.rdf('my-account', 'my-dataset', { overwrite: true }) The following options can be specified to configure the destination behavior: overwrite Whether the graphs that are being uploaded by TriplyETL should replace any existing graphs with the same name in the dataset. Graphs appearing in the dataset with a different name than those uploaded by TriplyETL are kept. The default value is false . synchronizeServices Whether active services should be automatically synchronized once new data is uploaded. The default value is false . triplyDb A configuration object describing a TriplyDB instance that is different from the one associated with the current API Token. (See the section on configuring multiple TriplyDB instance for more information.) truncateGraphs Whether to delete all graphs in the dataset before uploading any graphs from TriplyETL. Notice that this will also remove graphs that will not be re-uploaded by TriplyETL. The default value is false .","title":"Remote data destinations"},{"location":"triply-etl/publish/#local-data-destinations","text":"TriplyETL supports publishing RDF output into a local file. This is not often used, because files lack many of the features that TriplyDB destinations support, such as: The ability to browse the data. The ability to query the data. The ability to configure metadata. The ability to configure prefix declarations. Still, there may be cases in which a local file destination is useful, for example when you do not have an active Internet connection: Destination.file('my-file.trig'),","title":"Local data destinations"},{"location":"triply-etl/publish/#static-and-dynamic-destinations","text":"Destinations can be defined as static objects meaning that you can define destination beforehand. But it might be the case that you want to have multiple destinations for different records. In this case, you would need a dynamic destination, which should change based on certain information inside your source data. You can set static and dynamic destinations: const etl = new Etl({ sources: { someSource: Source.file('source.trig'), }, destinations: { someStaticDestination: Destination.file('static.ttl'), someDynamicDestination: context => Destination.file(context.getString('destination')), }, })","title":"Static and Dynamic destinations"},{"location":"triply-etl/publish/#configuring-multiple-triplydb-instances","text":"It is possible to use multiple TriplyDB instances in one TriplyETL pipeline. The following example illustrates how the data model is used from the production instance of TriplyDB. const etl = new Etl({ sources: { data_model: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['PRODUCTION_INSTANCE_TOKEN'], url: 'https://api.production.example.com' } } ), instance_data: Source.TriplyDb.rdf( 'my-account', 'my-dataset', { triplyDb: { token: process.env['ACCEPTANCE_INSTANCE_TOKEN'], url: 'https://api.acceptance.example.com' } } ), }, })","title":"Configuring multiple TriplyDB instances"},{"location":"triply-etl/publish/#direct-copying-of-source-data-to-destination","text":"TriplyETL supports copying sources directly to destination locations. This function is useful when you already have linked data that is used as a source, but is also needed at the destination. An example would be the information model. This would be available as a source, and with the copy function it can be uploaded to TriplyDB via TriplyETL. The following example shows the copy function: etl.copySource( Source.file(`${source_location}`), Destination.TriplyDb.rdf(`${destination_name}`) ), The function destination expects that source data is linked data. Copying a source that is not linked data can result in errors.","title":"Direct copying of source data to destination"},{"location":"triply-etl/publish/#using-triplydbjs-in-triplyetl","text":"All operations that can be performed in a TriplyDB instance can be automated with classes and methods in the TriplyDB.js library. This library is also used by TriplyETL in the background to implement many of the TriplyETL functionalities. Sometimes it is useful to use classes and methods in TriplyDB.js directly. This is done in the following way: // Create the ETL context. const etl = new Etl() // Use the context to access the TriplyDB.js connection. console.log((await etl.triplyDb.getInfo()).name) The above example prints the name of the TriplyDB instance. But any other TriplyDB.js operations can be performed. For example, the user of the current API Token can change their avatar image in TriplyDB: const user = await etl.triplyDb.getUser() await user.setAvatar('my-avatar.png')","title":"Using TriplyDB.js in TriplyETL"},{"location":"triply-etl/publish/#setting-up-acceptanceproduction-runs-dtap","text":"When working on a pipeline it is best to at least run it in the following two modes: Acceptance mode Upload the result of the pipeline to the user account for which the API Token was created. Production mode Upload the result of the pipeline to the organization where the production version of the data is published. Having multiple modes ensures that the production version of a dataset is not accidentally overwritten during development. export function account(): any { switch (Etl.environment) { case 'Development': return undefined case 'Testing': return 'my-org-testing' case 'Acceptance': return 'my-org-acceptance' case 'Production': return 'my-org' } } const etl = new Etl() etl.use( // Your ETL pipeline is configured here. toRdf(Destination.triplyDb.rdf(account(), 'my-dataset')), ) By default, you run the pipeline in Development mode. If you want to run in another mode, you must set the ENV environment variable. You can do this in the .env file of your TriplyETL repository. For example, the following runs the pipeline in Testing mode: ENV=Testing You can also set the ENV variable in the GitLab CI/CD environment. This allows you to automatically run different pipelines, according to the DTAP approach for production systems.","title":"Setting up Acceptance/Production runs (DTAP)"},{"location":"triply-etl/publish/#upload-prefix-declarations","text":"At the end of a TriplyETL script, it is common to upload the prefix declarations that are configured for that pipeline. This is often done directly before or after graphs are uploaded (function toRdf() ): import { declarePrefix, toRdf, uploadPrefixes } from '@triplyetl/etl/generic' const prefix = { // Your prefix declarations. } export default async function(): Promise<Etl> { const etl = new Etl({ prefixes: prefix }) etl.run( // You ETL pipeline toRdf({ account: 'my-account', dataset: 'my-dataset' }), uploadPrefixes({ account: 'my-account', dataset: 'my-dataset' }), ) return etl }","title":"Upload prefix declarations"},{"location":"triply-etl/tmp/automation/","text":"TriplyETL runs within a Gitlab CI environment ( Figure 1 ). Figure 1: The landing page of a TriplyETL project in Gitlab. Special key: $environment The special key $environment denotes the DTAP environment in which the TriplyETL pipeline is running. This allows special actions to be performed based on whether the pipeline runs in \"Debug\" , \"Test\" , \"Acceptance\" , or \"Production\" mode. See the DTAP documentation for more information.","title":"TriplyETL: Automation"},{"location":"triply-etl/tmp/automation/#special-key-environment","text":"The special key $environment denotes the DTAP environment in which the TriplyETL pipeline is running. This allows special actions to be performed based on whether the pipeline runs in \"Debug\" , \"Test\" , \"Acceptance\" , or \"Production\" mode. See the DTAP documentation for more information.","title":"Special key: $environment"},{"location":"triply-etl/tmp/ci-cd/","text":"This document explains how to maintain an ETL that runs in the gitlab CI. How to create a TriplyETL CI pipeline? Use the TriplyETL boilerplate from this repository , specifically the etl folder. Each customer organization in gitlab needs their own 'CI runner'. If you're adding a TriplyETL repo to an existing customer, then you're probably fine. But if this is a newly created organization, you will need to register a runner for this organization. To check whether the customer organization has a runner configured, go to the customer organization Settings in gitlab, then go to CI/CD . The URL should look something like this: https://git.triply.cc/groups/customers/<customer-name>/-/settings/ci_cd . Click Runners and verify that Available runners: is not zero. If you need to register a runner, contact a sysadmin (see here for the list of sysadmins) and ask them to create a runner for your organization. Modifying a pipeline To change what happens in a CI pipeline, all you need to do is modify the .gitlab-ci.yml file of your repository. Below we detail some relevant .gitlab-ci.yml fields that are used in most of our ETL pipelines (see here for the complete gitlab documentation on what all these fields mean) artifacts: Artifacts are files or directories that gitlab will save for you. These files are available on the gitlab pipelines page after a job ran. This is particularly useful for TriplyETL error/log files, as you download these from the gitlab UI. variables: You can define environment variables in several places. In the .gitlab.yml file, you can configure them at a job level or for all jobs. You can also configure them in the gitlab UI in the pipeline schedule form. Variables defined in a pipeline schedule will overwrite variables defined in the .gitlab-ci.yml file (see here for the gitlab documentation on variable precedence). script: This is the code that will run in the job. If you need a job to run two TriplyETL commands after each other, you can easily add another npx etl .... line here. only: This defines when a job should run. If a job does not have an only: configure, it will always run. See here for documentation about the syntax of only ). The boilerplate comes with some example only: rules that look like this: only: variables: - $JOB == \"$CI_JOB_NAME\" This means that we only run this job if there a JOB environment variable that is the same as CI_JOB_NAME . Notice that the CI_JOB_NAME is a default environment variable that gitlab gives us and that equals the name of the job, e.g. production (see other predefined variables here ). If you want to run this specific job, JOB is the environment variable that you should set in the pipeline schedule page. In other words, if you set JOB=production in the pipeline schedule page, then using the above only: rule, only the intended job will run.","title":"Ci cd"},{"location":"triply-etl/tmp/ci-cd/#how-to-create-a-triplyetl-ci-pipeline","text":"Use the TriplyETL boilerplate from this repository , specifically the etl folder. Each customer organization in gitlab needs their own 'CI runner'. If you're adding a TriplyETL repo to an existing customer, then you're probably fine. But if this is a newly created organization, you will need to register a runner for this organization. To check whether the customer organization has a runner configured, go to the customer organization Settings in gitlab, then go to CI/CD . The URL should look something like this: https://git.triply.cc/groups/customers/<customer-name>/-/settings/ci_cd . Click Runners and verify that Available runners: is not zero. If you need to register a runner, contact a sysadmin (see here for the list of sysadmins) and ask them to create a runner for your organization.","title":"How to create a TriplyETL CI pipeline?"},{"location":"triply-etl/tmp/ci-cd/#modifying-a-pipeline","text":"To change what happens in a CI pipeline, all you need to do is modify the .gitlab-ci.yml file of your repository. Below we detail some relevant .gitlab-ci.yml fields that are used in most of our ETL pipelines (see here for the complete gitlab documentation on what all these fields mean)","title":"Modifying a pipeline"},{"location":"triply-etl/tmp/ci-cd/#artifacts","text":"Artifacts are files or directories that gitlab will save for you. These files are available on the gitlab pipelines page after a job ran. This is particularly useful for TriplyETL error/log files, as you download these from the gitlab UI.","title":"artifacts:"},{"location":"triply-etl/tmp/ci-cd/#variables","text":"You can define environment variables in several places. In the .gitlab.yml file, you can configure them at a job level or for all jobs. You can also configure them in the gitlab UI in the pipeline schedule form. Variables defined in a pipeline schedule will overwrite variables defined in the .gitlab-ci.yml file (see here for the gitlab documentation on variable precedence).","title":"variables:"},{"location":"triply-etl/tmp/ci-cd/#script","text":"This is the code that will run in the job. If you need a job to run two TriplyETL commands after each other, you can easily add another npx etl .... line here.","title":"script:"},{"location":"triply-etl/tmp/ci-cd/#only","text":"This defines when a job should run. If a job does not have an only: configure, it will always run. See here for documentation about the syntax of only ). The boilerplate comes with some example only: rules that look like this: only: variables: - $JOB == \"$CI_JOB_NAME\" This means that we only run this job if there a JOB environment variable that is the same as CI_JOB_NAME . Notice that the CI_JOB_NAME is a default environment variable that gitlab gives us and that equals the name of the job, e.g. production (see other predefined variables here ). If you want to run this specific job, JOB is the environment variable that you should set in the pipeline schedule page. In other words, if you set JOB=production in the pipeline schedule page, then using the above only: rule, only the intended job will run.","title":"only:"},{"location":"triply-etl/tmp/context/","text":"Configuring the Context {#context} The TriplyETL Context is specified when the Etl object is instantiated. This often appears towards the start of a pipeline script. The TriplyETL Context allows the following things to be specified: The data sources that can be used in the ETL. The data destinations where linked data is published to. The named graph in which triple calls with no graph argument add their data. The prefix IRI for blank node-replacing well-known IRIs. Configuring the standard graph When we call triple with 3 arguments, a triple is created and placed in a named graph that is chosen by TriplyETL. You can change the name of this default graph by specifying it in the TriplyETL context. Notice that graph names must be IRIs: const etl = new Etl() Configuring the well-known IRI prefix TriplyDB performs Skolemization, an approach in which blank nodes are systematically replaced by well-known IRIs. TriplyDB chooses a well-known IRI prefix for you, const etl = new Etl({ wellKnownIriPrefix: 'https://triplydb.com/Triply/example/.well-known/genid/', })","title":"Context"},{"location":"triply-etl/tmp/context/#configuring-the-context-context","text":"The TriplyETL Context is specified when the Etl object is instantiated. This often appears towards the start of a pipeline script. The TriplyETL Context allows the following things to be specified: The data sources that can be used in the ETL. The data destinations where linked data is published to. The named graph in which triple calls with no graph argument add their data. The prefix IRI for blank node-replacing well-known IRIs.","title":"Configuring the Context {#context}"},{"location":"triply-etl/tmp/context/#configuring-the-standard-graph","text":"When we call triple with 3 arguments, a triple is created and placed in a named graph that is chosen by TriplyETL. You can change the name of this default graph by specifying it in the TriplyETL context. Notice that graph names must be IRIs: const etl = new Etl()","title":"Configuring the standard graph"},{"location":"triply-etl/tmp/context/#configuring-the-well-known-iri-prefix","text":"TriplyDB performs Skolemization, an approach in which blank nodes are systematically replaced by well-known IRIs. TriplyDB chooses a well-known IRI prefix for you, const etl = new Etl({ wellKnownIriPrefix: 'https://triplydb.com/Triply/example/.well-known/genid/', })","title":"Configuring the well-known IRI prefix"},{"location":"triply-etl/tmp/copy/","text":"Copy an existing entry over to a new entry {#copy} Copying is the act of creating a new thing that is based on a specific existing thing. Function signature The copy function has the following signature: app.use( copy({ fromKey: 'FROM_KEY', toKey: 'TO_KEY', type: 'VALUE_TYPE', change: value => FUNCTION_BODY}), ) This function copies the value from \u2018foo\u2019 to \u2018bar\u2019. The type key ensures that the value in \u2018foo\u2019 is cast to the specified type prior to being copied. The optional change key allows the cast value to be transformed prior to storing it in \u2018bar\u2019. Leaving the change key out results in a direct copy in which the value is not modified. This function emits an error if fromKey and toKey are the same. If you want to change a value in-place you should use change instead. This function emits an error if toKey already exists. If you want to replace the value in an existing entry then you should use replace instead. The change function only takes the value argument and does not take the context argument. If you need the context argument then they must use add instead.","title":"Copy an existing entry over to a new entry {#copy}"},{"location":"triply-etl/tmp/copy/#copy-an-existing-entry-over-to-a-new-entry-copy","text":"Copying is the act of creating a new thing that is based on a specific existing thing.","title":"Copy an existing entry over to a new entry {#copy}"},{"location":"triply-etl/tmp/copy/#function-signature","text":"The copy function has the following signature: app.use( copy({ fromKey: 'FROM_KEY', toKey: 'TO_KEY', type: 'VALUE_TYPE', change: value => FUNCTION_BODY}), ) This function copies the value from \u2018foo\u2019 to \u2018bar\u2019. The type key ensures that the value in \u2018foo\u2019 is cast to the specified type prior to being copied. The optional change key allows the cast value to be transformed prior to storing it in \u2018bar\u2019. Leaving the change key out results in a direct copy in which the value is not modified. This function emits an error if fromKey and toKey are the same. If you want to change a value in-place you should use change instead. This function emits an error if toKey already exists. If you want to replace the value in an existing entry then you should use replace instead. The change function only takes the value argument and does not take the context argument. If you need the context argument then they must use add instead.","title":"Function signature"},{"location":"triply-etl/tmp/faq/","text":"FAQ Why does my pipeline schedule only run an install job? This probably means that none of the only: rules in your .gitlab-ci.yml file match. You should check whether the variables you've set in the pipelines schedules page match with the only: rules in your .gitlab-ci.yml file. I made a change to the .gitlab-ci.yml file and after I push I see a pipeline failed with status yaml invalid . How can I fix this? Copy-past your .gitlab-ci.yml file here and press validate . This should show in more details what is wrong with the yml file. Why is my pipeline not running and marked as 'pending'? This probably means that you have not configured an ETL runner for this customer organization yet. See the section about getting started here What do all these $CI_... environment variables mean? These are environment variables added by gitlab. To see what they mean, go to this gitlab documentation page, also mentioned above. What should I do when the pipeline fail when I commit in a personal project? In a personal repository, you have available runners, but shared ones. Thus, your pipelines will fail. This is expected and it is not an issue. You can either ignore the failed pipeline or remove gitlab-ci.yml from the repository.","title":"FAQ"},{"location":"triply-etl/tmp/faq/#faq","text":"","title":"FAQ"},{"location":"triply-etl/tmp/faq/#why-does-my-pipeline-schedule-only-run-an-install-job","text":"This probably means that none of the only: rules in your .gitlab-ci.yml file match. You should check whether the variables you've set in the pipelines schedules page match with the only: rules in your .gitlab-ci.yml file.","title":"Why does my pipeline schedule only run an install job?"},{"location":"triply-etl/tmp/faq/#i-made-a-change-to-the-gitlab-ciyml-file-and-after-i-push-i-see-a-pipeline-failed-with-status-yaml-invalid-how-can-i-fix-this","text":"Copy-past your .gitlab-ci.yml file here and press validate . This should show in more details what is wrong with the yml file.","title":"I made a change to the .gitlab-ci.yml file and after I push I see a pipeline failed with status yaml invalid. How can I fix this?"},{"location":"triply-etl/tmp/faq/#why-is-my-pipeline-not-running-and-marked-as-pending","text":"This probably means that you have not configured an ETL runner for this customer organization yet. See the section about getting started here","title":"Why is my pipeline not running and marked as 'pending'?"},{"location":"triply-etl/tmp/faq/#what-do-all-these-ci_-environment-variables-mean","text":"These are environment variables added by gitlab. To see what they mean, go to this gitlab documentation page, also mentioned above.","title":"What do all these $CI_... environment variables mean?"},{"location":"triply-etl/tmp/faq/#what-should-i-do-when-the-pipeline-fail-when-i-commit-in-a-personal-project","text":"In a personal repository, you have available runners, but shared ones. Thus, your pipelines will fail. This is expected and it is not an issue. You can either ignore the failed pipeline or remove gitlab-ci.yml from the repository.","title":"What should I do when the pipeline fail when I commit in a personal project?"},{"location":"triply-etl/tmp/getting-started/","text":"Transforming RDF data If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const etl = new Etl({ defaultGraph: graph.model }) etl.use( loadRdf(Source.file(`data/shapes.trig`)), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, iri(prefix.graph, 'new-graph') ) ), toRdf(Destination.TriplyDb.rdf('my-dataset', remoteOptions)) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} ) Connect a data source This section extends the pipeline from the previous section by connecting a data source. TriplyETL can connect to database systems and web APIs, but to keep things simple we will use the following tabular input data from a local file: ID NAME 00001 Anna 00002 Bob 00003 Carol We then perform the following steps to build a pipelines that processes this data source: Create a text file called example.csv in a text editor, and copy/paste the following source data into that file: csv ID,NAME 00001,Anna 00002,Bob 00003,Carol Open text file main.ts and add the following content: ```ts import { Etl, declarePrefix, fromCsv, iri, literal, rdfs, Source, toRdf, triple } from '@triplyetl/etl/generic' import { rdfs } from '@triplyetl/etl/vocab' export default async function (): Promise { const etl = new Etl({ prefixes: { ex: declarePrefix('https://example.com/'), }, }) etl.use( // Connects the tabular source data to the pipeline. // Every row in the table is processed as a TriplyETL record. fromCsv(Source.file('example.csv')), // Create a linked data statement that is based on the // source data. triple(iri(etl.prefix.ex, 'ID'), rdfs.label, 'NAME'), toRdf(Destination.file('example.ttl')) ) return etl } ``` Transpile the code with: sh npm run build Run the ETL with: sh npx etl The TriplyETL script will give you a link to the uploaded dataset. This dataset contains the following graph content: Important terms before starting to work with TriplyETL Middlewares The most common occurrence in ETL are the middlewares. Middlewares are essentially reusable pieces of code that execute a certain long and/or complex piece of functionality. An middleware is a piece of code that transforms a record and can be invoked with app.use(). Example of middleware function: loadRdf(Source.TriplyDb.query('my-account', 'my-query')), What is a record? TriplyETL doesn't have infinite memory and not all data can be loaded at once. So instead of loading data all at once, first one part of data is processed and written to the file, and then the second one, third one, and so on. These parts are called records. Each record goes through all middlewares before a new record is started. What is the store? As mentioned above, when ETL is running we go through data record by record. Together with the input data we also have output data. Before being written to the final destination (triplyDB or file), output data has to be kept somewhere and that's what store is for. The store is for temporarily storing linked data. Every record has its own store. toRdf reads from the store. etl.use( toRdf(Ratt.Destination.file('example.ttl')) ) What is the context(ctx)? In TriplyETL, the context is an object that represents the current record. The context gives us access to the triple store, the in memory storage of our triples. It also contains utility functions that will be used to modify and transform our source data into linked data. Some examples of ctx: ctx.getString(\"address\") ctx.getIri(...) ctx.getArray(...) ctx.store.addQuad(...) ctx.store.getQuad(...) //etc. A JSON data source The following code snippet uses extractor fromJson() with two inline example records: import { fromJson, logRecord, Etl } from '@triplydb/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key ). An XML data source Now suppose that we change the source system. We no longer use in-line JSON, but will instead use an XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use the XML source connector: import { Etl, fromXml, logRecord, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source connector needs to be changed, and all transformations and assertions remain as they were.","title":"Getting started"},{"location":"triply-etl/tmp/getting-started/#transforming-rdf-data","text":"If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const etl = new Etl({ defaultGraph: graph.model }) etl.use( loadRdf(Source.file(`data/shapes.trig`)), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, iri(prefix.graph, 'new-graph') ) ), toRdf(Destination.TriplyDb.rdf('my-dataset', remoteOptions)) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} )","title":"Transforming RDF data"},{"location":"triply-etl/tmp/getting-started/#connect-a-data-source","text":"This section extends the pipeline from the previous section by connecting a data source. TriplyETL can connect to database systems and web APIs, but to keep things simple we will use the following tabular input data from a local file: ID NAME 00001 Anna 00002 Bob 00003 Carol We then perform the following steps to build a pipelines that processes this data source: Create a text file called example.csv in a text editor, and copy/paste the following source data into that file: csv ID,NAME 00001,Anna 00002,Bob 00003,Carol Open text file main.ts and add the following content: ```ts import { Etl, declarePrefix, fromCsv, iri, literal, rdfs, Source, toRdf, triple } from '@triplyetl/etl/generic' import { rdfs } from '@triplyetl/etl/vocab' export default async function (): Promise { const etl = new Etl({ prefixes: { ex: declarePrefix('https://example.com/'), }, }) etl.use( // Connects the tabular source data to the pipeline. // Every row in the table is processed as a TriplyETL record. fromCsv(Source.file('example.csv')), // Create a linked data statement that is based on the // source data. triple(iri(etl.prefix.ex, 'ID'), rdfs.label, 'NAME'), toRdf(Destination.file('example.ttl')) ) return etl } ``` Transpile the code with: sh npm run build Run the ETL with: sh npx etl The TriplyETL script will give you a link to the uploaded dataset. This dataset contains the following graph content:","title":"Connect a data source"},{"location":"triply-etl/tmp/getting-started/#important-terms-before-starting-to-work-with-triplyetl","text":"","title":"Important terms before starting to work with TriplyETL"},{"location":"triply-etl/tmp/getting-started/#middlewares","text":"The most common occurrence in ETL are the middlewares. Middlewares are essentially reusable pieces of code that execute a certain long and/or complex piece of functionality. An middleware is a piece of code that transforms a record and can be invoked with app.use(). Example of middleware function: loadRdf(Source.TriplyDb.query('my-account', 'my-query')),","title":"Middlewares"},{"location":"triply-etl/tmp/getting-started/#what-is-a-record","text":"TriplyETL doesn't have infinite memory and not all data can be loaded at once. So instead of loading data all at once, first one part of data is processed and written to the file, and then the second one, third one, and so on. These parts are called records. Each record goes through all middlewares before a new record is started.","title":"What is a record?"},{"location":"triply-etl/tmp/getting-started/#what-is-the-store","text":"As mentioned above, when ETL is running we go through data record by record. Together with the input data we also have output data. Before being written to the final destination (triplyDB or file), output data has to be kept somewhere and that's what store is for. The store is for temporarily storing linked data. Every record has its own store. toRdf reads from the store. etl.use( toRdf(Ratt.Destination.file('example.ttl')) )","title":"What is the store?"},{"location":"triply-etl/tmp/getting-started/#what-is-the-contextctx","text":"In TriplyETL, the context is an object that represents the current record. The context gives us access to the triple store, the in memory storage of our triples. It also contains utility functions that will be used to modify and transform our source data into linked data. Some examples of ctx: ctx.getString(\"address\") ctx.getIri(...) ctx.getArray(...) ctx.store.addQuad(...) ctx.store.getQuad(...) //etc.","title":"What is the context(ctx)?"},{"location":"triply-etl/tmp/getting-started/#a-json-data-source","text":"The following code snippet uses extractor fromJson() with two inline example records: import { fromJson, logRecord, Etl } from '@triplydb/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { id: '123', name: 'John' }, { id: '456', name: 'Jane' }, ]), logRecord(), ) return etl } Debug function logRecord() prints the current record to standard output. When this pipeline is run, the two records are printed as follows: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that TriplyETL adds two keys to both records: $recordId and $environment (see Special Key ).","title":"A JSON data source"},{"location":"triply-etl/tmp/getting-started/#an-xml-data-source","text":"Now suppose that we change the source system. We no longer use in-line JSON, but will instead use an XML file. The contents of the XML file are as follows: <?xml version=\"1.0\"?> <persons> <person> <id>123</id> <name>John</name> </person> <person> <id>456</id> <name>Jane</name> </person> </persons> Let us change the TriplyETL script to use the XML source connector: import { Etl, fromXml, logRecord, Source } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromXml(Source.file('example.xml')), logRecord(), ) return etl } This new script logs the following two records: { \"id\": \"123\", \"name\": \"John\", \"$recordId\": 1, \"$environment\": \"Development\" } { \"id\": \"456\", \"name\": \"Jane\", \"$recordId\": 2, \"$environment\": \"Development\" } Notice that the two records that are logged from an XML source are completely identical to the two records that were previously logged from a JSON source. This is an essential property of TriplyETL: it treats data from any source system in the same way, using the same intermediary record format. This makes it easy to write pipelines that process data from a large number of different data sources. This also makes replacing a data source in one format with a data source in another format a relatively cheap operation. More often than not, only the source connector needs to be changed, and all transformations and assertions remain as they were.","title":"An XML data source"},{"location":"triply-etl/tmp/main-loop/","text":"The main loop The following code snippet shows the main TriplyETL loop. Every TriplyETL pipeline consists of such a loop. import { Etl } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // Etc ) return etl } By adding the following five components, you configure the pipeline to create a linked data knowledge graph for your organization: Declarations declare IRI prefixes, graph names, and vocabularies that are used in the pipeline configuration. Source Connectors connect to the systems that add source data to your knowledge graph. Transformations clean, modify, and enrich the source data. Assertions generate the linked data that goes into the knowledge graph. Validation ensures that the linked data that is added to the knowledge graph follows your data model. Publication makes the linked data knowledge graph available in a triple store. These six components occur in specific places inside the TripleETL main loop, as indicated by the comments in the following code snippet: import { Etl } from '@triplyetl/etl/generic' // 1. Declarations are made before the main loop. export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // 2. Source Connectors appear at the top. // 3. Transformations appear in the middle. // 4. Assertions appear in the middle. // 5. Validation occurs directly before publication. // 6. Publication appears at the bottom. ) return etl }","title":"The main loop"},{"location":"triply-etl/tmp/main-loop/#the-main-loop","text":"The following code snippet shows the main TriplyETL loop. Every TriplyETL pipeline consists of such a loop. import { Etl } from '@triplyetl/etl/generic' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // Etc ) return etl } By adding the following five components, you configure the pipeline to create a linked data knowledge graph for your organization: Declarations declare IRI prefixes, graph names, and vocabularies that are used in the pipeline configuration. Source Connectors connect to the systems that add source data to your knowledge graph. Transformations clean, modify, and enrich the source data. Assertions generate the linked data that goes into the knowledge graph. Validation ensures that the linked data that is added to the knowledge graph follows your data model. Publication makes the linked data knowledge graph available in a triple store. These six components occur in specific places inside the TripleETL main loop, as indicated by the comments in the following code snippet: import { Etl } from '@triplyetl/etl/generic' // 1. Declarations are made before the main loop. export default async function (): Promise<Etl> { const etl = new Etl() etl.use( // 2. Source Connectors appear at the top. // 3. Transformations appear in the middle. // 4. Assertions appear in the middle. // 5. Validation occurs directly before publication. // 6. Publication appears at the bottom. ) return etl }","title":"The main loop"},{"location":"triply-etl/tmp/source-destination/","text":"An easier way to configure graph names and prefixes Instead of setting the graph name and the prefixes for every ETL, you can use functions for their generation: export function create_prefixes( organization: string = default_organization, dataset: string, host: string = default_host ) { const prefix_base = Ratt.prefixer(`https://${host}/${organization}/${dataset}/`) const prefix_bnode = Ratt.prefixer(prefix_base(`.well-known/genid/`)) const prefix_graph = Ratt.prefixer(prefix_base(`graph/`)) return { bnode: prefix_bnode, graph: prefix_graph, } } For example, if host==='triplydb.com' , organization==='exampleOrganization' and dataset='pokemon' , then the prefix for the blank nodes will be https://triplydb.com/exampleOrganization/pokemon/.well-known/genid/ . Then, similarly, you can use another function for the graph names: export function create_graphs( dataset: string, organization: string = default_organization, host: string = default_host ) { const prefix = create_prefixes(dataset, organization, host) return { default: prefix.graph('default'), metadata: prefix.graph('metadata'), instances: prefix.graph('instances'), instances_report: prefix.graph('instances/report'), shapes: prefix.graph('shapes'), } } Configuring data destinations Remote data destinations Destinations are usually online locations in TriplyDB where the output of your pipeline will be published. If no accountName is given, pipeline output is uploaded under the user account tied to the currently used API Token. Ratt.Destination.TriplyDb.rdf(datasetName) Ratt.Destination.TriplyDb.rdf(accountName, datasetName) Ratt.Destination.TriplyDb.rdf(accountName, datasetName, {overwrite: true}) The following options can be specified to configure the destination behavior: overwrite Whether the graphs that are being uploaded by RATT should replace any existing graphs with the same name in the dataset. Graphs appearing in the dataset with a different name than those uploaded by RATT are kept. The default value is false . synchronizeServices Whether active services should be automatically synchronized once new data is uploaded. The default value is false . triplyDb A configuration object describing a TriplyDB instance that is different from the one associated with the current API Token. (See the section on configuring multiple TriplyDB instance for more information.) truncateGraphs Whether to delete all graphs in the dataset before uploading any graphs from RATT. Notice that this will also remove graphs that will not be re-uploaded by RATT. The default value is false . Local data destinations RATT supports publishing RDF output into a local file. This is not often used, because files lack many of the features that TriplyDB destinations support, such as: The ability to browse the data. The ability to query the data. The ability to configure metadata. The ability to configure prefix declarations. Still, there may be cases in which a local file destination is useful, for example when you do not have an active Internet connection: Ratt.Destination.file(\"my-file.trig\"), Static and Dynamic destinations Destinations can be defined as static objects meaning that you can define destination beforehand. But it might be the case that you want to have multiple destinations for different records. In this case, you would need a dynamic destination, which should change based on certain information inside your source data. You can set static and dynamic destinations, like below: const app = new Ratt({ sources: { someSource: Ratt.Source.file(\"source.trig\") }, destinations: { someStaticDestination: Ratt.Destination.file(\"static.ttl\"), someDynamicDestination: (ctx) => Ratt.Destination.file(ctx.getString(\"destination\")) }, }) Configuring multiple TriplyDB instances It is possible to use multiple TriplyDB instances in one RATT pipeline. The following example illustrates how the data model is used from the production instance of TriplyDB. const account = 'Triply' const dataset = 'example' const app = new Ratt({ sources: { data_model: Ratt.Source.TriplyDb.rdf( account, dataset, {triplyDb: {token: process.env['PRODUCTION_INSTANCE_TOKEN'], url: 'https://api.production.example.com'}}), instance_data: Ratt.Source.TriplyDb.rdf( account, dataset, {triplyDb: {token: process.env['ACCEPTANCE_INSTANCE_TOKEN'], url: 'https://api.acceptance.example.com'}}), }, }) Direct copying of source data to destination RATT supports copying sources directly to destination locations. This function is useful when you already have linked data that is used as a source, but is also needed at the destination. An example would be the information model. This would be available as a source, and with the copy function it can be uploaded to TriplyDB via RATT. The following example shows the copy function: app.copySource(Ratt.Source.file(`${source_location}`), Ratt.Destination.TriplyDb.rdf(`${destination_name}`)) The function destination expects that source data is linked data. Copying a source that is not linked data can result in errors.","title":"Source destination"},{"location":"triply-etl/tmp/source-destination/#an-easier-way-to-configure-graph-names-and-prefixes","text":"Instead of setting the graph name and the prefixes for every ETL, you can use functions for their generation: export function create_prefixes( organization: string = default_organization, dataset: string, host: string = default_host ) { const prefix_base = Ratt.prefixer(`https://${host}/${organization}/${dataset}/`) const prefix_bnode = Ratt.prefixer(prefix_base(`.well-known/genid/`)) const prefix_graph = Ratt.prefixer(prefix_base(`graph/`)) return { bnode: prefix_bnode, graph: prefix_graph, } } For example, if host==='triplydb.com' , organization==='exampleOrganization' and dataset='pokemon' , then the prefix for the blank nodes will be https://triplydb.com/exampleOrganization/pokemon/.well-known/genid/ . Then, similarly, you can use another function for the graph names: export function create_graphs( dataset: string, organization: string = default_organization, host: string = default_host ) { const prefix = create_prefixes(dataset, organization, host) return { default: prefix.graph('default'), metadata: prefix.graph('metadata'), instances: prefix.graph('instances'), instances_report: prefix.graph('instances/report'), shapes: prefix.graph('shapes'), } }","title":"An easier way to configure graph names and prefixes"},{"location":"triply-etl/tmp/source-destination/#configuring-data-destinations","text":"","title":"Configuring data destinations"},{"location":"triply-etl/tmp/source-destination/#remote-data-destinations","text":"Destinations are usually online locations in TriplyDB where the output of your pipeline will be published. If no accountName is given, pipeline output is uploaded under the user account tied to the currently used API Token. Ratt.Destination.TriplyDb.rdf(datasetName) Ratt.Destination.TriplyDb.rdf(accountName, datasetName) Ratt.Destination.TriplyDb.rdf(accountName, datasetName, {overwrite: true}) The following options can be specified to configure the destination behavior: overwrite Whether the graphs that are being uploaded by RATT should replace any existing graphs with the same name in the dataset. Graphs appearing in the dataset with a different name than those uploaded by RATT are kept. The default value is false . synchronizeServices Whether active services should be automatically synchronized once new data is uploaded. The default value is false . triplyDb A configuration object describing a TriplyDB instance that is different from the one associated with the current API Token. (See the section on configuring multiple TriplyDB instance for more information.) truncateGraphs Whether to delete all graphs in the dataset before uploading any graphs from RATT. Notice that this will also remove graphs that will not be re-uploaded by RATT. The default value is false .","title":"Remote data destinations"},{"location":"triply-etl/tmp/source-destination/#local-data-destinations","text":"RATT supports publishing RDF output into a local file. This is not often used, because files lack many of the features that TriplyDB destinations support, such as: The ability to browse the data. The ability to query the data. The ability to configure metadata. The ability to configure prefix declarations. Still, there may be cases in which a local file destination is useful, for example when you do not have an active Internet connection: Ratt.Destination.file(\"my-file.trig\"),","title":"Local data destinations"},{"location":"triply-etl/tmp/source-destination/#static-and-dynamic-destinations","text":"Destinations can be defined as static objects meaning that you can define destination beforehand. But it might be the case that you want to have multiple destinations for different records. In this case, you would need a dynamic destination, which should change based on certain information inside your source data. You can set static and dynamic destinations, like below: const app = new Ratt({ sources: { someSource: Ratt.Source.file(\"source.trig\") }, destinations: { someStaticDestination: Ratt.Destination.file(\"static.ttl\"), someDynamicDestination: (ctx) => Ratt.Destination.file(ctx.getString(\"destination\")) }, })","title":"Static and Dynamic destinations"},{"location":"triply-etl/tmp/source-destination/#configuring-multiple-triplydb-instances","text":"It is possible to use multiple TriplyDB instances in one RATT pipeline. The following example illustrates how the data model is used from the production instance of TriplyDB. const account = 'Triply' const dataset = 'example' const app = new Ratt({ sources: { data_model: Ratt.Source.TriplyDb.rdf( account, dataset, {triplyDb: {token: process.env['PRODUCTION_INSTANCE_TOKEN'], url: 'https://api.production.example.com'}}), instance_data: Ratt.Source.TriplyDb.rdf( account, dataset, {triplyDb: {token: process.env['ACCEPTANCE_INSTANCE_TOKEN'], url: 'https://api.acceptance.example.com'}}), }, })","title":"Configuring multiple TriplyDB instances"},{"location":"triply-etl/tmp/source-destination/#direct-copying-of-source-data-to-destination","text":"RATT supports copying sources directly to destination locations. This function is useful when you already have linked data that is used as a source, but is also needed at the destination. An example would be the information model. This would be available as a source, and with the copy function it can be uploaded to TriplyDB via RATT. The following example shows the copy function: app.copySource(Ratt.Source.file(`${source_location}`), Ratt.Destination.TriplyDb.rdf(`${destination_name}`)) The function destination expects that source data is linked data. Copying a source that is not linked data can result in errors.","title":"Direct copying of source data to destination"},{"location":"triply-etl/tmp/static-dynamic-statements/","text":"Create dynamic statements Dynamic statements are statements that are based on some aspect of the source data. We use the following Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the Declare documentation for more information): const base = declarePrefix('https://triplydb.com/Triply/example/') const prefix = { def: declarePrefix(base('def/')), id: declarePrefix(base('id/')), xsd: declarePrefix('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: triple( iri(prefix.id, 'Country'), def.inhabitants, literal('Inhabitants', xsd.positiveInteger) ), Notice the following details: - iri() is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. app.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term. Static and dynamic triples Be aware that there are different approaches forstatic anddynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri() , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates thestatic* IRI [1b]. This IRI does not depend on the currently processed record. Notation [2a] creates thedynamic* IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed record. For a different record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates thedynamic* IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20 --> When should you use an IRI instead of an URI literal? An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri() , while an URI is created by using literal() . Limitation of literal() and iri() There is a limitation for both literal() and iri() . It is not possible to change the value in the record in the literal() and iri() assertions. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri() function or as a literal when called with the function literal() . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal() function. Instead we need to add a custom.change() middleware which will execute the transformation. custom.change({ key: 'Inhabitants', type: 'number', change: value => value / 1_000, }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ),","title":"Create dynamic statements"},{"location":"triply-etl/tmp/static-dynamic-statements/#create-dynamic-statements","text":"Dynamic statements are statements that are based on some aspect of the source data. We use the following Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the Declare documentation for more information): const base = declarePrefix('https://triplydb.com/Triply/example/') const prefix = { def: declarePrefix(base('def/')), id: declarePrefix(base('id/')), xsd: declarePrefix('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: triple( iri(prefix.id, 'Country'), def.inhabitants, literal('Inhabitants', xsd.positiveInteger) ), Notice the following details: - iri() is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. app.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term.","title":"Create dynamic statements"},{"location":"triply-etl/tmp/static-dynamic-statements/#static-and-dynamic-triples","text":"Be aware that there are different approaches forstatic anddynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri() , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates thestatic* IRI [1b]. This IRI does not depend on the currently processed record. Notation [2a] creates thedynamic* IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed record. For a different record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates thedynamic* IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20 -->","title":"Static and dynamic triples"},{"location":"triply-etl/tmp/static-dynamic-statements/#when-should-you-use-an-iri-instead-of-an-uri-literal","text":"An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri() , while an URI is created by using literal() .","title":"When should you use an IRI instead of an URI literal?"},{"location":"triply-etl/tmp/static-dynamic-statements/#limitation-of-literal-and-iri","text":"There is a limitation for both literal() and iri() . It is not possible to change the value in the record in the literal() and iri() assertions. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri() function or as a literal when called with the function literal() . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal() function. Instead we need to add a custom.change() middleware which will execute the transformation. custom.change({ key: 'Inhabitants', type: 'number', change: value => value / 1_000, }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ),","title":"Limitation of literal() and iri()"},{"location":"triply-etl/tmp/tmp/","text":"Create statements {#create-statements} After source data is connected and transformed, the RATT Record is ready to be transformed to linked data. Linked data statements are assertions or factual statements that consist of 3 terms (triple) or 4 terms (quadruples). Statements are created with the triple function. Calls to this function are part of the pipeline, and must appear inside the scope of app.use . Create static statements {#static-assertions} Static linked data statements are statements that only make use of constant terms (see working with IRIs ). Constant terms are introduced at the beginning of a RATT pipeline, typically prior to the occurrence of the first app.use scope. The following static statements make use of the constant terms introduced in the section on working with IRIs . app.use( // \u201cJohn is a person.\u201d triple(ex.john, a, foaf.Person), // \u201cMary is a person.\u201d triple(ex.mary, a, foaf.Person), ) Create dynamic statements {#dynamic-assertions} Dynamic statements are statements that are based on some aspect of the source data. We use the following RATT Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the section on working with IRIs for more information): const prefix_base = Ratt.prefixer('https://triplydb.com/Triply/example/') const prefix = { def: Ratt.prefixer(prefix_base('def/')), id: Ratt.prefixer(prefix_base('id/')), xsd: Ratt.prefixer('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: app.use( triple( iri('Country', {prefix: prefix.id}), def.inhabitants, literal('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - iri is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed RATT Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. app.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term. Static and dynamic triples Be aware that there are different approaches for static and dynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates the static IRI [1b]. This IRI does not depend on the currently processed RATT record. Notation [2a] creates the dynamic IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed RATT record. For a different RATT record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates the dynamic IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different RATT record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20 When should you use an IRI instead of an URI (which is a literal)? An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri , while an URI is created by using literal . Limitation of literal , iri and iri.hashed There is a limitation for both literal , iri and iri.hashed . It is not possible to change the value in the record in the literal , iri and iri.hashed middlewares. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri / iri.hashed function or as a literal when called with the function literal . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal function. Instead we need to add a change middleware which will execute the transformation. app.use( change({ key: 'Inhabitants', type: 'number', change: (value) => value/1000 }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ), ) Record IDs If your RATT Records do not contain a unique ID then you can use the recordId entry that RATT adds automatically. These recordId values are unique for every record processed in the same pipeline, but they are not an entry into the RATT Record by default. Record IDs are consistently assigned across runs of the same pipeline. They generate the same output as long as the input does not change. The following example code shows how the record ID can be added to each RATT Record: app.use( add({ key: 'ID', value: context => app.prefix.observation(context.recordId.toString()) }), triple(iri(prefix.id, key_id), a, def.Country), ) Process data conditionally {#process-data-conditionally} Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values to denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when function supports the creation of triples under certain conditions. The first argument that this function takes establishes whether or not a certain condition is met. After that, one or more additional statement arguments appear that will only be called if the condition is satisfied. The generic structure of when is as follows: app.use( when( '{condition}', '{statement-1}', '{statement-2}', '{statement-3}', ..., ) ) Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value. Null values {#null-values} If a key contains a null value in some records, then we need to specifically identify the criteria under which a triple must be added. app.use( // The source data uses '9999' to denote an unknown creation year. when( context => context.getNumber('CREATED') != 9999), triple( iri(prefix.id, 'ID'), dct.created, literal('CREATED', xsd.gYear))), Notice that the conditional function inside the when function takes the current RATT context as its single argument and returns a Boolean. Missing values If a value is sometimes completely missing from a source data record, then the following construct can be used to only add a triple in case the value is present: app.use( // The source data does not always include a value for 'zipcode'. when( context => context.isNotEmpty('ZIPCODE'), triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) Because missing values are very common in source data, RATT introduces special support for when the value for a specific key is missing. Instead of having to write context => context.isNotEmpty('foo') one can simply write the key name instead. The above example is equivalent to the following: app.use( // The source data does not always include a value for 'zipcode'. when( 'ZIPCODE', triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) It is also possible to check if a value is completely missing from the source data with ctx.isEmpty() A note for finding more methods RATT: One of the many advantages using Typescript is code completion. As such any methods available on a class in Ratt can be accessed using your IDE's intellisense ( ctrl + space in VSCODE). In Ratt the context and mw are two such classes that can be accessed in this way. The empty string Because source data often uses the empty string to signify NULL values, this particular string is treated in a special way by RATT. app.use( when( key.zipcode, // Skipped for the empty string. ...), ) Notice that it is almost never useful to store the empty string in linked data. So the treatment of the empty string as a NULL value is the correct default behavior. Custom functions If we want to extract a string value from the source data, we can write a custom function which can be used with when . when can receive two parameters: string(a key value) or a function. If when receives a string, it checks whether it is empty or not. But in case of a custom method specific instructions are required. For example, (ctx) => ctx.isNotEmpty('foo') && ctx.getString('foo') === 'foo' Notice details: ctx.isNotEmpty('foo') checks whether the string is empty or not and only if it is not empty, the function moves to the next step ctx.getString('bla') === 'something\u2019 , which is the next step, extracts 'foo' when it fulfills the required criteria Tree-shaped data Tree-shaped data is very common in different source systems. We will use the following JSON source data as an example in this section: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"id\": \"nl\", \"name\": \"The Netherlands\" }, { \"id\": \"de\", \"name\": \"Germany\" } ] } } The principles that are documented in this section can be applied to any form of tree-shaped data. For example, the following XML snippet is very similar to the JSON example: <?xml version=\"1.0\"?> <root> <metadata> <title> <name>Data about countries.</name> </title> </metadata> <data> <countries> <id>nl</id> <name>The Netherlands</name> </countries> <countries> <id>de</id> <name>Germany</name> </countries> </data> </root> Specifying paths (nested keys) In tabular data, keys (or column names) are singular. But in tree-shaped data a path of the tree can consist of one or more keys that must be traversed in sequence. Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the JSON example in the previous section, RATT can access the \"name\" key inside the \"title\" key, which itself is nested inside the \"metadata\" key. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which also accesses the \"name\" key, but nested inside the \"countries\" and then \"data\" keys. (The use of the [0] index is explained in the next section.) [1] metadata.title.name [2] data.countries[0].name Path expressions can be used as string keys in many places in RATT. For example, we can assert the title of a dataset in the following way: app.use( triple( prefix.dataset('my-dataset'), dct.title, literal('metadata.title.name', 'en')), ) This results in the following assertion: dataset:my-dataset dct:title 'Data about countries.'@en. Dealing with dots in RATT keys Mishandling dots in RATT keys can be quite troubling and difficult to detect since RATT would not always show an error in the code. In order to prevent that, there is a syntax that allow us to give the code the functionality that is needed. RATT uses the lodash library to implement dot-based path notation. Example: when('narrower_term_lref', [ triple(iri('_entity'), la.has_member, iri(prefix.collectors, 'narrower_term_lref[0].$text')), ]), when('[\"soort_collectie.lref\"]', [ triple(iri('_entity'), crm.P2_has_type, iri(prefix.thesaurus, '[\"soort_collectie.lref\"][0].$text')), ]), Here we can notice that in the first code snippet the notation does not seem to have extra requirements since it is referring to a key that does not use a special character such as dot. The second one, however, has a condition name that contains a dot. Therefore, when conditioning the statement we use the \u2018[\u201ca.b\u201d]\u2019 syntax. In this case we can observe using a RATT key as an array key. If we need an element from this array, the key should be addressed with the name notation \u2013 \u2018[\u201ca.b\u201d].$text\u2019. Overall, \u2018a.b\u2019 notation allow going into nested object and accessing values within the nest while \u2018[\u201ca.b\u201d]\u2019 takes value a.b key as a name, therefore does not go into the nest. In the following example the differences can be seen with the corresponding result: { \"a\": { \"$text\": \"1\" }, \"b\": { \"c\": { \"$text\": \"2\" } }, \"b.c\": { \"$text\": \"3\" }, \"d.d\": { \"e\": { \"$text\": \"4\" }, \"f\": { \"$text\": \"5\" } }, \"g.g\": [ { \"h.h\": { \"$text\": \"6\" } }, { \"h.h\": { \"$text\": \"7\" } } ] } Key Value 'a.$text' 1 'b.c.$text' 2 '[\"b.c\"].$text' 3 '[\"d.d\"].e.$text' 4 '[\"d.d\"].f'.$text' 5 '[\"g.g\"][0][\"h.h\"].$text' 6 '[\"g.g\"][1][\"h.h\"].$text' 7 Using the example at the top: when('[\"soort_collectie.lref\"]', [ triple(iri('_entity'), crm.P2_has_type, iri(prefix.thesaurus, '[\"soort_collectie.lref\"][0].$text')), ]), \u2502 \"soort_collectie\": [ \u2502 \u2502 { \u2502 \u2502 \"value\": [ \u2502 \u2502 { \u2502 \u2502 \"$text\": \"museum\", \u2502 \u2502 \"@invariant\": \"false\", \u2502 \u2502 \"@lang\": \"en-US\" \u2502 \u2502 }, \u2502 \u2502 { \u2502 \u2502 \"$text\": \"museum\", \u2502 \u2502 \"@invariant\": \"false\", \u2502 \u2502 \"@lang\": \"nl-NL\" \u2502 \u2502 } \u2502 \u2502 ] \u2502 \u2502 } \u2502 \u2502 ], \u2502 \u2502 \"soort_collectie.lref\": [ \u2502 \u2502 { \u2502 \u2502 \"$text\": \"63335\" \u2502 \u2502 } \u2502 \u2502 ], | Key | Value | | ------------------------ | ----------- | | \u2018[\u201csoort_collectie.lref\u201d][0].$text | 63335 | | \u2018soort_collectie.lref[0].$text\u2019 | empty | | \u2018soort_collectie.value[0]$text\u2019 | museum | Accessing lists by index {#accessing-lists-by-index} Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. RATT is able to access specific elements from lists based on their index or position. Following the standard practice in Computer Science, RATT refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of the first country as follows: app.use( triple( iri(prefix.country, 'data.countries[0].id'), rdfs.label, literal('data.countries[0].name', 'en')), ) This results in the following assertion: country:nl rdfs:label 'The Netherlands'@en. We can also assert the name of the second country. Notice that only the index is different (\u20181\u2019 instead of \u20180\u2019): app.use( triple( iri(prefix.country, 'data.countries[1].id'), This results in the following assertion: country:de rdfs:label 'Germany'@en. Iterating over lists of objects {#list-object} In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want RATT to make an assertion for every element in a list. RATT uses the forEach function for this purpose. The following code snippet asserts the name for each country in the example data: app.use( forEach('data.countries', triple( iri(prefix.country, 'id'), rdfs.label, literal('name', 'en'))), ) Notice the following details: - forEach uses the path expression 'data.countries' to identify the list. - Inside the forEach function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'@en. country:de rdfs:label 'Germany'@en. Notice that forEach only works for lists whose elements are objects . See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach iterates over are themselves RATT records. This implies that all functions that work for full RATT records also work for the RATT records inside forEach . The RATT records inside an forEach function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, RATT records inside forEach also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root ) Index key ( $index ) {#index-key} Each RATT record that is made available in forEach contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" }, \u2026 ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: app.use( forEach('countries', triple( iri(prefix.country, '$index'), rdfs.label, literal('name', 'en'))), ) This results in the following assertions: country:0 rdfs:label 'The Netherlands'@en. country:1 rdfs:label 'Germany'@en. country:2 rdfs:label 'Italy'@en. Parent key ( $parent ) {#parent-key} When forEach iterates through a list of elements, it makes the enclosing parent record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach . For example, the parent record in the following call is the record that directly contains the \"data\" key: app.use( forEach('data.countries', \u2026 ) ) The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: app.use( forEach('data.countries', logRecord()) ) For our example source data, this emits the following 2 RATT records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section . Root key ( $root ) {#root-key} Sometimes it may be necessary to access a part of the original RATT record that is outside of the scope of the forEach call. Every RATT record inside a forEach call contains the \"$root\" key. The value of the root key provides a link to the full RATT record. Because the $root key is part of the linked-to RATT record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: app.use( forEach('data.countries', forEach('labels', logRecord())), ) The following RATT record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" } Iterating over lists of primitives {#list-primitive} In the previous section we showed how to iterate over lists of objects. But what happens if a list does not contain objects but elements of primitive type? Examples include lists of strings or lists of numbers. Function forEach does not work with lists containing primitive types, because it assumes a RATT record structure which can only be provided by objects. Luckily, RATT includes the functions iri.forEach and literal.forEach that can be specifically used to iterate over lists of primitives. app.use( fromJson({\"id\": \"nl\", \"names\": [\"The Netherlands\", \"Holland\"]}), triple( iri(prefix.country, 'id'), rdfs.label, literal.forEach('names', 'en')), ) This makes the following assertion: country:nl rdfs:label 'The Netherlands'@en, 'Holland'@en. Transforming RDF data If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const app = new Ratt({ defaultGraph: graph.model, prefixes: prefix, sources: { inputFile: Ratt.Source.file(`data/shapes.trig`) }, destinations: { dataset: Ratt.Destination.TriplyDb.rdf(organization, dataset, remoteOptions) }, }) app.use( loadRdf(app.sources.inputFile), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, app.prefix.somePrefix(\"graph\") ) ), toRdf(app.destinations.dataset) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} )","title":"Tmp"},{"location":"triply-etl/tmp/tmp/#create-statements-create-statements","text":"After source data is connected and transformed, the RATT Record is ready to be transformed to linked data. Linked data statements are assertions or factual statements that consist of 3 terms (triple) or 4 terms (quadruples). Statements are created with the triple function. Calls to this function are part of the pipeline, and must appear inside the scope of app.use .","title":"Create statements {#create-statements}"},{"location":"triply-etl/tmp/tmp/#create-static-statements-static-assertions","text":"Static linked data statements are statements that only make use of constant terms (see working with IRIs ). Constant terms are introduced at the beginning of a RATT pipeline, typically prior to the occurrence of the first app.use scope. The following static statements make use of the constant terms introduced in the section on working with IRIs . app.use( // \u201cJohn is a person.\u201d triple(ex.john, a, foaf.Person), // \u201cMary is a person.\u201d triple(ex.mary, a, foaf.Person), )","title":"Create static statements {#static-assertions}"},{"location":"triply-etl/tmp/tmp/#create-dynamic-statements-dynamic-assertions","text":"Dynamic statements are statements that are based on some aspect of the source data. We use the following RATT Record as an example: Country Inhabitants France null Germany 83190556 Netherlands 17650200 We start with creating the prefix and term declarations (see the section on working with IRIs for more information): const prefix_base = Ratt.prefixer('https://triplydb.com/Triply/example/') const prefix = { def: Ratt.prefixer(prefix_base('def/')), id: Ratt.prefixer(prefix_base('id/')), xsd: Ratt.prefixer('http://www.w3.org/2001/XMLSchema#'), } const def = { Country: prefix.def('Country'), name: prefix.def('inhabitants'), } const xsd = { positiveInteger: prefix.xsd('positiveInteger'), string: prefix.xsd('string'), } const input_string = ['Country', 'inhabitants'] With these prefix and term constants in place, a dynamic statement is created as follows: app.use( triple( iri('Country', {prefix: prefix.id}), def.inhabitants, literal('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - iri is used to create a dynamic IRI term. - Arguments Country and Inhabitants allow values for these keys to be used from processed RATT Records. - The IRI prefix for the subject term is specified with constant prefix.id . - literal is used to create a dynamic literal term. - For literals a datatype IRI can be specified. If no datatype IRI is specified then the default IRI is xsd.string . iri.hashed can be used instead of iri when the ETL has a high number of blank nodes and they need more than one constant as input to hash a unique IRI. app.use( triple( iri.hashed(prefix.id, input_string), def.inhabitants, mw.toLiteral('Inhabitants', {datatype: xsd.positiveInteger})), ) Notice the following details: - input_string can pass more than one constant to hash a unique IRI term.","title":"Create dynamic statements {#dynamic-assertions}"},{"location":"triply-etl/tmp/tmp/#static-and-dynamic-triples","text":"Be aware that there are different approaches for static and dynamic IRIs: Static IRIs are created with prefix declarations (example [1a]). Dynamic IRIs are created with iri , iri.hashed and prefix declarations (example [2a]). [1a] prefix.id('person') [2a] iri(prefix.id, 'person'), [3a] iri.hashed(prefix.id, ['person','age']), Notation [1a] creates the static IRI [1b]. This IRI does not depend on the currently processed RATT record. Notation [2a] creates the dynamic IRI in [2b], assuming the \"person\" key contains the value \"John\" . This IRI depends on the currently processed RATT record. For a different RATT record, IRI [2c] may be created instead (assuming the \"person\" key contains the value \"Jane\" ). Notation [3a] creates the dynamic IRI in [3b], assuming the \"person\" key contains the value \"Sam\" and the \"age\" key contains the value \"30\" . For a different RATT record, IRI [3c] may be created instead (assuming the \"person\" key contains the value \"Roland\" and \"age\" key contains the value \"20\" ). [1b] id:person [2b] id:John [2c] id:Jane [3b] id:Sam , age: 30 [3c] id:Sam , age: 20","title":"Static and dynamic triples"},{"location":"triply-etl/tmp/tmp/#when-should-you-use-an-iri-instead-of-an-uri-which-is-a-literal","text":"An IRI is used to identify something, for example the city of Amsterdam. It is expected that accessing it returns linked data. An IRI can be used to make assertions about a subject. On the other hand, a URI is expected to return a non-linked data content, for example an HTML website, and can be used as objects in linked data, for example for inserting further information about the subject resource. In the example below, the subject IRI is described further by the object's URL. <https://dbpedia.org/resource/Amsterdam> rdfs:seeAlso \"https://www.iamsterdam.com\"^^xsd:anyURI. An IRI can be created with iri , while an URI is created by using literal .","title":"When should you use an IRI instead of an URI (which is a literal)?"},{"location":"triply-etl/tmp/tmp/#limitation-of-literal-iri-and-irihashed","text":"There is a limitation for both literal , iri and iri.hashed . It is not possible to change the value in the record in the literal , iri and iri.hashed middlewares. The value that is at that moment stored in the record for that key, is then added as either an IRI when called with the iri / iri.hashed function or as a literal when called with the function literal . The limitation is shown in the example below. In the example we want to round the inhabitants number to the nearest thousand. We can not transform this in the literal function. Instead we need to add a change middleware which will execute the transformation. app.use( change({ key: 'Inhabitants', type: 'number', change: (value) => value/1000 }), triple( iri(prefix.id, 'Country'), def.name, literal('Inhabitants', xsd.positiveInteger) ), )","title":"Limitation of literal, iri and iri.hashed"},{"location":"triply-etl/tmp/tmp/#record-ids","text":"If your RATT Records do not contain a unique ID then you can use the recordId entry that RATT adds automatically. These recordId values are unique for every record processed in the same pipeline, but they are not an entry into the RATT Record by default. Record IDs are consistently assigned across runs of the same pipeline. They generate the same output as long as the input does not change. The following example code shows how the record ID can be added to each RATT Record: app.use( add({ key: 'ID', value: context => app.prefix.observation(context.recordId.toString()) }), triple(iri(prefix.id, key_id), a, def.Country), )","title":"Record IDs"},{"location":"triply-etl/tmp/tmp/#process-data-conditionally-process-data-conditionally","text":"Source data often contains optional values. These are values that appear in some, but not all records. Source data often contains 'special' values to denote the absence of a value. Common examples are values such as 'NULL' or the empty string ( '' ) or 'clear' outliers such as 9999 for a missing year. We call such values \u2018null values\u2019. The when function supports the creation of triples under certain conditions. The first argument that this function takes establishes whether or not a certain condition is met. After that, one or more additional statement arguments appear that will only be called if the condition is satisfied. The generic structure of when is as follows: app.use( when( '{condition}', '{statement-1}', '{statement-2}', '{statement-3}', ..., ) ) Notice that it is often useful to specify multiple statements under the same condition: The first statement transforms an optional value, and the second statement uses the transformed optional value to make a triple assertion. The first statement asserts one triple based on the optional value, and the second statement asserts a second triple based on the same optional value.","title":"Process data conditionally {#process-data-conditionally}"},{"location":"triply-etl/tmp/tmp/#null-values-null-values","text":"If a key contains a null value in some records, then we need to specifically identify the criteria under which a triple must be added. app.use( // The source data uses '9999' to denote an unknown creation year. when( context => context.getNumber('CREATED') != 9999), triple( iri(prefix.id, 'ID'), dct.created, literal('CREATED', xsd.gYear))), Notice that the conditional function inside the when function takes the current RATT context as its single argument and returns a Boolean.","title":"Null values {#null-values}"},{"location":"triply-etl/tmp/tmp/#missing-values","text":"If a value is sometimes completely missing from a source data record, then the following construct can be used to only add a triple in case the value is present: app.use( // The source data does not always include a value for 'zipcode'. when( context => context.isNotEmpty('ZIPCODE'), triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) Because missing values are very common in source data, RATT introduces special support for when the value for a specific key is missing. Instead of having to write context => context.isNotEmpty('foo') one can simply write the key name instead. The above example is equivalent to the following: app.use( // The source data does not always include a value for 'zipcode'. when( 'ZIPCODE', triple( iri(prefix.id, 'ID'), def.zipcode, literal('ZIPCODE')), ..., ), ) It is also possible to check if a value is completely missing from the source data with ctx.isEmpty() A note for finding more methods RATT: One of the many advantages using Typescript is code completion. As such any methods available on a class in Ratt can be accessed using your IDE's intellisense ( ctrl + space in VSCODE). In Ratt the context and mw are two such classes that can be accessed in this way.","title":"Missing values"},{"location":"triply-etl/tmp/tmp/#the-empty-string","text":"Because source data often uses the empty string to signify NULL values, this particular string is treated in a special way by RATT. app.use( when( key.zipcode, // Skipped for the empty string. ...), ) Notice that it is almost never useful to store the empty string in linked data. So the treatment of the empty string as a NULL value is the correct default behavior.","title":"The empty string"},{"location":"triply-etl/tmp/tmp/#custom-functions","text":"If we want to extract a string value from the source data, we can write a custom function which can be used with when . when can receive two parameters: string(a key value) or a function. If when receives a string, it checks whether it is empty or not. But in case of a custom method specific instructions are required. For example, (ctx) => ctx.isNotEmpty('foo') && ctx.getString('foo') === 'foo' Notice details: ctx.isNotEmpty('foo') checks whether the string is empty or not and only if it is not empty, the function moves to the next step ctx.getString('bla') === 'something\u2019 , which is the next step, extracts 'foo' when it fulfills the required criteria","title":"Custom functions"},{"location":"triply-etl/tmp/tmp/#tree-shaped-data","text":"Tree-shaped data is very common in different source systems. We will use the following JSON source data as an example in this section: { \"metadata\": { \"title\": { \"name\": \"Data about countries.\" } }, \"data\": { \"countries\": [ { \"id\": \"nl\", \"name\": \"The Netherlands\" }, { \"id\": \"de\", \"name\": \"Germany\" } ] } } The principles that are documented in this section can be applied to any form of tree-shaped data. For example, the following XML snippet is very similar to the JSON example: <?xml version=\"1.0\"?> <root> <metadata> <title> <name>Data about countries.</name> </title> </metadata> <data> <countries> <id>nl</id> <name>The Netherlands</name> </countries> <countries> <id>de</id> <name>Germany</name> </countries> </data> </root>","title":"Tree-shaped data"},{"location":"triply-etl/tmp/tmp/#specifying-paths-nested-keys","text":"In tabular data, keys (or column names) are singular. But in tree-shaped data a path of the tree can consist of one or more keys that must be traversed in sequence. Paths are specified as dot-separated sequences of keys, starting at the top-level and ending at the required value. For the JSON example in the previous section, RATT can access the \"name\" key inside the \"title\" key, which itself is nested inside the \"metadata\" key. This path is expressed in [1]. Notice that the path expressed in [1] is different from the path expressed in [2], which also accesses the \"name\" key, but nested inside the \"countries\" and then \"data\" keys. (The use of the [0] index is explained in the next section.) [1] metadata.title.name [2] data.countries[0].name Path expressions can be used as string keys in many places in RATT. For example, we can assert the title of a dataset in the following way: app.use( triple( prefix.dataset('my-dataset'), dct.title, literal('metadata.title.name', 'en')), ) This results in the following assertion: dataset:my-dataset dct:title 'Data about countries.'@en.","title":"Specifying paths (nested keys)"},{"location":"triply-etl/tmp/tmp/#dealing-with-dots-in-ratt-keys","text":"Mishandling dots in RATT keys can be quite troubling and difficult to detect since RATT would not always show an error in the code. In order to prevent that, there is a syntax that allow us to give the code the functionality that is needed. RATT uses the lodash library to implement dot-based path notation. Example: when('narrower_term_lref', [ triple(iri('_entity'), la.has_member, iri(prefix.collectors, 'narrower_term_lref[0].$text')), ]), when('[\"soort_collectie.lref\"]', [ triple(iri('_entity'), crm.P2_has_type, iri(prefix.thesaurus, '[\"soort_collectie.lref\"][0].$text')), ]), Here we can notice that in the first code snippet the notation does not seem to have extra requirements since it is referring to a key that does not use a special character such as dot. The second one, however, has a condition name that contains a dot. Therefore, when conditioning the statement we use the \u2018[\u201ca.b\u201d]\u2019 syntax. In this case we can observe using a RATT key as an array key. If we need an element from this array, the key should be addressed with the name notation \u2013 \u2018[\u201ca.b\u201d].$text\u2019. Overall, \u2018a.b\u2019 notation allow going into nested object and accessing values within the nest while \u2018[\u201ca.b\u201d]\u2019 takes value a.b key as a name, therefore does not go into the nest. In the following example the differences can be seen with the corresponding result: { \"a\": { \"$text\": \"1\" }, \"b\": { \"c\": { \"$text\": \"2\" } }, \"b.c\": { \"$text\": \"3\" }, \"d.d\": { \"e\": { \"$text\": \"4\" }, \"f\": { \"$text\": \"5\" } }, \"g.g\": [ { \"h.h\": { \"$text\": \"6\" } }, { \"h.h\": { \"$text\": \"7\" } } ] } Key Value 'a.$text' 1 'b.c.$text' 2 '[\"b.c\"].$text' 3 '[\"d.d\"].e.$text' 4 '[\"d.d\"].f'.$text' 5 '[\"g.g\"][0][\"h.h\"].$text' 6 '[\"g.g\"][1][\"h.h\"].$text' 7 Using the example at the top: when('[\"soort_collectie.lref\"]', [ triple(iri('_entity'), crm.P2_has_type, iri(prefix.thesaurus, '[\"soort_collectie.lref\"][0].$text')), ]), \u2502 \"soort_collectie\": [ \u2502 \u2502 { \u2502 \u2502 \"value\": [ \u2502 \u2502 { \u2502 \u2502 \"$text\": \"museum\", \u2502 \u2502 \"@invariant\": \"false\", \u2502 \u2502 \"@lang\": \"en-US\" \u2502 \u2502 }, \u2502 \u2502 { \u2502 \u2502 \"$text\": \"museum\", \u2502 \u2502 \"@invariant\": \"false\", \u2502 \u2502 \"@lang\": \"nl-NL\" \u2502 \u2502 } \u2502 \u2502 ] \u2502 \u2502 } \u2502 \u2502 ], \u2502 \u2502 \"soort_collectie.lref\": [ \u2502 \u2502 { \u2502 \u2502 \"$text\": \"63335\" \u2502 \u2502 } \u2502 \u2502 ], | Key | Value | | ------------------------ | ----------- | | \u2018[\u201csoort_collectie.lref\u201d][0].$text | 63335 | | \u2018soort_collectie.lref[0].$text\u2019 | empty | | \u2018soort_collectie.value[0]$text\u2019 | museum |","title":"Dealing with dots in RATT keys"},{"location":"triply-etl/tmp/tmp/#accessing-lists-by-index-accessing-lists-by-index","text":"Tree-shaped data formats often allow multiple values to be specified in an ordered list. Examples of this are arrays in JSON and XML elements with the same tag that are directly nested under the same parent element. RATT is able to access specific elements from lists based on their index or position. Following the standard practice in Computer Science, RATT refers to the first element in the list as having index 0. The second element has index 1, etc. For the above example record, we can assert the name of the first country as follows: app.use( triple( iri(prefix.country, 'data.countries[0].id'), rdfs.label, literal('data.countries[0].name', 'en')), ) This results in the following assertion: country:nl rdfs:label 'The Netherlands'@en. We can also assert the name of the second country. Notice that only the index is different (\u20181\u2019 instead of \u20180\u2019): app.use( triple( iri(prefix.country, 'data.countries[1].id'), This results in the following assertion: country:de rdfs:label 'Germany'@en.","title":"Accessing lists by index {#accessing-lists-by-index}"},{"location":"triply-etl/tmp/tmp/#iterating-over-lists-of-objects-list-object","text":"In the previous section, we saw that we were able to assert the name of the first country and the name of the second country. But what do we do if we want to assert the name for every country in the world? And what do we do if some countries have a name in 2 languages, but other countries have a name in 1 or 3 languages? What we need is a simple way to express that we want RATT to make an assertion for every element in a list. RATT uses the forEach function for this purpose. The following code snippet asserts the name for each country in the example data: app.use( forEach('data.countries', triple( iri(prefix.country, 'id'), rdfs.label, literal('name', 'en'))), ) Notice the following details: - forEach uses the path expression 'data.countries' to identify the list. - Inside the forEach function, each element in the list is made available separately. - This allows the 'id' and 'name' keys to be identified directly. The above code snippet makes one assertion for every element in the \"countries\" list: country:nl rdfs:label 'The Netherlands'@en. country:de rdfs:label 'Germany'@en. Notice that forEach only works for lists whose elements are objects . See Iterating over lists of primitives for dealing with lists that do not contain objects. The elements that forEach iterates over are themselves RATT records. This implies that all functions that work for full RATT records also work for the RATT records inside forEach . The RATT records inside an forEach function are smaller. This allows the regular keys of the iterated-over elements to be accessed directly. In addition to these regular keys, RATT records inside forEach also contain additional keys that simplify common operations. The following subsections explain the following special keys: Index key ( $index ) Parent key ( $parent ) Root key ( $root )","title":"Iterating over lists of objects {#list-object}"},{"location":"triply-etl/tmp/tmp/#index-key-index-index-key","text":"Each RATT record that is made available in forEach contains the $index key. The value of this key is the index of the element in the list. This is the same index that is used to access specific elements in an list, as explained in the section on accessing lists by index . The index key is often useful for assigning a unique subject IRI to every element. Suppose we have the following source data. We do not want to use the values of the \"name\" key for our subject IRI, because these names contain spaces and possibly other problematic characters that make the IRI more difficult to read and use. { \"countries\": [ { \"name\": \"The Netherlands\" }, { \"name\": \"Germany\" }, { \"name\": \"Italy\" }, \u2026 ] } The following code snippet uses the $index key that is made available inside forEach in order to create a unique subject IRI for each country: app.use( forEach('countries', triple( iri(prefix.country, '$index'), rdfs.label, literal('name', 'en'))), ) This results in the following assertions: country:0 rdfs:label 'The Netherlands'@en. country:1 rdfs:label 'Germany'@en. country:2 rdfs:label 'Italy'@en.","title":"Index key ($index) {#index-key}"},{"location":"triply-etl/tmp/tmp/#parent-key-parent-parent-key","text":"When forEach iterates through a list of elements, it makes the enclosing parent record available under key $parent . The parent record is the record that directly contains the first key that appears in the path that was specified in forEach . For example, the parent record in the following call is the record that directly contains the \"data\" key: app.use( forEach('data.countries', \u2026 ) ) The $parent key can be observed when logRecord` is used to print the iterated-over elements to the terminal: app.use( forEach('data.countries', logRecord()) ) For our example source data, this emits the following 2 RATT records: { \"id\": \"en\", \"name\": \"The Netherlands\", \"$index\": 0, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } and: { \"id\": \"de\", \"name\": \"Germany\", \"$index\": 1, \"$parent\": { \"data\": { \"labels\": [ { \"id\": \"en\", \"name\": \"The Netherlands\", }, { \"id\": \"de\" \"name\": \"Germany\", } ] } }, \"$root\": \"__circular__\" } The $root key is explained in the next section .","title":"Parent key ($parent) {#parent-key}"},{"location":"triply-etl/tmp/tmp/#root-key-root-root-key","text":"Sometimes it may be necessary to access a part of the original RATT record that is outside of the scope of the forEach call. Every RATT record inside a forEach call contains the \"$root\" key. The value of the root key provides a link to the full RATT record. Because the $root key is part of the linked-to RATT record, it is not possible to print the value of the root key. (This would result in infinite output.) For this reason, the value of the $root key is printed as the special value \"__circular__\" . For the above examples, the parent record and root record are the same, but this is not always the case. Specifically, the parent record and root record are different when forEach calls are nested. The following data contains an inner list (key \"labels\" ) inside an outer list ( \"countries\" ): { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ] }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } } The following nested forEach call shows the difference between the \"$parent\" key and the $root key. The $parent key allows the individual country objects to be accessed, while the \"$root\" key allows the entire tree to be accessed: app.use( forEach('data.countries', forEach('labels', logRecord())), ) The following RATT record is printed first (3 records are printed in total). Notice that the value of the outer $parent and \"$root\" keys are now different: - The $parent key allows access to the first country. - The $root key allows access to the full record (describing multiple countries). { \"name\": \"The Netherlands\", \"locale\": \"en-us\", \"$index\": 0, \"$parent\": { \"id\": \"NL\", \"labels\": [ { \"name\": \"The Netherlands\", \"locale\": \"en-us\" }, { \"name\": \"Nederland\", \"locale\": \"nl-nl\" } ], \"$index\": 0, \"$parent\": { \"data\": { \"countries\": [ { \"id\": \"NL\", \"labels\": \"__circular__\" }, { \"id\": \"EN\", \"labels\": [ { \"name\": \"England\", \"locale\": \"en-gb\" } ] } ] } }, \"$root\": \"__circular__\" }, \"$root\": \"__circular__\" }","title":"Root key ($root) {#root-key}"},{"location":"triply-etl/tmp/tmp/#iterating-over-lists-of-primitives-list-primitive","text":"In the previous section we showed how to iterate over lists of objects. But what happens if a list does not contain objects but elements of primitive type? Examples include lists of strings or lists of numbers. Function forEach does not work with lists containing primitive types, because it assumes a RATT record structure which can only be provided by objects. Luckily, RATT includes the functions iri.forEach and literal.forEach that can be specifically used to iterate over lists of primitives. app.use( fromJson({\"id\": \"nl\", \"names\": [\"The Netherlands\", \"Holland\"]}), triple( iri(prefix.country, 'id'), rdfs.label, literal.forEach('names', 'en')), ) This makes the following assertion: country:nl rdfs:label 'The Netherlands'@en, 'Holland'@en.","title":"Iterating over lists of primitives {#list-primitive}"},{"location":"triply-etl/tmp/tmp/#transforming-rdf-data","text":"If you have RDF data that does not need to be transformed, see copying source data . If you have RDF data that does need to be transformed, you can use the following pattern. This example renames the graph. const app = new Ratt({ defaultGraph: graph.model, prefixes: prefix, sources: { inputFile: Ratt.Source.file(`data/shapes.trig`) }, destinations: { dataset: Ratt.Destination.TriplyDb.rdf(organization, dataset, remoteOptions) }, }) app.use( loadRdf(app.sources.inputFile), mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, quad.predicate, quad.object, app.prefix.somePrefix(\"graph\") ) ), toRdf(app.destinations.dataset) ) Similarly, you can change all the subject, predicates or objects in your data. Also, you can choose to transform triples of a specific subject, predicate, object or graph name. in this case, you should use: mapQuads( (quad, ctx) => ctx.store.quad( quad.subject, app.prefix.example('new-predicate'), quad.object, quad.graph ), {predicate: app.prefix.example(\"old-predicate\")} )","title":"Transforming RDF data"},{"location":"triply-etl/transform/","text":"The Transform step makes changes to the Record: graph LR source -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> tdb linkStyle 1 stroke:red,stroke-width:3px; ld[Internal Store] record[Record] source[Data Sources] tdb[(Triple Store)] If you do not have a stream of records yet, read the documentation for the Extract step first. Once you have a stream of records, the following transformations are typically needed: - Values need to be mapped onto a prepared list of IRIs or literals (e.g. from country names to country-denoting IRIs). - Values need to be translated into standards-compliant formats (e.g., from country name to ISO 3166 country codes). - Multiple existing values need to be combined into one new value (e.g., street name and house number may be combined into an address). - A single value needs to be split into multiple values (e.g., from 'apple, orange' to 'apple' and 'orange' ). - Values need to be cleaned because they are dirty in the source (e.g., from '001 ' to 1 ). TriplyETL supports the following transformation approaches: 2A. RATT transformations are a set of commonly used transformation functions that are developed and maintained by Triply. 2B. TypeScript can be used to write new customer transformations. Next steps The Transform step results in a cleaned and enriched record. The following link documents how you can use the record to make linked data assertions: 3. Assert uses data from the Record to generate linked data in the Internal Store.","title":"2. TriplyETL: Transform"},{"location":"triply-etl/transform/#next-steps","text":"The Transform step results in a cleaned and enriched record. The following link documents how you can use the record to make linked data assertions: 3. Assert uses data from the Record to generate linked data in the Internal Store.","title":"Next steps"},{"location":"triply-etl/transform/ratt/","text":"RATT transformations are a core set of functions that are commonly used to change the content of TriplyETL Records. RATT transformations started out as TypeScript transformations that turned out to be useful in a wide variety of TriplyETL pipelines. Triply maintains this core set of transformation functions to allow new ETLs to make use of off-the-shelf functionality that has proven useful in the past. Overview The following transformation functions are currently available: Function Description addHashedIri() Create a new IRI with a content-based local name. addIri() Create a new IRI based on a prefix and a local name. addLiteral() Create a new literal based on a lexical for and a datatype IRI or language tag. addRandomIri() Create a new IRI with a random local name. addSkolemIri() Create a new IRI with a random local name, which advertises that it may be consistently replaced with blank nodes. addTag() Create a language tag. addValue() Create a TypeScript value. capitalize() Transforms a string value to its capitalized variant. concat() Combine multiple strings into a new string. copy() Copy a value from an old into a new key. decodeHtml() Decode HTML entities that occur in strings. geojsonToWkt() Change GeoJSON strings to WKT strings. lowercase() Change strings to their lowercase variants. padEnd() Pad the end of strings. padStart() Pad the start of strings. replace() Replace part of a string. split() Split a string into multiple substrings. substring() Extract a substring from a string. translateAll() Translate all string values to other values. translateSome() Translate some string values to other strings. tryLiteral() Create literals for which the datatype is not know beforehand. uppercase() Change a string to its uppercase variant. wkt.addPoint() Add a geospatial point using the Well-Known Text (WKT) format. wkt.project() Change the projection of a Well-Known Text (WKT) literal from from Coordinate Reference System into another. Function addHashedIri() {#addHashedIri} Creates an IRI based on the specified IRI prefix and the hash calculated over the input content string(s). Parameters prefix An IRI, or a key that contains an IRI value. content A key that contains a string value, or a string value specified with function str() . key A new key where the created hashed IRI is stored. Use cases This function is used under the following circumstances: 1. You want to identify something with an IRI. 2. The thing that you want to identify does not have a readily available identifier. 3. The thing that you want to identify has one or more properties that together allow the thing to be uniquely identified. A benefit of addHashedIri () is that the created IRIs are the same across different ETL runs over the same source data. A downside of addHashedIri() is that it can take a lot of time to figure out which set of properties make every IRI unique. (In database theory this process is known as 'composite key detection'.) Furthermore, having to adjust the hashed content later may pose a maintenance burden. Example: Lazy identifiers Some source data does not include good identifiers for all data items. The following source table does not include a good identifier. What is more, none of the columns contains values that are unique for each row: First name Last name Jane Doe Jane Smith John Doe In such cases it may be an option to take a combination of columns, and use that combined sequence of columns for identification. This is called a 'composite key' in database theory. The following snippet uses the combination of the first name and last name fields (in that order) to create a locally unique hash, that can be used to create a globally IRI. (This does assume that every person in the dataset has a unique first/last name combination!) fromXlsx(Source.file('data.xlsx')), addHashedIri({ prefix: prefix.person, content: ['First name', 'Last name'], key: '_person', }), pairs('_person', [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), This results in the following linked data assertions: person:70020947bea6c39cccea20d27e30fbdf a sdo:Person; sdo:givenName 'John'; familyName 'Doe'. Or diagrammatically: graph LR person -- a --> Person person -- sdo:givenName --> john person -- sdo:familyName --> doe Person[sdo:Person]:::model doe['Doe']:::data john['John']:::data person([person:70020947bea6c39cccea20d27e30fbdf]):::data classDef model fill:lightblue classDef meta fill:sandybrown Example: Statement Reification The RDF standard allows individual statements to be identified. This approach is called 'Statement Reification' and is often used for asserting metadata about statements or to represent modalities such as probability or belief. The following snippet uses addHashedIri to create a unique identifier for every reified statement: fromJson([{ id: '1', name: 'John Doe' }]), // We first create the subject term, predicate term, and object. addIri({ prefix: prefix.person, content: 'id', key: 'subject', }), addIri({ prefix: prefix.def, content: str('name'), key: 'predicate', }), addLiteral({ content: 'name', key: 'object', }), // We then create the triple statement. triple('subject', 'predicate', 'object'), // We can now create the reified statement. addHashedIri({ prefix: prefix.statement, content: ['subject', 'predicate', 'object'], key: 'statement', }), pairs('statement', [a, rdf.Statement], [rdf.subject, 'subject'], [rdf.predicate, 'predicate'], [rdf.object, 'object'], ), This results in the following linked data assertions: person:1 def:name 'John Doe'. statement:549decc4c44204a907aa32b4cc9bfaba a rdf:Statement; rdf:subject person:1; rdf:predicate def:name; rdf:object 'John Doe'. Or diagrammatically: graph TB person --- name name --> johndoe statement -- a --> Statement statement -- rdf:subject --> person statement -- rdf:predicate --> name statement -- rdf:object --> johndoe Statement[rdf:Statement]:::model person([person:1]):::data name[def:name]:::model johndoe([John Doe]):::data statement([statement:549decc4c44204a907aa32b4cc9bfaba]):::meta classDef model fill:lightblue classDef meta fill:sandybrown Function addIri() {#addIri} Description Creates an IRI based on the specified parameters. This transformation can be used in the following two ways: 1. By using an IRI prefix and a local name. 2. By using a full absolute IRI. Parameters prefix Optionally, an IRI or a key that contains an IRI. If specified, this is the IRI prefix that will appear before the local name that is specified by the content argument. If this parameter is absent, content is assumed to contain a full absolute IRI. content A string, or a key that contains a string. If the prefix parameter is specified, content specifies the IRI local name that appears after the IRI prefix. If the prefix argument is not specified, content is assumed to encode a full absolute IRI. key A new key where the created IRI is stored. See also If the created IRI is used exactly once, it is often better to use inline function iri() instead. Example: Prefix declaration and local name The following snippet creates an IRI based on the specified IRI prefix and local name: addIri({ prefix: prefix.person, content: 'username', key: '_person', }), triple('_person', a, sdo.Person), This results in the following linked data assertions: person:johndoe a sdo:Person. Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(person:johndoe):::data The following snippet makes the same assertion, but uses assertion iri() instead of transformation addIri() : triple(iri(prefix.person, 'username'), a, sdo.Person), Example: Absolute IRI The following snippet creates the same IRI, but does not use a predefined prefix IRI: addIri({ content: 'https://example.com/id/person/johndoe', key: '_person', }), triple('_person', a, sdo.Person), Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(https://example.com/id/person/johndoe):::data The following snippet uses assertion iri() instead of transformation addIri() : triple(iri('https://example.com/id/person/johndoe'), a, sdo.Person), Function addLiteral() {#addLiteral} Creates an new literal and adds it to the Record under the specified key. This transformation can be used in the following 3 ways: If a datatype (key: datatype ) is given, a typed literal is created. If a language tag (key: languageTag ) is given, a language-tagged string (datatype rdf:langString ) is created. If neither a datatype not a language tag is given, a literal with datatype xsd:string is created. When to use This transformation is typically used when: The same literal occurs in two or more statement assertions (function triple() or quad() ). This avoids having to specify the same literal multiple times using function literal() . The datatype or language tag is derived from the source data record. Parameters content A key that contains a string value, or a string specified with function str() . datatype Optionally, a key that stores an IRI or a static IRI. languageTag Optionally, a language tag from the lang object, or a key that stores such a language tag. key A new key where the created literal is stored. See also If the created literal is used exactly once, it is often better to use the inline function literal() instead. Example: Typed literal The following snippet asserts a triple with a typed literal with datatype IRI xsd:date : fromJson([{ id: '123', date: '2022-01-30' }]), addLiteral({ content: 'date', datatype: xsd.date, key: '_dateCreated', }), triple(iri(prefix.book, 'id'), sdo.dateCreated, '_dateCreated'), This makes the following linked data assertion: book:123 sdo:dateCreated '2022-30-01'^^xsd:date. Notice that the same linked data could have been asserted with the following use the the [literal} assertion middleware: fromJson([{ id: '123', date: '2022-01-30' }]), triple(iri(prefix.book, 'id'), sdo.dateCreated, literal('date', xsd.date)), Example: String literal The following snippet asserts a triple with a string literal in the object position: fromJson([{name: 'London'}]), addLiteral({ content: 'name', key: '_name', }), triple(iri(prefix.city, '_name'), skos.prefLabel, '_name') This makes the following assertion: city:London sdo:name 'London'. The literal 'London' has type xsd:string . This is the standard datatype IRI for typed literals in the linked data languages (i.e. Turtle, TriG, and SPARQL). Notice that the same linked data could have been asserted with the following snippet, where the string value 'London' is automatically cast into a string literal: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, 'name'), Example: Language-tagged string The following snippet asserts a triple with a language-tagged string in the object position: fromJson([{ name: 'London' }]), addLiteral({ content: 'name', languageTag: lang['en-gb'], key: '_name', }), triple(iri(prefix.city, 'name'), skos.prefLabel, '_name'), This results in the following linked data assertion: city:London skos:prefLabel 'London'@en-gb. Notice that the same linked data could have been asserted with the following use the the [literal} assertion middleware: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, literal('name', lang['en-gb'])), Function addRandomIri() {#addRandomIri} Creates an IRI based on the specified IRI prefix and a universally unique random identifier. When to use? This function is used under the following circumstances: You want to identify something with an IRI. The thing that you want to identify does not have a readily available identifier. If an identifier is available, use transformation addIri() instead. The thing that you want to identify does not have unique properties in the source data, or it is too difficult or too expensive to specify these properties. If unique properties are available, use transformation addHashedIri() instead. This function has the advantage that, unlike transformation addHashedIri() , no identifying criteria need to be specified. This function has the disadvantage that, unlike transformation addHashedIri() , running the same ETL twice over the same source data results in different IRIs. Parameters prefix An IRI or a key that contains an IRI. key A new key where the created IRI is stored. Example The following snippet creates a triple with a subject that that is a random IRI: addRandomIri({ prefix: prefix.id, key: 'subject', }), triple('subject', sdo.dateCreated, literal('date', xsd.date)), This makes the following assertion in linked data: id:acb3ea010fe748bfa73a2ee2b65bef65 sdo:dateCreated '2000-12-30'^^xsd:date. See also Use transformation addIri() instead, if a unique identifier can be readily specified. Use transformation addHashedIri instead, if one or more properties that together uniquely identify a thing can be specified. Use transformation addSkolemIri() instead, if you want to communicate that the IRI can be replaced with a blank node. Function addSkolemIri() {#addSkolemIri} Creates a globally unique IRI that is intended to be used as a blank node identifier. Blank nodes are nodes without identification. It relatively difficult to work which such nodes in graph data, since they cannot be identified or dereferenced online. For this reason TriplyETL uses Skolem IRIs to denote blank nodes. This allows blank nodes to be identified and dereferenced. This Skolemization approach is part of the RDF standard. Skolem IRIs are random IRIs whose root path starts with .well-known/genid/ . This makes it easy to distinguish them from other random IRIs that are not used to denote blank nodes. prefix A IRI or a key that contains an IRI whose path starts with .well-known/genid/ . key A new key where the created IRI is stored. See also Tne Skolemization section in the RDF standard explains what Skolem IRIs are and how they should be used. Example The following snippet uses a hashed IRI to create a predictable identifier for a geospatial feature, and a Skolem IRI to create an unpredictable identifier for the geometry. The snippet includes the prefix declarations to illustrate that the path of the Skolem IRI must start with .well-known/genid. . const base = 'https://example.com/' const prefix = { feature: declarePrefix(base('id/feature/')), skolem: declarePrefix(base('.well-known/genid/'), } // Etc fromJson([{ point: 'Point(1.1 2.2)' }]), addHashedIri({ prefix: prefix.feature, content: 'point', key: '_feature', }), addSkolemIri({ prefix: prefix.skolem, key: '_geometry', }), triple('_feature', geo.hasGeometry, '_geometry'), triple('_geometry', geo.asWKT, literal('point', geo.wktLiteral)), This results in the following linked data assertions: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry skolem:0cf4b63252a0476a8afc20735aa03da6. skolem:0cf4b63252a0476a8afc20735aa03da6 geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Notice that the feature IRI will be the same across ELT runs if the source data stays the same, but the Skolem will always be different. Since the Skolem IRIs can be identified by the start of their path ( .well-known/genid/ ), the same linked data assertions can be displayed as follows: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry [ geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral ]. Function addTag() {#addTag} This middleware creates a language tag based on a given string value. Parameters content A string value that encodes a language tag according to the IANA language subtag registry. key A new key where the language tag will be stored. Throws An error is emitted if the given string value does not follow the language tag format, or denotes a language tag that is not currently registered. See also The language tag format is defined in the IETF BCP 47 standard (RFC 5646) . Language tags are registered in the IANA language subtag registry . Example The following snippet created a language tag for the Dutch language as spoken in The Netherlands, and uses it to assert a language-tagged string: fromJson([{ label: 'Amsterdam' }]), addTag({ content: 'nl-nl', key: 'lang', }), triple(iri(prefix.city, 'label'), rdfs.label, literal('label', 'lang')), Function addValue() {#addValue} This middleware allows any value to be added to the Record. Description This middleware is useful for data that is not present in the source data record, but must be used in one or more assertions. Parameters content Any value that can be represented in TypeScript. key A new key where the value is stored. Example The following snippet starts out with an empty source record ( {} ), and adds a new data key to it. The added value is an array that contains a string and a number (in that order). This new value is used in the triple assertion, where 'data[0]' extracts the string element and 'data[1]' extracts the number elements. fromJson([{}]), addValue({ content: ['johndoe', 22], key: 'data', }), triple(iri(prefix.person, 'data[0]'), foaf.age, 'data[1]'), This results in the following linked data assertion: person:johndoe foaf:age 22. Example The following snippet adds a key called _startDate that either contains the start date as specified in the data source record, or the value 'unknown' : fromJson([ { id: '123', start: '2022-02-12' }, { id: '456' }, ]), ifElse({ if: 'start', then: addLiteral({ content: 'start', datatype: xsd.date, key: '_start', }), }, { else: addValue({ content: 'unknown', key: '_start', }), }), triple(iri(prefix.event, 'id'), sdo.startDate, '_start'), This results in the following linked data assertions: event:123 sdo:startDate '2022-02-12'^^xsd:date. event:456 sdo:startDate 'unknown'. Function capitalize() {#capitalize} Transforms a string value to its capitalized variant. If the first character of a string has an uppercase variant, then that variant is used. If the first character does not have an uppercase variant -- because the character is already uppercase or is a punctuation character -- then the string remains unchanged. This transformation can uppercase the first character in any language; the Unicode Default Case Conversion algorithm is used. Parameters content A key that contains a string value. key A new key where the capitalized result is stored. Example: Class IRIs According to convention, classes in linked data are denoted by IRIs whose local name starts with a capital letter. The following source data contains nice values for the type key, but they do not start with a capital letter yet. The following snippet capitalizes the values of the type keys, and uses them to create class IRIs. fromJson([ { id: '1', type: 'location' }, { id: '2', type: 'person' }, ]), capitalize({ content: 'type', key: '_type', }), triple(iri(prefix.id, 'id'), a, iri(prefix.def, '_type')), This results in the following linked data assertions: id:1 a def:Location. id:2 a def:Person. Function concat() {#concat} Description Concatenates an array of strings into one new string. An optionally specified separator is placed in between every two consecutive string values. Parameters content An array of key that contain a string and/or strings specified with assertion str() . separator Optionally, the string that is places between every two consecutive string values. key A new key where the concatenated string is stored. Example The following snippet concatenates the first and last name of a person (in that order), using a space separator. fromJson([{ id: '1', first: 'John', last: 'Doe' }]), concat({ content: ['first', 'last'], separator: ' ', key: '_name', }), triple(iri(prefix.person, 'id'), foaf.name, '_name'), This results in the following linked data assertion: person:1 foaf:name 'John Doe'. Function copy() {#copy} Makes a plain copy from the value stored in the given key to a new key. Parameters content A value of any type, or a key that contains a value of any type. type Optionally, the name of the TypeScript type of the value. The default value is 'string' . key A new key where the plain copy is stored. Example Plain copies can be used to abbreviate long keys, especially in tree-shaped data like JSON or XML. In the following example, values stored in a long nested key are copies into a short and descriptive key. This is even more useful if the key is used many times in the script. copy({ content: 'record[0].family[0].children.child[0].id.$text', key: 'childId', }), Example Since plain copies introduce a new name for an existing value, the new name can be used to store extra information about the value. The following example stores an English name, if available; or a Dutch name, if available; or no name at all. This is a relatively complex example that can only be accomplished by copying the names for the encountered languages under descriptive key names. fromJson([ { id: '1', names: [ { language: 'en', value: 'London' }, { language: 'fr', value: 'Londres' }, { language: 'nl', value: 'Londen' }, ], }, { id: '2', names: [ { language: 'fr', value: 'Paris' }, { language: 'nl', value: 'Parijs' }, ], }, ]), forEach('names', [ _switch('language', // Plain copy of the English label, if available. ['en', copy({ content: 'value', key: '$parent.en' })], // Plain copy of the Dutch label, if available. ['nl', copy({ content: 'value', key: '$parent.nl' })], ), ]), ifElse({ // Prefer an English label over a Dutch label. if: 'en', then: copy({ content: 'en', key: '_preferredName' }), }, { // If there is no English label, a Dutch label is a second-best option. if: 'nl', then: copy({ content: 'nl', key: '_preferredName' }), }), // If there is either an English or a Dutch label, assert it. when('_preferredName', [ triple(iri(prefix.city, 'id'), rdfs.label, '_preferredName'), ]), This results in the following linked data assertions: city:1 rdfs:label 'London'. city:2 rdfs:label 'Parijs'. Function encodeHtml() {#encodeHtml} Description This transformation decodes any HTML entities that appear in a given key. The following HTML entities are common in source data: HTML entity Decoded &amp; & &gt; > &lt; < You do not need to use this transformation if you want to assert literals with datatype IRI rdf:HTML . HTML entities are meaningful in HTML, so there they should be preserved. Parameters content A key in the Record that contains string values with HTML entities. key A new key where the decoded content is stored. Example The following snippet takes HTML texts from the source data and asserts them as regular text literals. Since HTML entities are meaningless in regular text, decodeHtml is used to denote these entities. fromJson([ { id: '1', label: 'A&amp;B' }, { id: '2', label: '1 &lt; 2' }, ]), decodeHtml({ content: 'label', key: '_label', }), triple(iri(prefix.id, 'id'), rdfs.label, '_label'), This results in the following linked data assertions: id:1 rdfs:label 'A&B'. id:2 rdfs:label '1 < 2'. Function geojsonToWkt() {#geojsonToWkt} Transforms GeoJSON objects to their corresponding Well-Known Text (WKT) serialization strings. Parameters content A key that stores a GeoJSON object. crs Optionally, an IRI that denotes a Coordinate Reference System (CRS). You can use IRIs from the epsg object. If absent, uses https://epsg.io/4326 as the CRS. key A new key where the WKT serialization string is stored GeoJSON and Well-Known Text (WKT) According to the GeoJSON standard , the only Coordinate Reference System (CRS) that is allowed to be used is EPSG:4326/WGS84. In practice, source data sometimes (incorrectly) stores GeoJSON formatted data in other CRSes. An example of this is the GISCO dataset of the European Union, which uses the EPSG:3857 CRS. For cases like these, the optional crs parameter comes in handy. See also The GeoJSON format is standardized in RFC 7946 . The Well-Known Text (WKT) serialization format is standardized as part of ISO/IEC 13249-3:2016 standard . Example The following snippet converts GeoJSON objects that denote traffic light locations to their GeoSPARQL representation. fromJson([ { id: '123', geometry: { type: 'Point', coordinates: [6.256, 48.480], }, }, ]), addIri({ prefix: prefix.feature, content: 'id', key: '_feature', }), geojsonToWkt({ content: 'geometry', crs: epsg[3857], key: '_wkt', }), addHashedIri({ prefix: prefix.geometry, content: '_wkt', key: '_geometry' }), pairs('_feature', [a, def.TrafficLight], [geo.hasGeometry, '_geometry'], ), pairs('_geometry', [a, geo.Geometry], [geo.asWKT, literal('_wkt', geo.wktLiteral)], ), This results in the following linked data assertions: feature:123 a def:TrafficLight; geo:hasGeometry geometry:197e6376c2bd8192c24911f88c330606. geometry:197e6376c2bd8192c24911f88c330606 a geo:Geometry; geo:asWKT 'Point(6.256 48.480)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- a --> TrafficLight feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt Geometry[geo:Geometry]:::model TrafficLight[def:TrafficLight]:::model feature(feature:123):::data geometry(geometry:197e6376c2bd8192c24911f88c330606):::data wkt(\"'Point(6.256 48.480)'^^geo:wktLiteral\"):::data Function lowercase() {#lowercase} Description Translates a string value to its lowercase variant. This middleware can lowercase strings in any language; the Unicode Default Case Conversion algorithm is used. Use cases Older data formats sometimes use uppercase letters for header names or codes. The lowercase transformation middleware may be used to change such string values to lowercase. Parameters content A key that contains a string value. key A new key where the lowercase variant is stored. Example The following snppet starts out with header values that use uppercase characters exclusively. The lowerCase transformation is used to create lowercase names that can be used to create property IRIs. fromJson([ { from: '1', rel: 'PARENT', to: '2' }, { from: '2', rel: 'CHILD', to: '1' }, ]), lowercase({ content: 'rel', key: '_relationship', }), triple( iri(prefix.id, 'from'), iri(prefix.def, '_relationship'), iri(prefix.id, 'to'), ), This results in the following linked data assertions: id:1 def:parent id:2. id:2 def:child id:1. Function padEnd() {#padEnd} Description Adds a given padding string zero or more times to the end of a string value, until the resulting string value is exactly a given number of characters long. Use cases This transformation is useful for identifiers that must have fixed length and that may be suffixed by zero's. Parameters content A key that contains a string value. If the key contains a numeric value, that value is first cast to string. padString The string that is added to the end of the string value in key content , until the result string has exactly targetLength characters. Can be a static string or a key. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the padded string is stored. Example The following snippet processes identifiers of varying length, and ensures that they have the same length after suffixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padEnd({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two Records: [ { \"id\": \"16784\", \"_id\": \"167840\" }, { \"id\": \"129\", \"_id\": \"129000\" } ] Function padStart() {#padStart} Description Adds a given padding string zero or more times in front of a string value, until the resulting string value is exactly a given number of characters long. Use cases This transformation is useful for identifiers that must have fixed length and that may be prepended by zero's. If key content contains a numeric value, then that value is first cast to string. content A key that contains a string value. padString The string that is added in front of the string value in key content , until the result string has exactly targetLength characters. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the lowercased string is stored. Example: Fixed-length identifiers The following snippet processes identifiers of varying length, and ensures that they have the same length after prefixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padStart({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two records: [ { \"id\": \"16784\", \"_id\": \"016784\" }, { \"id\": \"129\", \"_id\": \"000129\" } ] Example: Create year literals In order to create standards-conforming temporal literal, we need to pad the year component to be at least 4 decimal digits long. (This requirement is defined in the XML Schema Datatypes 1.1: Part 2 Datatypes standard.) Suppose that the source data looks as follows: Artifact Year 0001 612 0002 1702 We can ensure that all years have at least 4 decimal digits by calling the following function: padStart({ content: 'Year', padString: '0', targetLength: 4, key: '_lexicalForm', }), triple( iri(prefix.id, 'Artifact'), dct.created, literal('_lexicalForm', xsd.gYear), ), This makes the following linked data assertions: id:0001 dct:created '0612'^^xsd:gYear. id:0002 dct:created '1702'^^xsd:gYear. Function replace() {#replace} Description Performs a regular expression replacement to the given input string, and stores the result in a new key. Parameters content A key that contains a string value, or a static string specified with assertion str() . from A JavaScript Regular Expression . to Optionally, a string that replaces potential matches of the Regular Expression ( from ). Use $1 , $2 , etc. to insert matches. If absent, the empty string is used. key A new key where the result of the replacement is stored. Example Suppose the source data contains date/time strings, but only the date component is needed: { \"created\": \"2020-01-02T00:00:00.0Z\" } It is possible to extract only the date part (everything up to the T ) in the following way: replace({ content: 'created', from: /^([^T]*).*$/, to: '$1', key: '_created', }), triple('_creativeWork', dct.created, literal('_created', xsd.date)), This results in the following Record: { \"created\": \"2020-01-02T00:00:00.0Z\", \"_created\": \"2020-01-02\" } Function split() {#split} Description Splits a string into an array of strings, and stores that array in a new key. Whitespace handling This transformation removes any trailing whitespace that remains after the strings are split. This ensures that irregular use of whitespace in the source data is taken care of automatically. Use cases The transformation is used when: - Tablular source data encodes multiple values inside singular cells. (Such concatenated storage inside cells is a data quality issue, because the table format cannot guarantee that the separator character does not (accidentally) occur inside individual values inside a cell. Tree-shaped source formats are able to store multiple values for the same key reliably, e.g. JSON and XML.) - Source data contains complex string values that can be decomposed into stand-alone components with distinct meaning. Parameters content A key that stores a string, or a string specified with assertion str() . separator A string or a regular expression that is used to separate the content. key A new key where the array of splitted strings is stored. Example: Multiple values in singular table cells Tabular formats are unable to store more than one value in a cell. Because of this limitation, tabular data sources sometimes encode multiple values in cells by encoding them into one string. A separator character is typically used to distinguish between these multiple values. Suppose that the source data looks as follows: Parent Child John Jane, Jake , Kate ,, The following transformation splits the cells that encode zero or more children for each parent: split({ content: 'Child', separator: ',', key: 'Children', }), This results in the following transformed records: { \"Parent\": \"John\", \"Child\": \"Jane, Jake , \", \"Children\": [ \"Jane\", \"Jake\" ] } { \"Parent\": \"Kate\", \"Child\": \",, \", \"Children\": [] } Notice that trailing whitespace and empty values are dealt with automatically. Since the split() transformation always results in an array of strings, we can use the term assertion iris() afterwards: split({ content: 'children', separator: ',', key: '_children', }), triple( iri(prefix.person, 'parent'), sdo.children, iris(prefix.person, '_children') ), This results in the following linked data assertions: person:johndoe sdo:children person:janedoe, person:jakedoe. Example: Split a complex string into components The following snippet uses a regular expression to split a KIX code. (A KIX code is a standardized format for representing postal addresses in The Netherlands.) fromJson([{ id: '1', KIX: '1231FZ13Xhs' }]), split({ content: 'KIX', separator: /^(\\d{4}[A-Z]{2})(\\d{1,5})(?:X(.{1,6}))/, key: 'KIX_components', }), triple(iri(prefix.id, 'id'), sdo.postalCode, 'KIX_components[1]'), This results in the following record: { \"id\": \"1\", \"KIX\": \"1231FZ13Xhs\", \"KIX_components\": [\"\", \"1231FZ\", \"13\", \"hs\", \"\"] } And in the following linked data assertion: id:1 sdo:postalCode '1231FZ'. Function substring() {#substring} Description This middleware takes a substring from the input string and stores the result in a new key. Parameters content A key that stores a string value, or a string specified with assertion str() . start The index of the first character that is included in the substring. The first character has index 0. end Optionally, the index of the first character that is excluded from the substring. If absent, the substring ends at the end of the source string. key The new key in which the substring is stored. Example The Library of Congress MARC format stores the type of record in the sixth character that appears in the leader key. We use substring() to extract this character, and then use transformation translateAll() to map them to a corresponding class IRI: substring({ content: 'metadata.marc:record.marc:leader.$text', start: 6, end: 7, key: '_typeOfRecord', }), translateAll({ content: '_typeOfRecord', table: { a: dcm.Text, k: dcm.StillImage, }, key: '_class', }), triple('_iri', a, '_class'), Function translateAll() {#translateAll} Description Translates all dynamic strings from a specific key to new values of an arbitrary type To , according to a specified translation table. Since this function translates all values, the mapped values can have any type T ; they do not need to be strings. For example, this allows strings to be translated to IRIs or to literals. When to use? This approach is used when: The set of source data values is small. The set of source data values is known ahead of time. The corresponding linked data terms are known ahead of time. The appearance of a new value is considered to be an error in the source data. Parameters content A key that contains a string value. table A translation table from strings to values of some arbitrary type T . nulls Optionally, a list of string values that are considered denote NULL values in the source data. When a NULL value is encountered, the special value undefined is added for the target key . default Optionally, a default value or a default value-determining function that is used for string values that are neither in the translation table ( table ) nor in the NULL values list ( nulls ). The function must return a value of type T . Use of a default value value is equivalent to using the following value-determining function: _ => value . key A new key where the results of the translation are stored. Example: Map source data to IRI values Suppose that source data contains country names. In linked data we want to use IRIs to denote countries, so that we can link additional information. Since the list of countries that appears in the source data is not that long, we can specify a translation table from names to IRIs by hand: change.translateAll({ content: 'country', table: { 'Belgium': country.be, 'Germany': country.de, 'England': country.gb, ..., }, nulls: ['Unknown'], key: '_country', }), when('country', [ triple('_country', a, sdo.Country), ]), Example: Map source data to IRI properties When we relate a creative work to its creator, we sometimes know whether the creator was the actor, architect, author, etc. of the creative work. But in other cases we only know that there is a generic creator relationship. The Library of Congress Relators vocabulary allows us to express specific and generic predicates of this kind. transform.translateAll({ table: { 'actor': rel.act, 'architect': rel.arc, 'author': rel.aut, ..., }, default: rel.oth, // generic relator key: '_relator', }), triple('_creativeWork', '_relator', '_creator'), Function translateSome() {#translateSome} Description Translates some strings, according to the specified translation table, to other strings. Strings that are not translated according to the translation table are copied over as-is. Parameters content A key that contains a string value. table A translation table that specifies translations from strings to strings. key A new key where the translated strings are stored. Use cases Source data often contains some strings that are correct and some that are incorrect. For example, if source data contains a key with city names, some of the names may be misspelled. In such cases, translateSome() can be used to translate the incorrect strings into correct ones. A translateSome() transformation is often performed directly before a translateAll() transformation. The former ensures that all string values are correct (e.g. fixing typo's in city names); the latter ensures that all strings are mapped onto IRIs (e.g. city names mapped onto city-denoting IRIs). Example The following example fixes an encoding issue that occurs in the source data: transform.translateSome({ content: 'name', table: { 'Frysl\ufffd\ufffdn': 'Frysl\u00e2n', // Other entries for typographic fixes go here. ..., }, key: '_name', }), Function tryLiteral() {#tryLiteral} Description This transformation is used when string values must be mapped onto literals with varying datatype IRIs. The datatype IRIs that could apply are specified in a list. The specified datatype IRIs are tried out from left to right. The first datatype IRI that results in a valid literal is chosen. content A key that contains a string value, or a string value specified with assertion str() . datatypes An array of two or more datatype IRIs. key A new key where the created literal is stored. Throws An exception is emitted if a string value does not belong to the lexical space of any of the specified datatype IRIs. Example A literal is valid if the given string value appears in the lexical space of a specific datatype IRI. This is best explained with an example: tryLiteral({ content: 'date', datatypes: [xsd.date, xsd.gYearMonth, xsd.gYear], key: '_publicationDate', }), Source data in key 'date' Result in key '_date' '1900-01-02' '1900-01-02'^^xsd:date '1900' '1900'^^xsd:gYear '02-01-1900' An error is emitted. If we do not want to emit errors for string values that cannot be satisfy any of the specified datatype IRIs, we may choose to include xsd.string as the last datatype IRI in the list. Do notice however that this will result in dates that cannot be compared on a timeline, since they were not transformed to an XSD date/time datatype. See also You only need to use tryLiteral() if the datatype IRI varies from record to record. If the datatype IRI is the same for every record, then the regular assertion function literal() should be used instead. Function uppercase() {#uppercase} Description Translates a string value to its uppercase variant. This middleware can uppercase strings in any language; the Unicode Default Case Conversion algorithm is used for this. Parameters content A key that contains a string value. key A new key where the uppercase variant is stored. Example We do not have a good example for this transformation middleware yet. Let us know in case you have a good example!","title":"2. Transform: RATT"},{"location":"triply-etl/transform/ratt/#overview","text":"The following transformation functions are currently available: Function Description addHashedIri() Create a new IRI with a content-based local name. addIri() Create a new IRI based on a prefix and a local name. addLiteral() Create a new literal based on a lexical for and a datatype IRI or language tag. addRandomIri() Create a new IRI with a random local name. addSkolemIri() Create a new IRI with a random local name, which advertises that it may be consistently replaced with blank nodes. addTag() Create a language tag. addValue() Create a TypeScript value. capitalize() Transforms a string value to its capitalized variant. concat() Combine multiple strings into a new string. copy() Copy a value from an old into a new key. decodeHtml() Decode HTML entities that occur in strings. geojsonToWkt() Change GeoJSON strings to WKT strings. lowercase() Change strings to their lowercase variants. padEnd() Pad the end of strings. padStart() Pad the start of strings. replace() Replace part of a string. split() Split a string into multiple substrings. substring() Extract a substring from a string. translateAll() Translate all string values to other values. translateSome() Translate some string values to other strings. tryLiteral() Create literals for which the datatype is not know beforehand. uppercase() Change a string to its uppercase variant. wkt.addPoint() Add a geospatial point using the Well-Known Text (WKT) format. wkt.project() Change the projection of a Well-Known Text (WKT) literal from from Coordinate Reference System into another.","title":"Overview"},{"location":"triply-etl/transform/ratt/#function-addhashediri-addhashediri","text":"Creates an IRI based on the specified IRI prefix and the hash calculated over the input content string(s).","title":"Function addHashedIri() {#addHashedIri}"},{"location":"triply-etl/transform/ratt/#parameters","text":"prefix An IRI, or a key that contains an IRI value. content A key that contains a string value, or a string value specified with function str() . key A new key where the created hashed IRI is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#use-cases","text":"This function is used under the following circumstances: 1. You want to identify something with an IRI. 2. The thing that you want to identify does not have a readily available identifier. 3. The thing that you want to identify has one or more properties that together allow the thing to be uniquely identified. A benefit of addHashedIri () is that the created IRIs are the same across different ETL runs over the same source data. A downside of addHashedIri() is that it can take a lot of time to figure out which set of properties make every IRI unique. (In database theory this process is known as 'composite key detection'.) Furthermore, having to adjust the hashed content later may pose a maintenance burden.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#example-lazy-identifiers","text":"Some source data does not include good identifiers for all data items. The following source table does not include a good identifier. What is more, none of the columns contains values that are unique for each row: First name Last name Jane Doe Jane Smith John Doe In such cases it may be an option to take a combination of columns, and use that combined sequence of columns for identification. This is called a 'composite key' in database theory. The following snippet uses the combination of the first name and last name fields (in that order) to create a locally unique hash, that can be used to create a globally IRI. (This does assume that every person in the dataset has a unique first/last name combination!) fromXlsx(Source.file('data.xlsx')), addHashedIri({ prefix: prefix.person, content: ['First name', 'Last name'], key: '_person', }), pairs('_person', [a, sdo.Person], [sdo.givenName, 'First name'], [sdo.familyName, 'Last name'], ), This results in the following linked data assertions: person:70020947bea6c39cccea20d27e30fbdf a sdo:Person; sdo:givenName 'John'; familyName 'Doe'. Or diagrammatically: graph LR person -- a --> Person person -- sdo:givenName --> john person -- sdo:familyName --> doe Person[sdo:Person]:::model doe['Doe']:::data john['John']:::data person([person:70020947bea6c39cccea20d27e30fbdf]):::data classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: Lazy identifiers"},{"location":"triply-etl/transform/ratt/#example-statement-reification","text":"The RDF standard allows individual statements to be identified. This approach is called 'Statement Reification' and is often used for asserting metadata about statements or to represent modalities such as probability or belief. The following snippet uses addHashedIri to create a unique identifier for every reified statement: fromJson([{ id: '1', name: 'John Doe' }]), // We first create the subject term, predicate term, and object. addIri({ prefix: prefix.person, content: 'id', key: 'subject', }), addIri({ prefix: prefix.def, content: str('name'), key: 'predicate', }), addLiteral({ content: 'name', key: 'object', }), // We then create the triple statement. triple('subject', 'predicate', 'object'), // We can now create the reified statement. addHashedIri({ prefix: prefix.statement, content: ['subject', 'predicate', 'object'], key: 'statement', }), pairs('statement', [a, rdf.Statement], [rdf.subject, 'subject'], [rdf.predicate, 'predicate'], [rdf.object, 'object'], ), This results in the following linked data assertions: person:1 def:name 'John Doe'. statement:549decc4c44204a907aa32b4cc9bfaba a rdf:Statement; rdf:subject person:1; rdf:predicate def:name; rdf:object 'John Doe'. Or diagrammatically: graph TB person --- name name --> johndoe statement -- a --> Statement statement -- rdf:subject --> person statement -- rdf:predicate --> name statement -- rdf:object --> johndoe Statement[rdf:Statement]:::model person([person:1]):::data name[def:name]:::model johndoe([John Doe]):::data statement([statement:549decc4c44204a907aa32b4cc9bfaba]):::meta classDef model fill:lightblue classDef meta fill:sandybrown","title":"Example: Statement Reification"},{"location":"triply-etl/transform/ratt/#function-addiri-addiri","text":"","title":"Function addIri() {#addIri}"},{"location":"triply-etl/transform/ratt/#description","text":"Creates an IRI based on the specified parameters. This transformation can be used in the following two ways: 1. By using an IRI prefix and a local name. 2. By using a full absolute IRI.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_1","text":"prefix Optionally, an IRI or a key that contains an IRI. If specified, this is the IRI prefix that will appear before the local name that is specified by the content argument. If this parameter is absent, content is assumed to contain a full absolute IRI. content A string, or a key that contains a string. If the prefix parameter is specified, content specifies the IRI local name that appears after the IRI prefix. If the prefix argument is not specified, content is assumed to encode a full absolute IRI. key A new key where the created IRI is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#see-also","text":"If the created IRI is used exactly once, it is often better to use inline function iri() instead.","title":"See also"},{"location":"triply-etl/transform/ratt/#example-prefix-declaration-and-local-name","text":"The following snippet creates an IRI based on the specified IRI prefix and local name: addIri({ prefix: prefix.person, content: 'username', key: '_person', }), triple('_person', a, sdo.Person), This results in the following linked data assertions: person:johndoe a sdo:Person. Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(person:johndoe):::data The following snippet makes the same assertion, but uses assertion iri() instead of transformation addIri() : triple(iri(prefix.person, 'username'), a, sdo.Person),","title":"Example: Prefix declaration and local name"},{"location":"triply-etl/transform/ratt/#example-absolute-iri","text":"The following snippet creates the same IRI, but does not use a predefined prefix IRI: addIri({ content: 'https://example.com/id/person/johndoe', key: '_person', }), triple('_person', a, sdo.Person), Or diagrammatically: graph LR johndoe -- a --> Person Person[sdo:Person]:::model johndoe(https://example.com/id/person/johndoe):::data The following snippet uses assertion iri() instead of transformation addIri() : triple(iri('https://example.com/id/person/johndoe'), a, sdo.Person),","title":"Example: Absolute IRI"},{"location":"triply-etl/transform/ratt/#function-addliteral-addliteral","text":"Creates an new literal and adds it to the Record under the specified key. This transformation can be used in the following 3 ways: If a datatype (key: datatype ) is given, a typed literal is created. If a language tag (key: languageTag ) is given, a language-tagged string (datatype rdf:langString ) is created. If neither a datatype not a language tag is given, a literal with datatype xsd:string is created.","title":"Function addLiteral() {#addLiteral}"},{"location":"triply-etl/transform/ratt/#when-to-use","text":"This transformation is typically used when: The same literal occurs in two or more statement assertions (function triple() or quad() ). This avoids having to specify the same literal multiple times using function literal() . The datatype or language tag is derived from the source data record.","title":"When to use"},{"location":"triply-etl/transform/ratt/#parameters_2","text":"content A key that contains a string value, or a string specified with function str() . datatype Optionally, a key that stores an IRI or a static IRI. languageTag Optionally, a language tag from the lang object, or a key that stores such a language tag. key A new key where the created literal is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#see-also_1","text":"If the created literal is used exactly once, it is often better to use the inline function literal() instead.","title":"See also"},{"location":"triply-etl/transform/ratt/#example-typed-literal","text":"The following snippet asserts a triple with a typed literal with datatype IRI xsd:date : fromJson([{ id: '123', date: '2022-01-30' }]), addLiteral({ content: 'date', datatype: xsd.date, key: '_dateCreated', }), triple(iri(prefix.book, 'id'), sdo.dateCreated, '_dateCreated'), This makes the following linked data assertion: book:123 sdo:dateCreated '2022-30-01'^^xsd:date. Notice that the same linked data could have been asserted with the following use the the [literal} assertion middleware: fromJson([{ id: '123', date: '2022-01-30' }]), triple(iri(prefix.book, 'id'), sdo.dateCreated, literal('date', xsd.date)),","title":"Example: Typed literal"},{"location":"triply-etl/transform/ratt/#example-string-literal","text":"The following snippet asserts a triple with a string literal in the object position: fromJson([{name: 'London'}]), addLiteral({ content: 'name', key: '_name', }), triple(iri(prefix.city, '_name'), skos.prefLabel, '_name') This makes the following assertion: city:London sdo:name 'London'. The literal 'London' has type xsd:string . This is the standard datatype IRI for typed literals in the linked data languages (i.e. Turtle, TriG, and SPARQL). Notice that the same linked data could have been asserted with the following snippet, where the string value 'London' is automatically cast into a string literal: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, 'name'),","title":"Example: String literal"},{"location":"triply-etl/transform/ratt/#example-language-tagged-string","text":"The following snippet asserts a triple with a language-tagged string in the object position: fromJson([{ name: 'London' }]), addLiteral({ content: 'name', languageTag: lang['en-gb'], key: '_name', }), triple(iri(prefix.city, 'name'), skos.prefLabel, '_name'), This results in the following linked data assertion: city:London skos:prefLabel 'London'@en-gb. Notice that the same linked data could have been asserted with the following use the the [literal} assertion middleware: fromJson([{ name: 'London' }]), triple(iri(prefix.city, 'name'), skos.prefLabel, literal('name', lang['en-gb'])),","title":"Example: Language-tagged string"},{"location":"triply-etl/transform/ratt/#function-addrandomiri-addrandomiri","text":"Creates an IRI based on the specified IRI prefix and a universally unique random identifier.","title":"Function addRandomIri() {#addRandomIri}"},{"location":"triply-etl/transform/ratt/#when-to-use_1","text":"This function is used under the following circumstances: You want to identify something with an IRI. The thing that you want to identify does not have a readily available identifier. If an identifier is available, use transformation addIri() instead. The thing that you want to identify does not have unique properties in the source data, or it is too difficult or too expensive to specify these properties. If unique properties are available, use transformation addHashedIri() instead. This function has the advantage that, unlike transformation addHashedIri() , no identifying criteria need to be specified. This function has the disadvantage that, unlike transformation addHashedIri() , running the same ETL twice over the same source data results in different IRIs.","title":"When to use?"},{"location":"triply-etl/transform/ratt/#parameters_3","text":"prefix An IRI or a key that contains an IRI. key A new key where the created IRI is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example","text":"The following snippet creates a triple with a subject that that is a random IRI: addRandomIri({ prefix: prefix.id, key: 'subject', }), triple('subject', sdo.dateCreated, literal('date', xsd.date)), This makes the following assertion in linked data: id:acb3ea010fe748bfa73a2ee2b65bef65 sdo:dateCreated '2000-12-30'^^xsd:date.","title":"Example"},{"location":"triply-etl/transform/ratt/#see-also_2","text":"Use transformation addIri() instead, if a unique identifier can be readily specified. Use transformation addHashedIri instead, if one or more properties that together uniquely identify a thing can be specified. Use transformation addSkolemIri() instead, if you want to communicate that the IRI can be replaced with a blank node.","title":"See also"},{"location":"triply-etl/transform/ratt/#function-addskolemiri-addskolemiri","text":"Creates a globally unique IRI that is intended to be used as a blank node identifier. Blank nodes are nodes without identification. It relatively difficult to work which such nodes in graph data, since they cannot be identified or dereferenced online. For this reason TriplyETL uses Skolem IRIs to denote blank nodes. This allows blank nodes to be identified and dereferenced. This Skolemization approach is part of the RDF standard. Skolem IRIs are random IRIs whose root path starts with .well-known/genid/ . This makes it easy to distinguish them from other random IRIs that are not used to denote blank nodes. prefix A IRI or a key that contains an IRI whose path starts with .well-known/genid/ . key A new key where the created IRI is stored.","title":"Function addSkolemIri() {#addSkolemIri}"},{"location":"triply-etl/transform/ratt/#see-also_3","text":"Tne Skolemization section in the RDF standard explains what Skolem IRIs are and how they should be used.","title":"See also"},{"location":"triply-etl/transform/ratt/#example_1","text":"The following snippet uses a hashed IRI to create a predictable identifier for a geospatial feature, and a Skolem IRI to create an unpredictable identifier for the geometry. The snippet includes the prefix declarations to illustrate that the path of the Skolem IRI must start with .well-known/genid. . const base = 'https://example.com/' const prefix = { feature: declarePrefix(base('id/feature/')), skolem: declarePrefix(base('.well-known/genid/'), } // Etc fromJson([{ point: 'Point(1.1 2.2)' }]), addHashedIri({ prefix: prefix.feature, content: 'point', key: '_feature', }), addSkolemIri({ prefix: prefix.skolem, key: '_geometry', }), triple('_feature', geo.hasGeometry, '_geometry'), triple('_geometry', geo.asWKT, literal('point', geo.wktLiteral)), This results in the following linked data assertions: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry skolem:0cf4b63252a0476a8afc20735aa03da6. skolem:0cf4b63252a0476a8afc20735aa03da6 geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral. Notice that the feature IRI will be the same across ELT runs if the source data stays the same, but the Skolem will always be different. Since the Skolem IRIs can be identified by the start of their path ( .well-known/genid/ ), the same linked data assertions can be displayed as follows: feature:22238008e490f725979118f8f2dd9b5a geo:hasGeometry [ geo:asWKT 'Point(1.1 2.2)'^^geo:wktLiteral ].","title":"Example"},{"location":"triply-etl/transform/ratt/#function-addtag-addtag","text":"This middleware creates a language tag based on a given string value.","title":"Function addTag() {#addTag}"},{"location":"triply-etl/transform/ratt/#parameters_4","text":"content A string value that encodes a language tag according to the IANA language subtag registry. key A new key where the language tag will be stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#throws","text":"An error is emitted if the given string value does not follow the language tag format, or denotes a language tag that is not currently registered.","title":"Throws"},{"location":"triply-etl/transform/ratt/#see-also_4","text":"The language tag format is defined in the IETF BCP 47 standard (RFC 5646) . Language tags are registered in the IANA language subtag registry .","title":"See also"},{"location":"triply-etl/transform/ratt/#example_2","text":"The following snippet created a language tag for the Dutch language as spoken in The Netherlands, and uses it to assert a language-tagged string: fromJson([{ label: 'Amsterdam' }]), addTag({ content: 'nl-nl', key: 'lang', }), triple(iri(prefix.city, 'label'), rdfs.label, literal('label', 'lang')),","title":"Example"},{"location":"triply-etl/transform/ratt/#function-addvalue-addvalue","text":"This middleware allows any value to be added to the Record.","title":"Function addValue() {#addValue}"},{"location":"triply-etl/transform/ratt/#description_1","text":"This middleware is useful for data that is not present in the source data record, but must be used in one or more assertions.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_5","text":"content Any value that can be represented in TypeScript. key A new key where the value is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_3","text":"The following snippet starts out with an empty source record ( {} ), and adds a new data key to it. The added value is an array that contains a string and a number (in that order). This new value is used in the triple assertion, where 'data[0]' extracts the string element and 'data[1]' extracts the number elements. fromJson([{}]), addValue({ content: ['johndoe', 22], key: 'data', }), triple(iri(prefix.person, 'data[0]'), foaf.age, 'data[1]'), This results in the following linked data assertion: person:johndoe foaf:age 22.","title":"Example"},{"location":"triply-etl/transform/ratt/#example_4","text":"The following snippet adds a key called _startDate that either contains the start date as specified in the data source record, or the value 'unknown' : fromJson([ { id: '123', start: '2022-02-12' }, { id: '456' }, ]), ifElse({ if: 'start', then: addLiteral({ content: 'start', datatype: xsd.date, key: '_start', }), }, { else: addValue({ content: 'unknown', key: '_start', }), }), triple(iri(prefix.event, 'id'), sdo.startDate, '_start'), This results in the following linked data assertions: event:123 sdo:startDate '2022-02-12'^^xsd:date. event:456 sdo:startDate 'unknown'.","title":"Example"},{"location":"triply-etl/transform/ratt/#function-capitalize-capitalize","text":"Transforms a string value to its capitalized variant. If the first character of a string has an uppercase variant, then that variant is used. If the first character does not have an uppercase variant -- because the character is already uppercase or is a punctuation character -- then the string remains unchanged. This transformation can uppercase the first character in any language; the Unicode Default Case Conversion algorithm is used.","title":"Function capitalize() {#capitalize}"},{"location":"triply-etl/transform/ratt/#parameters_6","text":"content A key that contains a string value. key A new key where the capitalized result is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example-class-iris","text":"According to convention, classes in linked data are denoted by IRIs whose local name starts with a capital letter. The following source data contains nice values for the type key, but they do not start with a capital letter yet. The following snippet capitalizes the values of the type keys, and uses them to create class IRIs. fromJson([ { id: '1', type: 'location' }, { id: '2', type: 'person' }, ]), capitalize({ content: 'type', key: '_type', }), triple(iri(prefix.id, 'id'), a, iri(prefix.def, '_type')), This results in the following linked data assertions: id:1 a def:Location. id:2 a def:Person.","title":"Example: Class IRIs"},{"location":"triply-etl/transform/ratt/#function-concat-concat","text":"","title":"Function concat() {#concat}"},{"location":"triply-etl/transform/ratt/#description_2","text":"Concatenates an array of strings into one new string. An optionally specified separator is placed in between every two consecutive string values.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_7","text":"content An array of key that contain a string and/or strings specified with assertion str() . separator Optionally, the string that is places between every two consecutive string values. key A new key where the concatenated string is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_5","text":"The following snippet concatenates the first and last name of a person (in that order), using a space separator. fromJson([{ id: '1', first: 'John', last: 'Doe' }]), concat({ content: ['first', 'last'], separator: ' ', key: '_name', }), triple(iri(prefix.person, 'id'), foaf.name, '_name'), This results in the following linked data assertion: person:1 foaf:name 'John Doe'.","title":"Example"},{"location":"triply-etl/transform/ratt/#function-copy-copy","text":"Makes a plain copy from the value stored in the given key to a new key.","title":"Function copy() {#copy}"},{"location":"triply-etl/transform/ratt/#parameters_8","text":"content A value of any type, or a key that contains a value of any type. type Optionally, the name of the TypeScript type of the value. The default value is 'string' . key A new key where the plain copy is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_6","text":"Plain copies can be used to abbreviate long keys, especially in tree-shaped data like JSON or XML. In the following example, values stored in a long nested key are copies into a short and descriptive key. This is even more useful if the key is used many times in the script. copy({ content: 'record[0].family[0].children.child[0].id.$text', key: 'childId', }),","title":"Example"},{"location":"triply-etl/transform/ratt/#example_7","text":"Since plain copies introduce a new name for an existing value, the new name can be used to store extra information about the value. The following example stores an English name, if available; or a Dutch name, if available; or no name at all. This is a relatively complex example that can only be accomplished by copying the names for the encountered languages under descriptive key names. fromJson([ { id: '1', names: [ { language: 'en', value: 'London' }, { language: 'fr', value: 'Londres' }, { language: 'nl', value: 'Londen' }, ], }, { id: '2', names: [ { language: 'fr', value: 'Paris' }, { language: 'nl', value: 'Parijs' }, ], }, ]), forEach('names', [ _switch('language', // Plain copy of the English label, if available. ['en', copy({ content: 'value', key: '$parent.en' })], // Plain copy of the Dutch label, if available. ['nl', copy({ content: 'value', key: '$parent.nl' })], ), ]), ifElse({ // Prefer an English label over a Dutch label. if: 'en', then: copy({ content: 'en', key: '_preferredName' }), }, { // If there is no English label, a Dutch label is a second-best option. if: 'nl', then: copy({ content: 'nl', key: '_preferredName' }), }), // If there is either an English or a Dutch label, assert it. when('_preferredName', [ triple(iri(prefix.city, 'id'), rdfs.label, '_preferredName'), ]), This results in the following linked data assertions: city:1 rdfs:label 'London'. city:2 rdfs:label 'Parijs'.","title":"Example"},{"location":"triply-etl/transform/ratt/#function-encodehtml-encodehtml","text":"","title":"Function encodeHtml() {#encodeHtml}"},{"location":"triply-etl/transform/ratt/#description_3","text":"This transformation decodes any HTML entities that appear in a given key. The following HTML entities are common in source data: HTML entity Decoded &amp; & &gt; > &lt; < You do not need to use this transformation if you want to assert literals with datatype IRI rdf:HTML . HTML entities are meaningful in HTML, so there they should be preserved.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_9","text":"content A key in the Record that contains string values with HTML entities. key A new key where the decoded content is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_8","text":"The following snippet takes HTML texts from the source data and asserts them as regular text literals. Since HTML entities are meaningless in regular text, decodeHtml is used to denote these entities. fromJson([ { id: '1', label: 'A&amp;B' }, { id: '2', label: '1 &lt; 2' }, ]), decodeHtml({ content: 'label', key: '_label', }), triple(iri(prefix.id, 'id'), rdfs.label, '_label'), This results in the following linked data assertions: id:1 rdfs:label 'A&B'. id:2 rdfs:label '1 < 2'.","title":"Example"},{"location":"triply-etl/transform/ratt/#function-geojsontowkt-geojsontowkt","text":"Transforms GeoJSON objects to their corresponding Well-Known Text (WKT) serialization strings.","title":"Function geojsonToWkt() {#geojsonToWkt}"},{"location":"triply-etl/transform/ratt/#parameters_10","text":"content A key that stores a GeoJSON object. crs Optionally, an IRI that denotes a Coordinate Reference System (CRS). You can use IRIs from the epsg object. If absent, uses https://epsg.io/4326 as the CRS. key A new key where the WKT serialization string is stored","title":"Parameters"},{"location":"triply-etl/transform/ratt/#geojson-and-well-known-text-wkt","text":"According to the GeoJSON standard , the only Coordinate Reference System (CRS) that is allowed to be used is EPSG:4326/WGS84. In practice, source data sometimes (incorrectly) stores GeoJSON formatted data in other CRSes. An example of this is the GISCO dataset of the European Union, which uses the EPSG:3857 CRS. For cases like these, the optional crs parameter comes in handy.","title":"GeoJSON and Well-Known Text (WKT)"},{"location":"triply-etl/transform/ratt/#see-also_5","text":"The GeoJSON format is standardized in RFC 7946 . The Well-Known Text (WKT) serialization format is standardized as part of ISO/IEC 13249-3:2016 standard .","title":"See also"},{"location":"triply-etl/transform/ratt/#example_9","text":"The following snippet converts GeoJSON objects that denote traffic light locations to their GeoSPARQL representation. fromJson([ { id: '123', geometry: { type: 'Point', coordinates: [6.256, 48.480], }, }, ]), addIri({ prefix: prefix.feature, content: 'id', key: '_feature', }), geojsonToWkt({ content: 'geometry', crs: epsg[3857], key: '_wkt', }), addHashedIri({ prefix: prefix.geometry, content: '_wkt', key: '_geometry' }), pairs('_feature', [a, def.TrafficLight], [geo.hasGeometry, '_geometry'], ), pairs('_geometry', [a, geo.Geometry], [geo.asWKT, literal('_wkt', geo.wktLiteral)], ), This results in the following linked data assertions: feature:123 a def:TrafficLight; geo:hasGeometry geometry:197e6376c2bd8192c24911f88c330606. geometry:197e6376c2bd8192c24911f88c330606 a geo:Geometry; geo:asWKT 'Point(6.256 48.480)'^^geo:wktLiteral. Or diagrammatically: graph LR feature -- a --> TrafficLight feature -- geo:hasGeometry --> geometry geometry -- a --> Geometry geometry -- geo:asWKT --> wkt Geometry[geo:Geometry]:::model TrafficLight[def:TrafficLight]:::model feature(feature:123):::data geometry(geometry:197e6376c2bd8192c24911f88c330606):::data wkt(\"'Point(6.256 48.480)'^^geo:wktLiteral\"):::data","title":"Example"},{"location":"triply-etl/transform/ratt/#function-lowercase-lowercase","text":"","title":"Function lowercase() {#lowercase}"},{"location":"triply-etl/transform/ratt/#description_4","text":"Translates a string value to its lowercase variant. This middleware can lowercase strings in any language; the Unicode Default Case Conversion algorithm is used.","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases_1","text":"Older data formats sometimes use uppercase letters for header names or codes. The lowercase transformation middleware may be used to change such string values to lowercase.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_11","text":"content A key that contains a string value. key A new key where the lowercase variant is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_10","text":"The following snppet starts out with header values that use uppercase characters exclusively. The lowerCase transformation is used to create lowercase names that can be used to create property IRIs. fromJson([ { from: '1', rel: 'PARENT', to: '2' }, { from: '2', rel: 'CHILD', to: '1' }, ]), lowercase({ content: 'rel', key: '_relationship', }), triple( iri(prefix.id, 'from'), iri(prefix.def, '_relationship'), iri(prefix.id, 'to'), ), This results in the following linked data assertions: id:1 def:parent id:2. id:2 def:child id:1.","title":"Example"},{"location":"triply-etl/transform/ratt/#function-padend-padend","text":"","title":"Function padEnd() {#padEnd}"},{"location":"triply-etl/transform/ratt/#description_5","text":"Adds a given padding string zero or more times to the end of a string value, until the resulting string value is exactly a given number of characters long.","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases_2","text":"This transformation is useful for identifiers that must have fixed length and that may be suffixed by zero's.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_12","text":"content A key that contains a string value. If the key contains a numeric value, that value is first cast to string. padString The string that is added to the end of the string value in key content , until the result string has exactly targetLength characters. Can be a static string or a key. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the padded string is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_11","text":"The following snippet processes identifiers of varying length, and ensures that they have the same length after suffixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padEnd({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two Records: [ { \"id\": \"16784\", \"_id\": \"167840\" }, { \"id\": \"129\", \"_id\": \"129000\" } ]","title":"Example"},{"location":"triply-etl/transform/ratt/#function-padstart-padstart","text":"","title":"Function padStart() {#padStart}"},{"location":"triply-etl/transform/ratt/#description_6","text":"Adds a given padding string zero or more times in front of a string value, until the resulting string value is exactly a given number of characters long.","title":"Description"},{"location":"triply-etl/transform/ratt/#use-cases_3","text":"This transformation is useful for identifiers that must have fixed length and that may be prepended by zero's. If key content contains a numeric value, then that value is first cast to string. content A key that contains a string value. padString The string that is added in front of the string value in key content , until the result string has exactly targetLength characters. targetLength The exact number of characters that the resulting string should have. The string value is copied over as-is when targetLength is smaller than or equal to the length of the string value in key content . This includes cases where targetLength is negative or zero. key A new key where the lowercased string is stored.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#example-fixed-length-identifiers","text":"The following snippet processes identifiers of varying length, and ensures that they have the same length after prefixing '0' characters. fromJson([ { id: '16784' }, { id: '129' }, ]), padStart({ content: 'id', padString: '0', targetLength: 6, key: '_id', }), This results in the following two records: [ { \"id\": \"16784\", \"_id\": \"016784\" }, { \"id\": \"129\", \"_id\": \"000129\" } ]","title":"Example: Fixed-length identifiers"},{"location":"triply-etl/transform/ratt/#example-create-year-literals","text":"In order to create standards-conforming temporal literal, we need to pad the year component to be at least 4 decimal digits long. (This requirement is defined in the XML Schema Datatypes 1.1: Part 2 Datatypes standard.) Suppose that the source data looks as follows: Artifact Year 0001 612 0002 1702 We can ensure that all years have at least 4 decimal digits by calling the following function: padStart({ content: 'Year', padString: '0', targetLength: 4, key: '_lexicalForm', }), triple( iri(prefix.id, 'Artifact'), dct.created, literal('_lexicalForm', xsd.gYear), ), This makes the following linked data assertions: id:0001 dct:created '0612'^^xsd:gYear. id:0002 dct:created '1702'^^xsd:gYear.","title":"Example: Create year literals"},{"location":"triply-etl/transform/ratt/#function-replace-replace","text":"","title":"Function replace() {#replace}"},{"location":"triply-etl/transform/ratt/#description_7","text":"Performs a regular expression replacement to the given input string, and stores the result in a new key.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_13","text":"content A key that contains a string value, or a static string specified with assertion str() . from A JavaScript Regular Expression . to Optionally, a string that replaces potential matches of the Regular Expression ( from ). Use $1 , $2 , etc. to insert matches. If absent, the empty string is used. key A new key where the result of the replacement is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_12","text":"Suppose the source data contains date/time strings, but only the date component is needed: { \"created\": \"2020-01-02T00:00:00.0Z\" } It is possible to extract only the date part (everything up to the T ) in the following way: replace({ content: 'created', from: /^([^T]*).*$/, to: '$1', key: '_created', }), triple('_creativeWork', dct.created, literal('_created', xsd.date)), This results in the following Record: { \"created\": \"2020-01-02T00:00:00.0Z\", \"_created\": \"2020-01-02\" }","title":"Example"},{"location":"triply-etl/transform/ratt/#function-split-split","text":"","title":"Function split() {#split}"},{"location":"triply-etl/transform/ratt/#description_8","text":"Splits a string into an array of strings, and stores that array in a new key.","title":"Description"},{"location":"triply-etl/transform/ratt/#whitespace-handling","text":"This transformation removes any trailing whitespace that remains after the strings are split. This ensures that irregular use of whitespace in the source data is taken care of automatically.","title":"Whitespace handling"},{"location":"triply-etl/transform/ratt/#use-cases_4","text":"The transformation is used when: - Tablular source data encodes multiple values inside singular cells. (Such concatenated storage inside cells is a data quality issue, because the table format cannot guarantee that the separator character does not (accidentally) occur inside individual values inside a cell. Tree-shaped source formats are able to store multiple values for the same key reliably, e.g. JSON and XML.) - Source data contains complex string values that can be decomposed into stand-alone components with distinct meaning.","title":"Use cases"},{"location":"triply-etl/transform/ratt/#parameters_14","text":"content A key that stores a string, or a string specified with assertion str() . separator A string or a regular expression that is used to separate the content. key A new key where the array of splitted strings is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example-multiple-values-in-singular-table-cells","text":"Tabular formats are unable to store more than one value in a cell. Because of this limitation, tabular data sources sometimes encode multiple values in cells by encoding them into one string. A separator character is typically used to distinguish between these multiple values. Suppose that the source data looks as follows: Parent Child John Jane, Jake , Kate ,, The following transformation splits the cells that encode zero or more children for each parent: split({ content: 'Child', separator: ',', key: 'Children', }), This results in the following transformed records: { \"Parent\": \"John\", \"Child\": \"Jane, Jake , \", \"Children\": [ \"Jane\", \"Jake\" ] } { \"Parent\": \"Kate\", \"Child\": \",, \", \"Children\": [] } Notice that trailing whitespace and empty values are dealt with automatically. Since the split() transformation always results in an array of strings, we can use the term assertion iris() afterwards: split({ content: 'children', separator: ',', key: '_children', }), triple( iri(prefix.person, 'parent'), sdo.children, iris(prefix.person, '_children') ), This results in the following linked data assertions: person:johndoe sdo:children person:janedoe, person:jakedoe.","title":"Example: Multiple values in singular table cells"},{"location":"triply-etl/transform/ratt/#example-split-a-complex-string-into-components","text":"The following snippet uses a regular expression to split a KIX code. (A KIX code is a standardized format for representing postal addresses in The Netherlands.) fromJson([{ id: '1', KIX: '1231FZ13Xhs' }]), split({ content: 'KIX', separator: /^(\\d{4}[A-Z]{2})(\\d{1,5})(?:X(.{1,6}))/, key: 'KIX_components', }), triple(iri(prefix.id, 'id'), sdo.postalCode, 'KIX_components[1]'), This results in the following record: { \"id\": \"1\", \"KIX\": \"1231FZ13Xhs\", \"KIX_components\": [\"\", \"1231FZ\", \"13\", \"hs\", \"\"] } And in the following linked data assertion: id:1 sdo:postalCode '1231FZ'.","title":"Example: Split a complex string into components"},{"location":"triply-etl/transform/ratt/#function-substring-substring","text":"","title":"Function substring() {#substring}"},{"location":"triply-etl/transform/ratt/#description_9","text":"This middleware takes a substring from the input string and stores the result in a new key.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_15","text":"content A key that stores a string value, or a string specified with assertion str() . start The index of the first character that is included in the substring. The first character has index 0. end Optionally, the index of the first character that is excluded from the substring. If absent, the substring ends at the end of the source string. key The new key in which the substring is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_13","text":"The Library of Congress MARC format stores the type of record in the sixth character that appears in the leader key. We use substring() to extract this character, and then use transformation translateAll() to map them to a corresponding class IRI: substring({ content: 'metadata.marc:record.marc:leader.$text', start: 6, end: 7, key: '_typeOfRecord', }), translateAll({ content: '_typeOfRecord', table: { a: dcm.Text, k: dcm.StillImage, }, key: '_class', }), triple('_iri', a, '_class'),","title":"Example"},{"location":"triply-etl/transform/ratt/#function-translateall-translateall","text":"","title":"Function translateAll() {#translateAll}"},{"location":"triply-etl/transform/ratt/#description_10","text":"Translates all dynamic strings from a specific key to new values of an arbitrary type To , according to a specified translation table. Since this function translates all values, the mapped values can have any type T ; they do not need to be strings. For example, this allows strings to be translated to IRIs or to literals.","title":"Description"},{"location":"triply-etl/transform/ratt/#when-to-use_2","text":"This approach is used when: The set of source data values is small. The set of source data values is known ahead of time. The corresponding linked data terms are known ahead of time. The appearance of a new value is considered to be an error in the source data.","title":"When to use?"},{"location":"triply-etl/transform/ratt/#parameters_16","text":"content A key that contains a string value. table A translation table from strings to values of some arbitrary type T . nulls Optionally, a list of string values that are considered denote NULL values in the source data. When a NULL value is encountered, the special value undefined is added for the target key . default Optionally, a default value or a default value-determining function that is used for string values that are neither in the translation table ( table ) nor in the NULL values list ( nulls ). The function must return a value of type T . Use of a default value value is equivalent to using the following value-determining function: _ => value . key A new key where the results of the translation are stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example-map-source-data-to-iri-values","text":"Suppose that source data contains country names. In linked data we want to use IRIs to denote countries, so that we can link additional information. Since the list of countries that appears in the source data is not that long, we can specify a translation table from names to IRIs by hand: change.translateAll({ content: 'country', table: { 'Belgium': country.be, 'Germany': country.de, 'England': country.gb, ..., }, nulls: ['Unknown'], key: '_country', }), when('country', [ triple('_country', a, sdo.Country), ]),","title":"Example: Map source data to IRI values"},{"location":"triply-etl/transform/ratt/#example-map-source-data-to-iri-properties","text":"When we relate a creative work to its creator, we sometimes know whether the creator was the actor, architect, author, etc. of the creative work. But in other cases we only know that there is a generic creator relationship. The Library of Congress Relators vocabulary allows us to express specific and generic predicates of this kind. transform.translateAll({ table: { 'actor': rel.act, 'architect': rel.arc, 'author': rel.aut, ..., }, default: rel.oth, // generic relator key: '_relator', }), triple('_creativeWork', '_relator', '_creator'),","title":"Example: Map source data to IRI properties"},{"location":"triply-etl/transform/ratt/#function-translatesome-translatesome","text":"","title":"Function translateSome() {#translateSome}"},{"location":"triply-etl/transform/ratt/#description_11","text":"Translates some strings, according to the specified translation table, to other strings. Strings that are not translated according to the translation table are copied over as-is.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_17","text":"content A key that contains a string value. table A translation table that specifies translations from strings to strings. key A new key where the translated strings are stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#use-cases_5","text":"Source data often contains some strings that are correct and some that are incorrect. For example, if source data contains a key with city names, some of the names may be misspelled. In such cases, translateSome() can be used to translate the incorrect strings into correct ones. A translateSome() transformation is often performed directly before a translateAll() transformation. The former ensures that all string values are correct (e.g. fixing typo's in city names); the latter ensures that all strings are mapped onto IRIs (e.g. city names mapped onto city-denoting IRIs).","title":"Use cases"},{"location":"triply-etl/transform/ratt/#example_14","text":"The following example fixes an encoding issue that occurs in the source data: transform.translateSome({ content: 'name', table: { 'Frysl\ufffd\ufffdn': 'Frysl\u00e2n', // Other entries for typographic fixes go here. ..., }, key: '_name', }),","title":"Example"},{"location":"triply-etl/transform/ratt/#function-tryliteral-tryliteral","text":"","title":"Function tryLiteral() {#tryLiteral}"},{"location":"triply-etl/transform/ratt/#description_12","text":"This transformation is used when string values must be mapped onto literals with varying datatype IRIs. The datatype IRIs that could apply are specified in a list. The specified datatype IRIs are tried out from left to right. The first datatype IRI that results in a valid literal is chosen. content A key that contains a string value, or a string value specified with assertion str() . datatypes An array of two or more datatype IRIs. key A new key where the created literal is stored.","title":"Description"},{"location":"triply-etl/transform/ratt/#throws_1","text":"An exception is emitted if a string value does not belong to the lexical space of any of the specified datatype IRIs.","title":"Throws"},{"location":"triply-etl/transform/ratt/#example_15","text":"A literal is valid if the given string value appears in the lexical space of a specific datatype IRI. This is best explained with an example: tryLiteral({ content: 'date', datatypes: [xsd.date, xsd.gYearMonth, xsd.gYear], key: '_publicationDate', }), Source data in key 'date' Result in key '_date' '1900-01-02' '1900-01-02'^^xsd:date '1900' '1900'^^xsd:gYear '02-01-1900' An error is emitted. If we do not want to emit errors for string values that cannot be satisfy any of the specified datatype IRIs, we may choose to include xsd.string as the last datatype IRI in the list. Do notice however that this will result in dates that cannot be compared on a timeline, since they were not transformed to an XSD date/time datatype.","title":"Example"},{"location":"triply-etl/transform/ratt/#see-also_6","text":"You only need to use tryLiteral() if the datatype IRI varies from record to record. If the datatype IRI is the same for every record, then the regular assertion function literal() should be used instead.","title":"See also"},{"location":"triply-etl/transform/ratt/#function-uppercase-uppercase","text":"","title":"Function uppercase() {#uppercase}"},{"location":"triply-etl/transform/ratt/#description_13","text":"Translates a string value to its uppercase variant. This middleware can uppercase strings in any language; the Unicode Default Case Conversion algorithm is used for this.","title":"Description"},{"location":"triply-etl/transform/ratt/#parameters_18","text":"content A key that contains a string value. key A new key where the uppercase variant is stored.","title":"Parameters"},{"location":"triply-etl/transform/ratt/#example_16","text":"We do not have a good example for this transformation middleware yet. Let us know in case you have a good example!","title":"Example"},{"location":"triply-etl/transform/typescript/","text":"The vast majority of ETLs can be written with the core set of RATT Transformations . But sometimes a custom transformation is necessary that cannot be handled by this core set. For such circumstances, TriplyETL allows a custom TypeScript function to be written. Notice that the use of a custom TypeScript function should be somewhat uncommon. The vast majority of real-world transformations should be supported by the core set of RATT Transformations. Context Custom TypeScript functions have access to various resources inside the TriplyETL. These resources include, but are not limited to, the full Record and the full Internal Store. TriplyETL refers to these resources as the Context . context.app The TriplyETL pipeline object. context.getX Tetrieves the value of a specific key in the Record and assumes it has type X , e.g. getAny() , getNumber() , getString() . context.record The current Record. context.store The Internal Store. Function custom.add() {#add} Adds a new entry to the Record, based on more than one existing entry. The value of the entry is the result of an arbitrary TypeScript function that has access to the full Context . Function signature The custom.add function has the following signature: etl.use( custom.add({ value: context => FUNCTION_BODY, key: 'NEW_KEY', }), ) The function can be configured in the following ways: - FUNCTION_BODY the body of a function, taking the Context as its input parameter ( context) and ending with a return statement returning the newly added value. - NEW_KEY must be the name of a new entry in the Record. Error conditions This function emits an error if NEW_KEY already exists in the current Record. See also Notice that it is bad practice to use custom.add() for adding a new entry that is based on exactly one existing entry. In such cases, the use of function custom.copy() is better, since it does not require access to the full Context. Example: Numeric calculations Suppose the source data contains a numeric balance and a numeirc rate. We can use function custom.add() to calculate the interest and store it in a new key: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { balance: 100, rate: 0.1 }, { balance: 200, rate: 0.2 } ]), custom.add({ value: context => context.getNumber('balance') * context.getNumber('rate'), key: 'interest', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100, \"rate\": 0.1, \"interest\": 10 } { \"balance\": 200, \"rate\": 0.2, \"interest\": 40 } Function custom.change() {#change} Changes an existing entry in the Record. The change function takes the old value and returns the new value. Function signature This function has the following signature: etl.use( custom.change({ key: 'KEY_NAME', type: 'VALUE_TYPE', change: value => FUNCTION_BODY, }), ) The function can be configured in the following way: - KEY_NAME must be the name of a key in the record. - VALUE_TYPE must be one of the following type-denoting strings: - 'array' an array whose elements have type any . - 'boolean' a Boolean value ( true or false ). - 'iri' a universal identifier / IRI term. - 'literal' an RDF literal term. - 'number' a natural number or floating-point number. - 'string' a sequence of characters. - 'unknown' an unknown type. - FUNCTION_BODY a function body that returns the new value. Error conditions This function emits an error if the specified key ( KEY_NAME ) does not exist in the RATT record. Use custom.copy() if you want to create a new entry based on an existing one. Example: Numeric calculation Suppose the source data contains a balance in thousands. We can use function custom.change() to multiply the balance inplace: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: 200 }]), custom.change({ change: value => 1_000 * value, type: 'number', key: 'balance', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100000 } { \"balance\": 200000 } Notice that the values for the balance keys were changed. Example: Cast numeric data Some source data formats are unable to represent numeric data. A good example are the CSV and TSV formats, where every cell value is represented as a string. If such a source data format that cannot represent numeric data is used, it is often useful to explicitly cast string values to numbers. For example, assume the following input table that uses strings to encode the number of inhabitants for each country: Country Inhabitants France '' Germany '83190556' Italy 'empty' Netherlands '17650200' We can use the custom.change() function to cast the values stored in the 'Inhabitants' key to numbers in the following way: custom.change({ change: value => +(value as number), type: 'unknown', key: 'Inhabitants', }), Notice that the type must be set to 'unknown' because a string is not allowed to be cast to a number in TypeScript (because not every string can be cast to a number). After custom.change() has been applied, the record looks as follows: Country Inhabitants France 0 Germany 83190556 Italy null Netherlands 17650200 Notice that strings that encode a number are correctly transformed, and non-empty strings that do not encode a number are transformed to null . Most of the time, this is the behaviour that you want in a linked data pipeline. Also notice that the empty string is cast to the number zero. Most of the time, this is not what you want. If you want to prevent this transformation from happening, and you almost certainly do, you must process the source data conditionally, using control structures . Example: Variant type A variant is a value that does not always have the same type. Variants may appear in dirty source data, where a value is sometimes given in one way and sometimes in another. In such cases, the type parameter must be set to 'unknown' . Inside the body of the change function we first cast the value to a variant type. In TypeScript the notation for this is a sequence of types separated by the pipe ( | ) character. Finally, the typeof operator is used to clean the source data to a uniform type that is easier to process in the rest of the ETL. The following code snippet processes source data where the balance is sometimes specified as a number and sometimes as a string: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: \"200\" }]), custom.change({ key: 'balance', type: 'unknown', change: value => { const tmp = value as number | string switch (typeof tmp) { case 'number': return value as number case 'string': return parseInt(value as string) } }, }), logRecord(), ) return etl } This prints the following two records, where the balance is now always a number that can be uniformly processed: { \"balance\": 100 } { \"balance\": 200 } Example: String or object In the following example the name of a person is sometimes given as a plain string and sometimes as an object with a fistName and a lastName key: The following function transforms this variant to a uniform string type: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { name: 'John Doe' }, { name: { firstName: 'Jane', lastName: 'Doe' } } ]), custom.change({ key: 'name', type: 'unknown', change: value => { const tmp = value as { firstName: string, lastName: string } | string switch (typeof tmp) { case 'string': return tmp case 'object': return tmp.firstName + ' ' + tmp.lastName } }, }), logRecord(), ) return etl } This print the following two records that can now be uniformly processed: { \"name\": \"John Doe\" } { \"name\": \"Jane Doe\" } custom.replace() {#replace} Replaces the value of an existing key based on the value from another key. Function signature The custom.replace() function has the following signature: etl.use( custom.replace({ fromKey: 'FROM_KEY', type: 'VALUE_TYPE', change?: value => FUNCTION_BODY, toKey: 'FROM_TYPE', }), ) fromKey is the name of the key whose value is going to be used to replace the old value with. type is the name of the type of the value in fromKey . The change key optionally specifies a function that takes the cast value of fromKey and that returns the value that will be stored in toKey . If the change function is not specified, it is identical to value => value . toKey is the name of the existing key whose value is going to be replaced. Error conditions This function emits an error under the following conditions: - fromKey does not specify a key name that exists in the current Record. - toKey does not specify a key name that exists in the current Record. - fromKey and toKey are the same. See also If fromKey and toKey are the same, then function custom.change() must be used instead.","title":"2. Transform: TypeScript"},{"location":"triply-etl/transform/typescript/#context","text":"Custom TypeScript functions have access to various resources inside the TriplyETL. These resources include, but are not limited to, the full Record and the full Internal Store. TriplyETL refers to these resources as the Context . context.app The TriplyETL pipeline object. context.getX Tetrieves the value of a specific key in the Record and assumes it has type X , e.g. getAny() , getNumber() , getString() . context.record The current Record. context.store The Internal Store.","title":"Context"},{"location":"triply-etl/transform/typescript/#function-customadd-add","text":"Adds a new entry to the Record, based on more than one existing entry. The value of the entry is the result of an arbitrary TypeScript function that has access to the full Context .","title":"Function custom.add() {#add}"},{"location":"triply-etl/transform/typescript/#function-signature","text":"The custom.add function has the following signature: etl.use( custom.add({ value: context => FUNCTION_BODY, key: 'NEW_KEY', }), ) The function can be configured in the following ways: - FUNCTION_BODY the body of a function, taking the Context as its input parameter ( context) and ending with a return statement returning the newly added value. - NEW_KEY must be the name of a new entry in the Record.","title":"Function signature"},{"location":"triply-etl/transform/typescript/#error-conditions","text":"This function emits an error if NEW_KEY already exists in the current Record.","title":"Error conditions"},{"location":"triply-etl/transform/typescript/#see-also","text":"Notice that it is bad practice to use custom.add() for adding a new entry that is based on exactly one existing entry. In such cases, the use of function custom.copy() is better, since it does not require access to the full Context.","title":"See also"},{"location":"triply-etl/transform/typescript/#example-numeric-calculations","text":"Suppose the source data contains a numeric balance and a numeirc rate. We can use function custom.add() to calculate the interest and store it in a new key: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { balance: 100, rate: 0.1 }, { balance: 200, rate: 0.2 } ]), custom.add({ value: context => context.getNumber('balance') * context.getNumber('rate'), key: 'interest', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100, \"rate\": 0.1, \"interest\": 10 } { \"balance\": 200, \"rate\": 0.2, \"interest\": 40 }","title":"Example: Numeric calculations"},{"location":"triply-etl/transform/typescript/#function-customchange-change","text":"Changes an existing entry in the Record. The change function takes the old value and returns the new value.","title":"Function custom.change() {#change}"},{"location":"triply-etl/transform/typescript/#function-signature_1","text":"This function has the following signature: etl.use( custom.change({ key: 'KEY_NAME', type: 'VALUE_TYPE', change: value => FUNCTION_BODY, }), ) The function can be configured in the following way: - KEY_NAME must be the name of a key in the record. - VALUE_TYPE must be one of the following type-denoting strings: - 'array' an array whose elements have type any . - 'boolean' a Boolean value ( true or false ). - 'iri' a universal identifier / IRI term. - 'literal' an RDF literal term. - 'number' a natural number or floating-point number. - 'string' a sequence of characters. - 'unknown' an unknown type. - FUNCTION_BODY a function body that returns the new value.","title":"Function signature"},{"location":"triply-etl/transform/typescript/#error-conditions_1","text":"This function emits an error if the specified key ( KEY_NAME ) does not exist in the RATT record. Use custom.copy() if you want to create a new entry based on an existing one.","title":"Error conditions"},{"location":"triply-etl/transform/typescript/#example-numeric-calculation","text":"Suppose the source data contains a balance in thousands. We can use function custom.change() to multiply the balance inplace: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: 200 }]), custom.change({ change: value => 1_000 * value, type: 'number', key: 'balance', }), logRecord(), ) return etl } This prints the following two records: { \"balance\": 100000 } { \"balance\": 200000 } Notice that the values for the balance keys were changed.","title":"Example: Numeric calculation"},{"location":"triply-etl/transform/typescript/#example-cast-numeric-data","text":"Some source data formats are unable to represent numeric data. A good example are the CSV and TSV formats, where every cell value is represented as a string. If such a source data format that cannot represent numeric data is used, it is often useful to explicitly cast string values to numbers. For example, assume the following input table that uses strings to encode the number of inhabitants for each country: Country Inhabitants France '' Germany '83190556' Italy 'empty' Netherlands '17650200' We can use the custom.change() function to cast the values stored in the 'Inhabitants' key to numbers in the following way: custom.change({ change: value => +(value as number), type: 'unknown', key: 'Inhabitants', }), Notice that the type must be set to 'unknown' because a string is not allowed to be cast to a number in TypeScript (because not every string can be cast to a number). After custom.change() has been applied, the record looks as follows: Country Inhabitants France 0 Germany 83190556 Italy null Netherlands 17650200 Notice that strings that encode a number are correctly transformed, and non-empty strings that do not encode a number are transformed to null . Most of the time, this is the behaviour that you want in a linked data pipeline. Also notice that the empty string is cast to the number zero. Most of the time, this is not what you want. If you want to prevent this transformation from happening, and you almost certainly do, you must process the source data conditionally, using control structures .","title":"Example: Cast numeric data"},{"location":"triply-etl/transform/typescript/#example-variant-type","text":"A variant is a value that does not always have the same type. Variants may appear in dirty source data, where a value is sometimes given in one way and sometimes in another. In such cases, the type parameter must be set to 'unknown' . Inside the body of the change function we first cast the value to a variant type. In TypeScript the notation for this is a sequence of types separated by the pipe ( | ) character. Finally, the typeof operator is used to clean the source data to a uniform type that is easier to process in the rest of the ETL. The following code snippet processes source data where the balance is sometimes specified as a number and sometimes as a string: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ balance: 100 }, { balance: \"200\" }]), custom.change({ key: 'balance', type: 'unknown', change: value => { const tmp = value as number | string switch (typeof tmp) { case 'number': return value as number case 'string': return parseInt(value as string) } }, }), logRecord(), ) return etl } This prints the following two records, where the balance is now always a number that can be uniformly processed: { \"balance\": 100 } { \"balance\": 200 }","title":"Example: Variant type"},{"location":"triply-etl/transform/typescript/#example-string-or-object","text":"In the following example the name of a person is sometimes given as a plain string and sometimes as an object with a fistName and a lastName key: The following function transforms this variant to a uniform string type: import { Etl, fromJson } from '@triplyetl/etl/generic' import { custom, logRecord } from '@triplyetl/etl/ratt' export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([ { name: 'John Doe' }, { name: { firstName: 'Jane', lastName: 'Doe' } } ]), custom.change({ key: 'name', type: 'unknown', change: value => { const tmp = value as { firstName: string, lastName: string } | string switch (typeof tmp) { case 'string': return tmp case 'object': return tmp.firstName + ' ' + tmp.lastName } }, }), logRecord(), ) return etl } This print the following two records that can now be uniformly processed: { \"name\": \"John Doe\" } { \"name\": \"Jane Doe\" }","title":"Example: String or object"},{"location":"triply-etl/transform/typescript/#customreplace-replace","text":"Replaces the value of an existing key based on the value from another key.","title":"custom.replace() {#replace}"},{"location":"triply-etl/transform/typescript/#function-signature_2","text":"The custom.replace() function has the following signature: etl.use( custom.replace({ fromKey: 'FROM_KEY', type: 'VALUE_TYPE', change?: value => FUNCTION_BODY, toKey: 'FROM_TYPE', }), ) fromKey is the name of the key whose value is going to be used to replace the old value with. type is the name of the type of the value in fromKey . The change key optionally specifies a function that takes the cast value of fromKey and that returns the value that will be stored in toKey . If the change function is not specified, it is identical to value => value . toKey is the name of the existing key whose value is going to be replaced.","title":"Function signature"},{"location":"triply-etl/transform/typescript/#error-conditions_2","text":"This function emits an error under the following conditions: - fromKey does not specify a key name that exists in the current Record. - toKey does not specify a key name that exists in the current Record. - fromKey and toKey are the same.","title":"Error conditions"},{"location":"triply-etl/transform/typescript/#see-also_1","text":"If fromKey and toKey are the same, then function custom.change() must be used instead.","title":"See also"},{"location":"triply-etl/validate/","text":"The Validate step ensures that the linked data a pipeline produces conforms to the requirements specified in the data model. graph LR source -- 1. Extract --> record record -- 2. Transform --> record record -- 3. Assert --> ld ld -- 4. Enrich --> ld ld -- 5. Validate --> ld ld -- 6. Publish --> tdb linkStyle 4 stroke:red,stroke-width:3px; ld[Internal Store] record[Record] source[Data Sources] tdb[(Triple Store)] Triply believes that every ETL should include the Validate step to ensure that only valid data is published in knowlede graphs. Two approaches TriplyETL supports two approaches for validating linked data: 5A. Graph Comparison uses one or more manually created 'gold records'. Graph comparison ensures that these records are transformed in the intended way by the ETL pipeline. 5B. SHACL Validation uses a generic data model. SHACL Validation ensures that each individual record is processed in accordance with the generic data model. Notice that it is possible to combine these two approaches in the same ETL pipeline: you can use graph comparison to test for specific conformities, and use SHACL to test for generic conformities.","title":"5. TriplyETL: Validate"},{"location":"triply-etl/validate/#two-approaches","text":"TriplyETL supports two approaches for validating linked data: 5A. Graph Comparison uses one or more manually created 'gold records'. Graph comparison ensures that these records are transformed in the intended way by the ETL pipeline. 5B. SHACL Validation uses a generic data model. SHACL Validation ensures that each individual record is processed in accordance with the generic data model. Notice that it is possible to combine these two approaches in the same ETL pipeline: you can use graph comparison to test for specific conformities, and use SHACL to test for generic conformities.","title":"Two approaches"},{"location":"triply-etl/validate/graph-comparison/","text":"Graph comparison is an approach for validating that the data produced by your TriplyETL pipeline is correct. For a limited set of key records, the linked data that is generated by the TriplyETL pipeline is compared to graphs that were created by hand. This comparison must follow certain rules that are laid out in the RDF standards. Prerequisites Graph comparison can be used when the following preconditions are met: You have identified a number of key records that are representative of your dataset. For these identified key records, you have created RDF files that contain the linked data that should be generated by your TriplyETL pipeline. You have added these RDF files to your TriplyETL configuration, e.g. by storing them in a special subdirectory next to your *.ts files. The function for comparing graphs can be imported from the generic TriplyETL library: import { compareGraphs } from '@triplyetl/etl/generic' A complete example We use the following full TriplyETL script to explain the validation feature. Do not be afraid by the length of the script; we will go through each part step-by-step. import { compareGraphs, declarePrefix, Etl, fromJson, Source } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { qudt, rdf, unit } from '@triplyetl/etl/vocab' const base = declarePrefix('https://example.com/') const prefix = { graph: declarePrefix(base('id/graph/')), skolem: declarePrefix(base('.well-known/genid/')), } export default async function (): Promise<Etl> { const etl = new Etl({ defaultGraph: prefix.graph('instances') }) etl.use( fromJson([{ id: '1', height: 15 }]), pairs(iri(prefix.skolem, 'id'), [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), compareGraphs(Source.file('product-1.trig')), ) return etl } Step 1: Identify representative records Select a limited number of records from your data sources that together are representative for the full data source systems. This often includes typical records where the expected data items are included, as well as atypical records that are 'outliers'. There must be reasonable confidence that a TriplyETL pipeline that produces the correct results for these selected records is likely to produce correct results for all records. Step 2: Manually create linked data Create an RDF file for every representative record. The file must contain the linked data that should be generated for that specific record. It is probably best to use the TriG format for this, but all RDF formats are supported. For our example, we create a file called product-1.trig with the following content: prefix graph: <https://example.com/id/graph/> prefix qudt: <http://qudt.org/schema/qudt/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix unit: <http://qudt.org/vocab/unit/> graph:instances { [] qudt:unit unit:CentiM; rdf:value 15. } Step 3: Create the ETL Write the TriplyETL configuration. In our example we start with the prefix declarations: const base = declarePrefix('https://example.com/') const prefix = { graph: declarePrefix(base('id/graph/')), skolem: declarePrefix(base('.well-known/genid/')), } We use the following extractor: fromJson([{ id: '1', height: 15 }]), And we use the following assertions: pairs(iri(prefix.skolem, 'id'), [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), Step 4. Perform graph comparison In our example, we only generate linked data for one record, so we can trivially compare the results for that one record with the manually created RDF that is stored in the local file: compareGraphs(Source.file('product-1.trig')), In more complex pipleines you may need to determine the local file based on the data. For example, check whether the value for key id in the record corresponds to a local file called product-ID.trig . If the generated graph differs in any meaningful way from the RDF stored in the local file, and error is emitted and the TriplyETL pipeline is interrupted.","title":"5. Validate: Graph Comparison"},{"location":"triply-etl/validate/graph-comparison/#prerequisites","text":"Graph comparison can be used when the following preconditions are met: You have identified a number of key records that are representative of your dataset. For these identified key records, you have created RDF files that contain the linked data that should be generated by your TriplyETL pipeline. You have added these RDF files to your TriplyETL configuration, e.g. by storing them in a special subdirectory next to your *.ts files. The function for comparing graphs can be imported from the generic TriplyETL library: import { compareGraphs } from '@triplyetl/etl/generic'","title":"Prerequisites"},{"location":"triply-etl/validate/graph-comparison/#a-complete-example","text":"We use the following full TriplyETL script to explain the validation feature. Do not be afraid by the length of the script; we will go through each part step-by-step. import { compareGraphs, declarePrefix, Etl, fromJson, Source } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { qudt, rdf, unit } from '@triplyetl/etl/vocab' const base = declarePrefix('https://example.com/') const prefix = { graph: declarePrefix(base('id/graph/')), skolem: declarePrefix(base('.well-known/genid/')), } export default async function (): Promise<Etl> { const etl = new Etl({ defaultGraph: prefix.graph('instances') }) etl.use( fromJson([{ id: '1', height: 15 }]), pairs(iri(prefix.skolem, 'id'), [qudt.unit, unit.CentiM], [rdf.value, 'height'], ), compareGraphs(Source.file('product-1.trig')), ) return etl }","title":"A complete example"},{"location":"triply-etl/validate/graph-comparison/#step-1-identify-representative-records","text":"Select a limited number of records from your data sources that together are representative for the full data source systems. This often includes typical records where the expected data items are included, as well as atypical records that are 'outliers'. There must be reasonable confidence that a TriplyETL pipeline that produces the correct results for these selected records is likely to produce correct results for all records.","title":"Step 1: Identify representative records"},{"location":"triply-etl/validate/graph-comparison/#step-2-manually-create-linked-data","text":"Create an RDF file for every representative record. The file must contain the linked data that should be generated for that specific record. It is probably best to use the TriG format for this, but all RDF formats are supported. For our example, we create a file called product-1.trig with the following content: prefix graph: <https://example.com/id/graph/> prefix qudt: <http://qudt.org/schema/qudt/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix unit: <http://qudt.org/vocab/unit/> graph:instances { [] qudt:unit unit:CentiM; rdf:value 15. }","title":"Step 2: Manually create linked data"},{"location":"triply-etl/validate/graph-comparison/#step-3-create-the-etl","text":"Write the TriplyETL configuration. In our example we start with the prefix declarations: const base = declarePrefix('https://example.com/') const prefix = { graph: declarePrefix(base('id/graph/')), skolem: declarePrefix(base('.well-known/genid/')), } We use the following extractor: fromJson([{ id: '1', height: 15 }]), And we use the following assertions: pairs(iri(prefix.skolem, 'id'), [qudt.unit, unit.CentiM], [rdf.value, 'height'], ),","title":"Step 3: Create the ETL"},{"location":"triply-etl/validate/graph-comparison/#step-4-perform-graph-comparison","text":"In our example, we only generate linked data for one record, so we can trivially compare the results for that one record with the manually created RDF that is stored in the local file: compareGraphs(Source.file('product-1.trig')), In more complex pipleines you may need to determine the local file based on the data. For example, check whether the value for key id in the record corresponds to a local file called product-ID.trig . If the generated graph differs in any meaningful way from the RDF stored in the local file, and error is emitted and the TriplyETL pipeline is interrupted.","title":"Step 4. Perform graph comparison"},{"location":"triply-etl/validate/shacl/","text":"This page documents how SHACL is used to validate linked data in the internal store of your ETL pipeline. Prerequisites SHACL Validation can be used when the following preconditions are met: A data model that uses SHACL constraints. Some data must be asserted in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add assertions to that store. The function for running SHACL Validation is imported as follows: import { validate } from '@triplyetl/etl/shacl' A complete example We use the following full TriplyETL script to explain the validation feature. Do not worry about the length of the script; we will go through each part step-by-step. import { Etl, Source, declarePrefix, fromJson, toTriplyDb } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { validate } from '@triplyetl/etl/shacl' import { a, foaf } from '@triplyetl/etl/vocab' const prefix = { id: declarePrefix('https://triplydb.com/Triply/example/id/'), } export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), toTriplyDb({ dataset: 'test' }), ) return etl } Step 1: Source data In our example we are using the following source data that records the age of a person: { \"age\": \"twelve\", \"id\": \"id\" } In our example the data source is inline JSON , but notice that any source format could have been used: fromJson([{ age: 'twelve', id: '1' }]), Step 2: Target data (informal) Based on the source data in Step 1, we want to publish the following linked data in TriplyDB: id:123 a foaf:Person; foaf:age 'twelve'. Step 3: Information Model (informal) Our intended target data in Step 2 looks ok at first glance. But we want to specify the requirements for our data in generic terms. Such a specification is called an Information Model . An Information Model is a generic specification of the requirements for our data. It is common to illustrate an Information Model with a picture: classDiagram class foaf_Person { foaf_age: xsd_nonNegativeInteger [1..1] } This Information Model specifies that instances of class foaf:Person must have exactly one value for the foaf:age property. Values for this property must have datatype xsd:nonNegativeInteger . Step 4: Transformation We now have source data (Step 1), and a fair intuition about our target data (Step 2), and an Information Model (Step 3). We can automate the mapping from source to target data with an Assertion : etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) That looks about right: we create instances of class foaf:Person and triples that assert a foaf:age property for each such person. However, a linked data expert may notice that the value 'twelve' from the source data will not be transformed into a non-negative integer ( xsd:nonNegativeInteger ). Indeed, our 'age' assertion will create a literal with datatype xsd:string . Oops, that violates the Information Model! How can we automate such checks? The above example is relatively simple, so a linked data expert may notice the error and fix it. But what happens when the ETL configuration is hundreds of lines long and is spread across multiple files? What happens when there is a large number of classes, and each class has a large number of properties? What if some of the properties are required, while others are optional? Etc. Obviously, any real-world ETL will quickly become too complex to validate by hand. For this reason, TriplyETL provides automated validation. Triply considers having an automated validation step best practice for any ETL. This is the case even for small and simple ETLs, since they tend to grow into complex ones some day. Step 5: Information Model (formal) The linked data ecosystem includes the SHACL standard for encoding Information Models. SHACL allows us to formally express the picture from Step 3. The model is itself expressed in linked data: prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age. Notice the following details: - We enforce a Closed World Semantics (CWA) in our Information Models with the sh:closed property. If a property is not explicitly specified in our Information Model, it is not allowed to be used with instance data. - We create IRIs in the dedicated shp: namespace for nodes in the Information Model. - Elements in our Information Model are always in a one-to-one correspondence with elements in our Knowledge Model: - Node shapes such as shp:Person relate to a specific class such as foaf:Person . - Property shapes such as shp:Person_age relate to a specific property such as foaf:age . Step 6: Use the validate() function TriplyETL has a dedicated function that can be used to automatically enforce Information Models such as the one expressed in Step 5. Since the Information Model is relatively small, it can be specified in-line using the string source type . Larger models will probably be stored in a separate file or in a TriplyDB graph or asset. validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), When we run the validate() function at the end of our ETL script, we will receive the following error: ERROR (Record #1) SHACL Violation on node id:1 for path foaf:age, source shape shp:Person_age: 1. Value does not have datatype xsd:nonNegativeInteger Oops! The value for the foaf:age property has an incorrect datatype. This is indeed the automated check and feedback that we want. Notice that the requirement that was violated ( shp:Person_age ) is mentioned in the notification. If we want to learn more, we can look up this node in our Information Model. If we want to take a look at a concrete example in our instance data, we can also take look at node id:1 which is also mentioned in the notfication. Step 7: Fix the validation error Now that we receive the automated validation error in Step 6, we can look for ways to fix our ETL. Let us take one more look at our current assertions: etl.run( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) We could change the term assertion for the value of foaf:age to explicitly make use of the xsd:nonNegativeInteger datatype: literal('age', xsd.nonNegativeInteger) But that would not work in TriplyETL: the Triply software (luckily!) does not allow us to create incorrect linked data. Since the following literal would be incorrect, TriplyETL does not even allow us to assert it: 'twelve'^^xsd:nonNegativeInteger Well, it is nice that TriplyETL does not allow us to create incorrect data. But how can we fix the issue at hand? How can we create linked data that follows our Information Model? As in any ETL error, there are 3 possible solutions: Change the data in the source system. Change the ETL transformations and/or assertions. Change the Information Model. Option 1: Change the source data In this case, changing the data in the source system seem the most logical. After all, there may be multiple ways in which the age of a person can be described using one or more English words. Expressing ages numerically is a good idea in general, since it will make the source data easier to interpret. Option 2: Change the transformation and/or assertions Alternatively, it is possible to transform English words that denote numbers to their corresponding numeric values. Since people can get up to one hundred years old, or even older, there are many words that we must consider and transform. This can be done with the translateAll() transformation : translateAll({ content: 'age', table: { 'one': 1, ... 'twelve': 12, ..., 'one hundred': 100, ..., }, key: '_age', }), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, literal('_age', xsd.nonNegativeInteger)], ), But even the above transformation may not suffice. The same number can be expressed in multiple ways in natural language, so the mapping will never be truly complete and reliable. This seems to be the worst of the three options in this case. Option 3: Change the Information Model Finally, we could loosen the Information Model. For example, we could change the datatype to check for strings: shp:Person_age sh:datatype xsd:string. But that would invalidate ETLs that generate numeric ages for persons, even though that seems perfectly fine, if not better than generating strings. Also, this would allow literals like 'abc' to pass validation as a legal value for foaf:age . Alternatively, we can remove the sh:datatype requirement from our Information Model entirely. That would allow either string-based ages or numeric ages to be specified. But now even weirder values for age, e.g. '2023-01-01'^^xsd:date , would be considered valid values for age. Reflections on which option to choose Notice that TriplyETL does not tell you which of the 3 options you should follow in order to fix issues in your ETL. After all, creating an ETL requires domain knowledge based on which you weight the pros and const of different options. However, TriplyETL does give you the tools to discover issues that prompt you to come up with such solutions. And once you have decided on a specific solution, TriplyETL provides you with the tools to implement it.","title":"5. Validate: SHACL"},{"location":"triply-etl/validate/shacl/#prerequisites","text":"SHACL Validation can be used when the following preconditions are met: A data model that uses SHACL constraints. Some data must be asserted in the internal store. If your internal store is still empty, you can read the Assert documentation on how to add assertions to that store. The function for running SHACL Validation is imported as follows: import { validate } from '@triplyetl/etl/shacl'","title":"Prerequisites"},{"location":"triply-etl/validate/shacl/#a-complete-example","text":"We use the following full TriplyETL script to explain the validation feature. Do not worry about the length of the script; we will go through each part step-by-step. import { Etl, Source, declarePrefix, fromJson, toTriplyDb } from '@triplyetl/etl/generic' import { iri, pairs } from '@triplyetl/etl/ratt' import { validate } from '@triplyetl/etl/shacl' import { a, foaf } from '@triplyetl/etl/vocab' const prefix = { id: declarePrefix('https://triplydb.com/Triply/example/id/'), } export default async function (): Promise<Etl> { const etl = new Etl() etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), toTriplyDb({ dataset: 'test' }), ) return etl }","title":"A complete example"},{"location":"triply-etl/validate/shacl/#step-1-source-data","text":"In our example we are using the following source data that records the age of a person: { \"age\": \"twelve\", \"id\": \"id\" } In our example the data source is inline JSON , but notice that any source format could have been used: fromJson([{ age: 'twelve', id: '1' }]),","title":"Step 1: Source data"},{"location":"triply-etl/validate/shacl/#step-2-target-data-informal","text":"Based on the source data in Step 1, we want to publish the following linked data in TriplyDB: id:123 a foaf:Person; foaf:age 'twelve'.","title":"Step 2: Target data (informal)"},{"location":"triply-etl/validate/shacl/#step-3-information-model-informal","text":"Our intended target data in Step 2 looks ok at first glance. But we want to specify the requirements for our data in generic terms. Such a specification is called an Information Model . An Information Model is a generic specification of the requirements for our data. It is common to illustrate an Information Model with a picture: classDiagram class foaf_Person { foaf_age: xsd_nonNegativeInteger [1..1] } This Information Model specifies that instances of class foaf:Person must have exactly one value for the foaf:age property. Values for this property must have datatype xsd:nonNegativeInteger .","title":"Step 3: Information Model (informal)"},{"location":"triply-etl/validate/shacl/#step-4-transformation","text":"We now have source data (Step 1), and a fair intuition about our target data (Step 2), and an Information Model (Step 3). We can automate the mapping from source to target data with an Assertion : etl.use( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) That looks about right: we create instances of class foaf:Person and triples that assert a foaf:age property for each such person. However, a linked data expert may notice that the value 'twelve' from the source data will not be transformed into a non-negative integer ( xsd:nonNegativeInteger ). Indeed, our 'age' assertion will create a literal with datatype xsd:string . Oops, that violates the Information Model! How can we automate such checks? The above example is relatively simple, so a linked data expert may notice the error and fix it. But what happens when the ETL configuration is hundreds of lines long and is spread across multiple files? What happens when there is a large number of classes, and each class has a large number of properties? What if some of the properties are required, while others are optional? Etc. Obviously, any real-world ETL will quickly become too complex to validate by hand. For this reason, TriplyETL provides automated validation. Triply considers having an automated validation step best practice for any ETL. This is the case even for small and simple ETLs, since they tend to grow into complex ones some day.","title":"Step 4: Transformation"},{"location":"triply-etl/validate/shacl/#step-5-information-model-formal","text":"The linked data ecosystem includes the SHACL standard for encoding Information Models. SHACL allows us to formally express the picture from Step 3. The model is itself expressed in linked data: prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age. Notice the following details: - We enforce a Closed World Semantics (CWA) in our Information Models with the sh:closed property. If a property is not explicitly specified in our Information Model, it is not allowed to be used with instance data. - We create IRIs in the dedicated shp: namespace for nodes in the Information Model. - Elements in our Information Model are always in a one-to-one correspondence with elements in our Knowledge Model: - Node shapes such as shp:Person relate to a specific class such as foaf:Person . - Property shapes such as shp:Person_age relate to a specific property such as foaf:age .","title":"Step 5: Information Model (formal)"},{"location":"triply-etl/validate/shacl/#step-6-use-the-validate-function","text":"TriplyETL has a dedicated function that can be used to automatically enforce Information Models such as the one expressed in Step 5. Since the Information Model is relatively small, it can be specified in-line using the string source type . Larger models will probably be stored in a separate file or in a TriplyDB graph or asset. validate(Source.string(` prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix sh: <http://www.w3.org/ns/shacl#> prefix shp: <https://triplydb.com/Triply/example/model/shp/> prefix xsd: <http://www.w3.org/2001/XMLSchema#> shp:Person a sh:NodeShape; sh:closed true; sh:ignoredProperties ( rdf:type ); sh:property shp:Person_age; sh:targetClass foaf:Person. shp:Person_age a sh:PropertyShape; sh:datatype xsd:nonNegativeInteger; sh:maxCount 1; sh:minCount 1; sh:path foaf:age.` )), When we run the validate() function at the end of our ETL script, we will receive the following error: ERROR (Record #1) SHACL Violation on node id:1 for path foaf:age, source shape shp:Person_age: 1. Value does not have datatype xsd:nonNegativeInteger Oops! The value for the foaf:age property has an incorrect datatype. This is indeed the automated check and feedback that we want. Notice that the requirement that was violated ( shp:Person_age ) is mentioned in the notification. If we want to learn more, we can look up this node in our Information Model. If we want to take a look at a concrete example in our instance data, we can also take look at node id:1 which is also mentioned in the notfication.","title":"Step 6: Use the validate() function"},{"location":"triply-etl/validate/shacl/#step-7-fix-the-validation-error","text":"Now that we receive the automated validation error in Step 6, we can look for ways to fix our ETL. Let us take one more look at our current assertions: etl.run( fromJson([{ age: 'twelve', id: '1' }]), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, 'age'], ), ) We could change the term assertion for the value of foaf:age to explicitly make use of the xsd:nonNegativeInteger datatype: literal('age', xsd.nonNegativeInteger) But that would not work in TriplyETL: the Triply software (luckily!) does not allow us to create incorrect linked data. Since the following literal would be incorrect, TriplyETL does not even allow us to assert it: 'twelve'^^xsd:nonNegativeInteger Well, it is nice that TriplyETL does not allow us to create incorrect data. But how can we fix the issue at hand? How can we create linked data that follows our Information Model? As in any ETL error, there are 3 possible solutions: Change the data in the source system. Change the ETL transformations and/or assertions. Change the Information Model.","title":"Step 7: Fix the validation error"},{"location":"triply-etl/validate/shacl/#option-1-change-the-source-data","text":"In this case, changing the data in the source system seem the most logical. After all, there may be multiple ways in which the age of a person can be described using one or more English words. Expressing ages numerically is a good idea in general, since it will make the source data easier to interpret.","title":"Option 1: Change the source data"},{"location":"triply-etl/validate/shacl/#option-2-change-the-transformation-andor-assertions","text":"Alternatively, it is possible to transform English words that denote numbers to their corresponding numeric values. Since people can get up to one hundred years old, or even older, there are many words that we must consider and transform. This can be done with the translateAll() transformation : translateAll({ content: 'age', table: { 'one': 1, ... 'twelve': 12, ..., 'one hundred': 100, ..., }, key: '_age', }), pairs(iri(prefix.id, 'id'), [a, foaf.Person], [foaf.age, literal('_age', xsd.nonNegativeInteger)], ), But even the above transformation may not suffice. The same number can be expressed in multiple ways in natural language, so the mapping will never be truly complete and reliable. This seems to be the worst of the three options in this case.","title":"Option 2: Change the transformation and/or assertions"},{"location":"triply-etl/validate/shacl/#option-3-change-the-information-model","text":"Finally, we could loosen the Information Model. For example, we could change the datatype to check for strings: shp:Person_age sh:datatype xsd:string. But that would invalidate ETLs that generate numeric ages for persons, even though that seems perfectly fine, if not better than generating strings. Also, this would allow literals like 'abc' to pass validation as a legal value for foaf:age . Alternatively, we can remove the sh:datatype requirement from our Information Model entirely. That would allow either string-based ages or numeric ages to be specified. But now even weirder values for age, e.g. '2023-01-01'^^xsd:date , would be considered valid values for age.","title":"Option 3: Change the Information Model"},{"location":"triply-etl/validate/shacl/#reflections-on-which-option-to-choose","text":"Notice that TriplyETL does not tell you which of the 3 options you should follow in order to fix issues in your ETL. After all, creating an ETL requires domain knowledge based on which you weight the pros and const of different options. However, TriplyETL does give you the tools to discover issues that prompt you to come up with such solutions. And once you have decided on a specific solution, TriplyETL provides you with the tools to implement it.","title":"Reflections on which option to choose"},{"location":"triplydb-js/","text":"TriplyDB.js is the official programming library for interacting with TriplyDB . TriplyDB.js allows you to automate operations that would otherwise be performed in the TriplyDB GUI. TriplyDB.js is implemented in TypeScript . TypeScript is a type-safe language that transpiles to JavaScript . This allows you to use TriplyDB.js in web browsers as well as on servers (using Node.js ). TriplyDB.js is open source and its source code is published on GitHub . Please contact support@triply.cc for questions and suggestions. Overview TriplyDB.js contains several classes, each with their own methods. The documentation for every method includes at least one code example. These code examples can be run by inserting them into the following overall script. Notice that process.env.TOKEN picks up an API token that is stored in the environment variable called TOKEN . Follow the steps on this page to create a new API token in the TriplyDB GUI. require('source-map-support/register') import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) async function run() { // This is where the code examples in this reference section should be placed. } run().catch(e => { console.error(e) process.exit(1) }) process.on('uncaughtException', function (e) { console.error('Uncaught exception', e) process.exit(1) }) process.on('unhandledRejection', (reason, p) => { console.error('Unhandled Rejection at: Promise', p, 'reason:', reason) process.exit(1) }) The following sections document the various TriplyDB.js classes. Each class comes with its own methods. Classes are related through methods that connect them. For example, calling the getAccount method on a App object returns an Account object. classDiagram class Account { asOrganization() asUser() } Account --> Dataset: getDataset Account --> Dataset: getDatasets Account --> Query: getQuery Account --> Query: getQueries Account --> Story: getStory Account --> Story: getStories class App { getInfo() } App --> Account: getAccount App --> Account: getAccounts App --> Organization: getOrganization App --> User: getUser class Asset { getInfo() } class Dataset { getInfo() } Dataset --> Asset: getAsset Dataset --> Asset: getAssets Dataset --> Service: getService Dataset --> Service: getServices class Organization { } Account <|-- Organization Organization --> User: getMember Organization --> User: getMembers class Query { getInfo() } class Story { getInfo() } class User { } Account <|-- User User --> Organization: getOrganizations App Instances of the App class are specific application connections that are set-up with a TriplyDB server. Connections to TriplyDB servers can be created with and without setting an API token. When no API token is set, the connection can be used to perform read-only operations over public data. When an API token is set, the connection can be used to perform read/write operations over public/private data the API token grants access to. The following snippet creates an instance of the App object that establishes read-only access to the TriplyDB server at https://triplydb.com : import App from '@triply/triplydb' const triply = App.get({ url: 'https://api.triplydb.com' }) Notice that the URL must point to the API of the TriplyDB server that the App object connects to. The API URL is typically created by adding the api. subdomain in front of the server's host name. For example, since [1] is the web-based GUI for the TriplyDB server, then [2] is the corresponding API for that instance. [1] https://triplydb.com [2] https://api.triplydb.com When an API token is specified, the operations that can be performed through the App object are determined by: The access level of the token: either \u201cRead access\u201d, \u201cWrite acces\u201d, or \u201cManagement access\u201d. The credentials of the user account for which the API token is created. When a user is a member of an organization, she has access to all its datasets, stories, and queries; a user always has access to her own datasets, stores and queries. The following token access levels are available: \u201cRead access\u201d allows: Read operations over data with access level \u201cPublic\u201d. Read operations over data with access level \u201cInternal\u201d. Read operations over data with access level \u201cPrivate\u201d that belongs to the user who created the token. Read operations over data with access level \u201cPrivate\u201d that belongs to organizations to which the user who created the token is a member. \u201cWrite acces\u201d allows: All operations allows by \u201cRead acces\u201d. Write operations over data that has access setting \u201cInternal\u201d. Write operations over data \u201cManagement access\u201d allows the following operations to be performed: creating organizations, adding/removing members to/from organizations. The following creates a App object with an API token that is made available through an environment variable (see section Setting up a secure read/write project ): import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) It is typical for one TriplyDB.js script to have exactly one App object. App.getAccount(name?: string) Returns the TriplyDB account with the given name . If name is omitted, the TriplyDB account that is associated with the current API token is returned. Examples The following snippet returns the account called 'Triply' . ts const account = await triply.getAccount('Triply') The following snippet returns the current account. This is the account for which the currently configured API token was created. ts const account = await triply.getAccount() See also This method returns an account object. See class Account for an overview of the methods that can be called on such objects. Class Account has two specializations: class Organization and class User . In line with these class specializations, there are also two method specializations: Method App.getOrganization(name: string) returns an organization object. Method App.getUser(name?: string) returns a user object. App.getAccounts() Returns an async iterator over all accounts in the TriplyDB server. Example The following snippet prints the display names for all accounts in the TriplyDB server at https://triplydb.com : ts const triply = App.get({ url: 'https://api.triplydb.com' }) for await (const account of triply.getAccounts()) { console.log((await account.getInfo()).name) } The following snippet returns an array that contains all account objects: ts console.log(await triply.getAccounts().toArray()) See class Account for an overview of the methods that can be used with account objects. App.getInfo() Returns information about the TriplyDB server that the App is connected to. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples The following snippet prints the contact email for the TriplyDB server: console.log((await triply.getInfo()).contactEmail) The following snippet returns an object describing the used TriplyDB server: console.log(await triply.getInfo()) App.getOrganization(name: string) Returns the TriplyDB organization with the given name . This method is similar to App.getAccount(name?: string) , but differs in the following ways: This method only works for accounts that represent TriplyDB organizations. This method returns an organization object. Class Organization is a specialization of class Account . Examples The following snippet returns the organization called 'Triply' : const organization = await triply.getOrganization('Triply') See class Organization for an overview of the methods that can be used with organization objects. Alternatives This method is a shorthand for calling the following two methods: Call method App.getAccount(name?: string) to retrieve an account object. Then call method Account.asOrganization() to cast the account object into an organization object. The following snippet returns the same result as the previous example, but uses two methods instead of one: const account = await triply.getAccount('Triply') const organization = account.asOrganization() See also This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects. App.getUser(name?: string) Returns the TriplyDB user with the given name . If name is omitted, the TriplyDB user that is associated with the current API token is returned. This only works if an API token is configured for the current App object. Examples The following snippet returns the user with name 'somebody' : const user = await triply.getUser('somebody') The following snippet returns the user for whom the API token was created. This only works if an API token was configured when the App object was created: const me = await triply.getUser() Alternatives This method is a shorthand for the following two methods: Call method App.getAccount() to retrieve an account object. Then call method Account.asUser() to cast the account object into a user object. The following snippet returns the same result as the previous examples, but uses two methods instead of one: const account = await triply.getAccount('somebody') const user = account.asUser() See also This method returns a user object. See class User for an overview of the methods that can be called on such objects. App.isCompatibleWith(minimumVersion: string) Succeeds if and only if the currently connected to TriplyDB server has a version that is identical to or higher than the given minimum version. Arguments Argument minimumVersion must be a string that uses Semantic Versioning. For example '1.2.3' . See also To inspect the current version of the connected-to TriplyDB server, use App.getInfo() . Account Instances of the Account class denote TriplyDB accounts. Accounts can be either organizations ( Organization ) or users ( User ). Account objects are obtained by calling the following method: App.getAccount(name?: string) Account.addDataset(name: string, metadata?: object) Adds a new TriplyDB dataset with the given name to the current account. The optional metadata argument is used to specify the metadata for the dataset. Access restrictions Creating a new dataset only succeeds if an API token is configured that provides write access to the current account. The default access level for a newly created dataset is private . If you want to publish a dataset with a different access level, you must specify the accessLevel key in the metadata argument. Arguments The name argument specifies the URL-friendly name of the new dataset. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The full URL of the newly created dataset has the following structure: https://{host}/{account}/{dataset} The metadata argument optionally specifies the access level and other important metadata: accessLevel The access level of the dataset. The following values are supported: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. When no access level is specified, the most restrictive access level ( private ) is used. description The human-readable description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL' 'None' (default) prefixes The IRI prefix declarations that are configured for the dataset. This is specified as a dictionary object whose keys are aliases and whose values are IRI prefixes. Examples The following snippet creates a new dataset called 'iris' under the account called 'Triply' : The dataset has private access, because the access level is not specified explicitly. The dataset has a description. The dataset has a display name. The dataset has the PDDL license. const account = await triply.getAccount('Triply') const dataset = await account.addDataset('iris', { description: 'A multivariate dataset that quantifies morphologic variation of Iris flowers.', displayName: 'Iris', license: 'PDDL', name: 'iris', prefixes: { def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', }, }) See also This method returns a dataset object. See the Dataset section for an overview of the methods that can be called on such objects. Account.addQuery(name: string, metadata: object) Adds a new SPARQL query to the account. Arguments Required: name: string The URL-friendly name of the new query. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). queryString: string The SPARQL query string (e.g., 'select * { ?s ?p ?o }' ). dataset: Dataset An instance of class Dataset that the current API token gives access to. or service: Service An instance of class Service that the current API token gives access to and that you want to be associated with this query. The Service given will be used as a preferred service for this query. Optional: The metadata argument specifies the required Dataset or Service and access level. Other important metadata can be set optionally: accessLevel The access level of the query. If none is set it defaults to 'private' . The following values are supported: 'private' The query can only be accessed by the Account object for which it is created. 'internal' The query can only be accessed by people who are logged into the TriplyDB server. 'public' The query can be accessed by everybody. description: string A human-readable description of the query. displayName: string The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name . output: string The visualization plugin that is used to display the result set of the query. If none is set it defaults to 'table' . 'boolean' The [boolean](https://triply.cc/docs/yasgui#table) view is a special view for ask queries. The value is either 'true' or 'false', and is visualized as `X` (False) or `V` (True). 'gallery' The [gallery](https://triply.cc/docs/yasgui#gallery) view allows SPARQL results to be displayed in an HTML gallery. 'gchart' The [gchart](https://triply.cc/docs/yasgui#charts) renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. 'geo' The [geo](https://triply.cc/docs/yasgui#geo) allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map. 'geoEvents' The [geoEvents](https://triply.cc/docs/yasgui#geoEvents) plugin renders geographical events as a story map. 'geo3d' The [geo3d](https://triply.cc/docs/yasgui#geo3d) allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. 'markup' The [markup](https://triply.cc/docs/yasgui#markup) can be used to render a variety of markup languages. This requires the use of the `?markup` variable to identify which variable to render. 'network' The [network](https://triply.cc/docs/yasgui#network) renders SPARQL Construct results in a graph representation. The maximum amount of results that can be visualized is 1.000 due to performance. 'pivot' The [pivot](https://triply.cc/docs/yasgui#pivot) view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows. 'response' The [response](https://triply.cc/docs/yasgui#response) view shows the body of the response and offers a easy way to download the result as a file. 'table' The [table](https://triply.cc/docs/yasgui#table) view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. 'timeline' The [timeline](https://triply.cc/docs/yasgui#timeline) timeline renders the SPARQL results on a Timeline. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form Variable (see below) Instances of Variable are objects that can have the following keys: Required: name: string A SPARQL variable name. The variable name must appear in the query string. The question mark ( ? ) or dollar sign ( $ ) is not included. termType: 'Literal'|'NamedNode' The kind of variable. This must be either 'Literal' for literals or 'NamedNode' for IRIs. Optional: allowedValues: string[] The list of string values that is allowed for this variable. datatype: string (if termType='Literal' ) The datatype IRI for the literal variable. language: string (if termType='Literal' ) The language tag for the literal variable. Setting this implies that the dataset IRI is rdf:langString . defaultValue: string The default string value for the required: boolean Whether a query request must include an explicit value for this variable. The default value is false . Example The following snippet creates a query with the given query string: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const myDataset = await user.getDataset('my-dataset') const query = await user.addQuery('my-query', { dataset: myDataset, queryString: 'select (count(*) as ?n) { ?s ?p ?o. }', output: 'response', }) Account.addStory(name: string, metadata?: object) Adds a new data story. Required name: string The URL-friendly name of the data story. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). Optional accessLevel The access level of the dataset. If none is given the default of 'private' is used. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. content: StoryElementUpdate[] A list of story elements. The building blocks of the Story. displayName: string The human-readable name of the data story. This name may include spaces and other characters that are not allowed in the URL-friendly name. A story element is an object with the following keys: caption: string The caption is an explanatory text about a specific query. id: string Each Story element gets an Id when it is created. When you want to update a Story element you will need this Id. The Id is only required when updating an element and not needed when adding an object. paragraph: string The Markdown content of a story paragraph. Only allowed when the type is set to 'paragraph' query: Query An instance of class Query . queryVersion: number The version that is used of the specified query. type Either 'paragraph' or 'query' . Examples Example 1 - creates a new story that has access level 'private' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story') Example 2 - creates a new story that has access level 'public' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story', { accessLevel: 'public', }) Account.asOrganization() Casts the TriplyDB account object to its corresponding organization object. Class Organization is a specialization of class Account . Calling this method on an Organization object does nothing. Examples The following snippet retrieves the account named 'Triply' and casts it to an organization: const account = await triply.getAccount('Triply') const organization = account.asOrganization() Alternatives This method is not needed if the organization is directly retrieved with the specialization method App.getOrganization(name: string) . The following snippet returns the same result as the above example, but in a more direct way: const organization = await triply.getOrganization('Triply') See also This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects. Account.asUser() Casts the TriplyDB account object to its corresponding user object. Class User is a specialization of class Account . Calling this method on a User object does nothing. Examples The following snippet retrieves the account that represents the current user, and casts it to a user object: const account = await triply.getAccount() const user = account.asUser() Alternatives This method is not needed if the user is directly retrieved with the specialization method App.getUser(name?: string) . The following snippet returns the same result as the above example, but in a more direct way: const user = await triply.getUser() See also This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects. Account.ensureDataset(name: string, metadata?: object) Ensures the existence of a dataset with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a dataset with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a dataset, and conditionally create a new dataset or make metadata changes to an existing dataset. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a dataset with the given name , then the behavior is identical to calling Account.addDataset(name: string, metadata?: object) with the same arguments. If this account already has a dataset with the given name and with the same metadata , then this method makes no changes. Example const account = await triply.getAccount('Triply') const myDataset = await account.ensureDataset(`my-dataset`, { license: 'PDDL', }) See also The meaning of the argument to this method are identical to those of the Account.addDataset(name: string, metadata?: object) method. Account.getDataset(name: string) Returns the dataset with the given name that is published by this account. Examples The following snippet prints the name of the Iris dataset that is published by the Triply account: const account = await triply.getAccount('Triply') const dataset = await triply.getDataset('iris') console.log((await dataset.getInfo()).name) See also This method returns a dataset object. See class Dataset for an overview of the methods that can be called on such objects. Account.getDatasets() Returns an async iterator over the accessible datasets for the current account. Access restrictions The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public datasets belonging to this account. If an API token is configured, the iterator will include all public and internal datasets belonging to this account, and will include all private datasets belonging to this account if the API token gives read access to the account. Examples The following snippet prints the names of all accessible dataset under the Triply account: ts const account = await triply.getAccount('Triply') for await (const dataset of account.getDatasets()) { console.log((await dataset.getInfo()).name) } The following snippet prints the list of names of all accessible datasets under the Triply account: ts const account = await triply.getAccount('Triply') console.log(await account.getDatasets().toArray()) Account.getInfo() Returns information about this account. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for accounts includes the following keys: avatarUrl A URL to the account image. accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. createdAt The date and time on which the account was created. datasetCount The number of datasets for the account. queryCount The number of queries for the account. storyCount The number of stories for the account pinnedDatasets An array containing the pinned dataset for the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. type The account type: either organization or user . role The role of the account orgs An array of organizations of which the account is a member. Email address The email address of the account. updatedAt The date and time on which the account was last updated. lastActivity The date and time on which the account was last online on TriplyDB. Examples The following snippet prints the full information object for the account called \u2018Triply\u2019: ts const account = await triply.getAccount('Triply') console.log(await account.getInfo()) The output for this snippet can look as follows: json { 'accountName': 'Triply', 'avatarUrl': 'https://www.gravatar.com/avatar/9bc28997dd1074e405e1c66196d5e117?d=mm', 'createdAt': 'Mon Mar 19 2018 14:39:18 GMT+0000 (Coordinated Universal Time)', 'datasetCount': 16, 'name': 'Triply', 'queryCount': 37, 'storyCount': 7, 'type': 'org', 'updatedAt': 'Tue Nov 27 2018 09:29:38 GMT+0000 (Coordinated Universal Time)' } The following snippet prints the name of the account called \u2018Triply\u2019: ts const account = await triply.getAccount('Triply') console.log((await account.getInfo()).name) Account.getPinnedItems() Returns the list of datasets, stories and queries that are pinned for the current account. A pinned item is an item that is displayed in a prominent way on the account web page. Order considerations The order in which the pinned datasets are returned reflects the order in which they appear on the organization homepage (from top-left to bottom-right). Examples The following snippet prints the names of the items that are pinned on the Triply account page: const account = await triply.getAccount('Triply') for await (const item of account.getPinnedItems()) { console.log((await item.getInfo()).name) } See also This method returns various types of objects. Each class has different functionalities: See class Dataset for an overview of the methods for dataset objects. See class Query for an overview of the methods for query objects. See class Story for an overview of the methods for story objects. Account.getQuery(name: string) Returns the TriplyDB query with the given name . Examples The following snippet prints the query string for a query called animal-gallery that belongs to the account called Triply : const account = await triply.getAccount('Triply') const query = await account.getQuery('animal-gallery') console.log((await query.getInfo()).requestConfig?.payload.query) See also See class Query for an overview of the methods for query objects. Account.getQueries() Returns an async iterator over the accessible queries that belong to the account. Access restrictions The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public queries belonging to this account. If an API token is configured, the iterator will include all public and internal queries that belong to this account, and will include all private queries that belong to this account if the API token gives read access to the account. Examples The following snippet prints the names of the queries that belong to the account called Triply : const account = await triply.getAccount('Triply') for await (const query of account.getQueries()) { console.log((await query.getInfo()).name) } See also See class Query for an overview of the methods for query objects. Account.ensureStory(name: string, metadata: object) Ensures the existence of a story with the given name and with the specified metadata , if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a story with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a story, and conditionally create a new story or make metadata changes to an existing story. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a story with the given name , then the behavior is identical to calling Account.addStory(name: string, metadata?: object) with the same arguments. If this account already has a story with the given name and with the same metadata , then this method returns that story. Optional displayName Accepts a string value to be used as the display name for the story. accessLevel Accepts either of the following values: 'private' (default), 'internal' , 'public' . content Accepts a list of StoryElementUpdate objects, defined below. Note: If no accessLevel is specified, the default used is 'private'. Examples Example 1: To ensure a Story only requires a name of type string. It's access level will default to private await someUser.ensureStory(`someStoryName`) Example 2: Ensure a Story setting it's accessLevel and displayName . await someUser.ensureStory(`someStoryName`, { accessLevel: 'public', displayName: `This is a Story`, }) Account.addStory(name: string, newStoryOptions?: object) Required Adds and returns the TriplyDB story with the given name . Optional The optional new story object that can be passed accepts the following properties: displayName Accepts a string value to be used as a display name for the story accessLevel Sets the access level for the story. Accepts either of the following: 'private' (default), 'internal' , 'public' . If no accesslevel is specified, the default value private is used. Examples : Example 1 - creates a newStory that is 'private' const newStory = await someUser.addStory('name-of-story') Example 2 - creates a newStory that is 'public' const newStory = await someUser.addStory('name-of-story', { accessLevel: 'public', }) Account.getStory(name: string) Returns the TriplyDB story with the given name . Examples The following snippet prints the paragraphs in the story called the-iris-dataset that is published under the account called Triply . Stories are sequences of paragraphs and queries. This program prints the paragraphs in the sequence in which they appear in the story. const account = await triply.getAccount('Triply') const story = await account.getStory('the-iris-dataset') See also See class Story for an overview of the methods for story objects. Account.getStories() Returns an iterator with the TriplyDB stories that belong to the account. Examples The following snippet prints the names of the queries that belong to the Triply account: const account = await triply.getAccount('Triply') for await (const story of account.getStories()) { console.log((await story.getInfo()).name) } See also See class Story for an overview of the methods for story objects. Account.pinItems(items: array[Dataset|Story|Query]) Pins the given datasets, stores, and/or queries to the home page of this account. The pinned elements can be seen by people who visit the account online. They are also included in the account metadata. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const query = await user.getQuery('name-of-query') const newStory = await user.getStory('name-of-story') user.pinItems([query,newStory]) Account.setAvatar(file: string) Sets a new image that characterizes this account. A circular version of this image is displayed inside the TriplyDB GUI. This image is also published as part of account metadata. Examples The following snippet uploads the local image in file logo.svg and set it as the characterizing image for the Triply account: const account = await triply.getAccount('Triply') await account.setAvatar('logo.svg') Account.update(metadata: object) Updates the metadata for this account. To update the metadata profile with information within the metadata itself, we need the following steps: Obtain the relevant piece of information as a variable/const: getObject() Update the metadata profile with the obtained information stored in the variable/const: update() getObject() Define a constant ( const ) and assign it to ctx.store.getObjects() . The arguments for the function will be the subject, predicate, and graph. The function retrieves the object so the other 3 parts of a quad need to be specified. update() Update the relevant part of the metadata profile with the corresponding piece of information. .update({}) Example If one wants to update the display name of a metadata profile with the object of the following triple within the metadata: <https://example.org/example> <https://schema.org/name> 'Example Name'@en async (ctx) => { // Fetch displayName const displayName = ctx.store .getObjects( 'https://example.org/example', 'https://schema.org/name', graph.metadata ) .find( (node) => node.termType === 'Literal' && node.language === 'en' )?.value; // Specify the environment variable, if necessary const _dataset = process.env['MODE'] === 'Production' ? (await app.triplyDb.getOrganization(organization)).getDataset(dataset) : (await app.triplyDb.getUser()).getDataset(organization + '-' + dataset) // Update the display name if (displayName) await (await _dataset).update({ displayName }) }; The metadata object for accounts can include the following keys: accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. Email address The email address of the account. Asset Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB as Assets and can be integrated into the Knowledge Graph. Each asset has a specific identifier that can be used in the Knowledge Graph. An asset is always uploaded per dataset, for which the function uploadAsset() is used. see Dataset.uploadAsset() for uploading an asset. If the asset already has been created following functions can retrieve it from the dataset. - Dataset.getAsset(assetName: string, versionNumber?: number) - Dataset.getAssets() TriplyDB.js supports several functions to manipulate an asset on TriplyDB. Asset.addVersion(path: File | string) Update an asset with a new version of the document using the addVersion function. The input of this function is a path to the file location that you want to update the asset with. The file you want to add as a new version does not in any ways have to correspond to the asset. Example The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.addVersion('my-file.pdf') Asset.delete() To delete an asset with all of its versions execute the delete() function. Example The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.delete() Asset.getInfo(version?: number) Returns information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Optionally you can give the version number to retrieve the assetInfo of a particular version. The information object for assets includes the following keys: assetName The URL-friendly name of the asset. identifier The hexadecimal identifier of the asset createdAt The date and time on which the asset was created. url The url of the asset. versions An array containing all versions of the asset. uploadedAt The date and time on which the asset was uploaded. fileSize Number with the bytesize of the asset Examples The following snippet prints the full information object for the asset called \u2018my-asset\u2019: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getInfo()) Asset.getVersionInfo(version: number) Returns version specific information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The version specific information object for assets includes the following keys: id The hexadecimal identifier of the asset fileSize Number with the bytesize of the asset url The url of the asset. uploadedAt The date and time on which the asset was uploaded. Examples The following snippet prints the version information object for the asset called \u2018my-asset\u2019 at version 1 : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getVersionInfo(1)) Asset.selectVersion(version: number) With the selectVersion() function you can select a specific version of an Asset. Each version corresponds to a iteration of the file that is added as an asset. The argument of the selectVersion() function is a number of the version you want to retrieve. Example To select the first asset from the list of assets use the selectVersion with the argument 1 . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') const versionedAsset = asset.selectVersion(1) Asset.toFile(path: string, version?: number) The binary representation of an asset can be retrieved and written to file via the asset.toFile() function. This function takes as input a string path to the download location and optionally a version number. Example To download the latest version of my-asset asset to the file my-file-location.txt . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toFile('my-file-location.txt') Asset.toStream(version?: number) If instead of downloading the asset to a file for later usage you want to directly use the asset. The toStream() functionality is available. This downloads the asset as a stream for use in a script. The toStream() has as optional argument a version number. Example To get the latest version of my-asset asset as a stream available. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toStream() Dataset The Dataset class represents a TriplyDB dataset. Dataset.addPrefixes(prefixes: object) Adds IRI prefix declarations to the dataset. The prefixes argument is a dictionary object whose keys are aliases and whose values are IRI prefixes. Examples The following snippet adds prefix declarations for aliases id and def to the Iris dataset: const organization = await triply.getOrganization('Triply') const dataset = await organization.getDataset(iris) await dataset.addPrefixes({ def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', }) Dataset.ensureService(name: string, metadata?: object) Ensures the existence of a service with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a service with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a service, and conditionally create a new service or make metadata changes to an existing service. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this dataset does not yet have a service with the given name , then the behavior is identical to calling Dataset.addService(name: string, metadata?: object) with the same arguments. If this dataset already has a service with the given name , but with different metadata specified for it, then the behavior is identical to calling Account.getDataset(name: string) and Dataset.update(metadata: object) . If this dataset already has a service with the given name and with the same metadata , then this method returns that service. Required name Accepts a string value which is the name of the service to ensure. Optional: metadata serviceMetadata = { type: 'elasticsearch' | 'virtuoso' | 'jena' ; config?: { reasoner?: 'OWL' | 'RDFS' | 'None'; }; }; type Accepts a string value of one of the following: 'virtuoso' , 'elasticsearch' , 'jena' . config Config is an optional property. It accepts an object with a reasoner property. reasoner The reasoner property accepts a string value of either 'OWL' , 'RDFS' , or 'None' . Note: If no options are specified the default service is of type: virtuoso . Note that the config.reasoner will only accept a value when type is: 'jena' Examples Example 1: Ensure a service with no arguments. If not found it's type defaults to virtuoso . await someDataset.ensureService('someServiceName') Example 2: Ensure a service of type jena . await someDataset.ensureService('someServiceName', { type: 'jena' }) Dataset.addService(name: string, metadata?: object) Creates a new service for this dataset. Arguments Required name The URL-friendly name of the service. The name must only contain alphanumeric characters and hyphens (`[A-Za-z0-9\\-]`). Optional The service type is specified with the type parameter. If no type is given, a default of 'virtuoso' is used. It supports the following values: 'virtuoso' Starts a SPARQL service. A SPARQL 1.1 compliant service is very scalable and performance, but without advanced reasoning capabilities. 'jena' Starts a SPARQL JENA service. A SPARQL 1.1 compliant service that is less scalable and less performant, but allows reasoning (RDFS or OWL) to be enabled. 'elasticSearch' Starts an Elasticsearch service. A text search engine that can be used to power a search bar or similar textual search API. The name argument can be used to distinguish between different endpoints over the same dataset that are used for different tasks. Examples The following snippet starts two SPARQL endpoints over a specific dataset. One endpoint will be used in the acceptance environment while the other endpoint will be used in the production system. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const acceptance = await dataset.addService('acceptance') const production = await dataset.addService('production', { type: 'elasticsearch', }) const reasoning = await dataset.addService('reasoning', { type: 'jena', config: { reasoner: 'OWL' }, }) See also See class Service for an overview of the methods that can be used with service objects. Dataset.clear(...resourceType: string) Removes one or more resource types from the current dataset. Arguments The resources are specified by the rest parameter resourceType , which supports the following values : 'assets' Removes all assets in the dataset. 'graphs' Removes all graphs in the dataset. 'services' Removes all services in the dataset. Examples The following example code removes all graphs and services for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.clear('graphs', 'services') Dataset.copy(account: string, dataset: string) Creates a copy of the current dataset. The owner (user or organization) of the copy is specified with parameter account . The name of the copy is specified with parameter dataset . This operation does not overwrite existing datasets: if the copied-to dataset already exists, a new dataset with suffix -1 will be created. Examples const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.copy('account name', 'copy dataset name')) Dataset.delete() Deletes the dataset. This includes deleting the dataset metadata, all of its graphs, all of its services, and all of its assets. Examples The following snippet deletes a specific dataset that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.delete() See also Sometimes it is more useful to only delete the graphs that belong to a dataset, but leave the dataset metadata, services, and assets in place. The following methods can be used for this purpose: Dataset.deleteGraph(graphName: string) Dataset.removeAllGraphs() Dataset.deleteGraph(name: string) Deletes the graph with the given name from this dataset. Graph names are IRIs. Examples The following snippet deletes a specific graph from a specified dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.deleteGraph('https://example.org/some-graph') Dataset.describe(iri: string|NamedNode) Each dataset is a collection of triples that describe objects in linked data. Each object is defined with an IRI, an identifier for that object. An object often has incoming and outgoing connections. The Dataset.describe() call can retrieve the incoming and outgoing triples per object. The function returns for a given iri a list of quads where the iri is either in the subject or the object position. Examples The following snippet returns all triples that have https://example.org/id/some-instance in the subject or the object position: const user = await triply.getUser() const dataset = await account.getDataset('my-dataset') console.log(await dataset.describe('https://example.org/id/some-instance')) Dataset.getAsset(name: string, version?: number) Returns the asset with the given name for this dataset. Optionally allows the version number ( version ) of the asset to be specified. If the version number is absent, the latest version of the assert with the given name is returned. Examples The following snippet returns the original version of an image of a dog from the animals dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') const asset = await dataset.getAsset('file.png', 1) Dataset.getAssets() Returns an async iterator over the assets that belong to this dataset. Assets are binary files that are stored together with data graphs. Common examples include documents, images and videos. Examples The following snippet prints the assets for a specific dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const asset of dataset.getAssets()) { console.log(asset) } The following snippet prints the list of assets for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getAssets().toArray()) Dataset.getGraph(name: string) Each dataset with data consists out of one or more named graphs. All graphs together are thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. Instead of searching over the complete dataset where you want to scope it to a certain graph you can use the getGraph() function to specify the graph. Dataset.getGraph(name: string) returns the graph with the given name that belongs to this dataset. The name is the string representation of the graph IRI. The Dataset.getGraph returns a graph object. Examples The following snippet returns the graph about cats from the dataset about animals: const user = await triply.getUser() const dataset = await user.getDataset('animals') const graph = dataset.getGraph('https://example.com/cats') Dataset.getGraphs() Returns an async iterator over graphs that belong to this dataset. Examples The following snippet retrieves the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getGraphs().toArray()) Dataset.getInfo() Returns information about this dataset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples The following snippet prints the information from the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') console.log(await dataset.getInfo()) Dataset.getPrefixes() Returns the prefixes that are defined for this dataset. This contains prefix declarations that are generic and configured for this TriplyDB server, and prefix declarations that are defined for this specific dataset. Examples The following snippet prints the prefix declarations that hold for my-dataset : const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const prefix of dataset.getPrefixes()) { console.log(prefix) } Dataset.getService(name: string) Returns the service with the given name for this dataset. Examples The following snippet retrieves the acceptance service for the product catalog of an imaginary company: const organization = triply.getOrganization('some-company') const dataset = organization.getDataset('product-catalog') const service = dataset.getService('acceptance') Dataset.getServices() Returns an async iterator over TriplyDB services under a dataset. See class Service for an overview of the methods for service objects. Examples The following snippet emits the services that are enabled for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') for await (const service of dataset.getServices()) { console.log(service) } If you do not want to iterate over the services with an async iterator, but instead want to get an array of services use the .toArray() call instead: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getServices().toArray()) Dataset.getStatements({subject?: string, predicate?: string, object?: string, graph?: string}) Returns an async iterator with statements (quadruples) that fit the specified pattern. Arguments subject , if specified, is the subject term that should be matched. predicate , if specified, is the predicate term that should be matched. object , if specified, is the object term that should be matched. graph , if specified, is the graph name that should be matched. Example The following prints all statements in the dataset: const user = triply.getUser() const dataset = await user.getDataset('my-dataset') for await (const statement of dataset.getStatements()) { console.log(statement) } The following prints the description of the Amsterdam resource in the DBpedia dataset: const association = triply.getOrganization('DBpedia-association') const dbpedia = association.getDataset('dbpedia') for await (const statement of dbpedia.getStatements({subject: 'http://dbpedia.org/resource/Amsterdam'})) { console.log(statement) } Get the data locally Most of the time you do not need to download the entire dataset locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use the entire graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from TriplyDB. graphsToFile() , graphsToStore() and graphsToStream() . Dataset.graphsToFile(destinationPath: string, arguments?: object) The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error. Optional The optional properties accepted as arguments for graphsToFile Compressed Argument compressed optionally is a boolean defining if a graph is compressed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension. Graph Argument Graph optionally is an specific graph that you want to write to file. These graph is an instance of a 'Graph' class Examples The following example downloads the dataset to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') await dataset.graphsToFile('my-filename.ttl', {compressed: true}) Dataset.graphsToStore(graph?: Graph) The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a graphsToStore() where a N3 store is returned as a result of the graphsToStore() function. Optional The optional argument for graphsToStore is Graph . With Graph you can optionally define a specific graph that you want to write to file. These graph is an instance of a 'Graph' class. Examples The following example downloads the dataset as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const store = await dataset.graphsToStore() Dataset.graphsToStream(type: 'compressed' | 'rdf-js', arguments?: object) The final method to download linked data to a local source is the graphsToStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard . Optional The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`. Graph Argument Graph optionally is an specific graph that you want to write to file. This graph is an instance of a 'Graph' class Examples The following example streams through the dataset as rdf-js quad objects and prints the quad to the screen. Notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the dataset as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) } Dataset.importFromDataset(fromDataset: Dataset, arguments?: object) Imports one or more named graphs from a different dataset into this dataset. Data reuse is an important principle in linked data. This functionality makes it very easy to pull in vocabularies and datasets from other places. Changes in the fromDataset dataset are not automatically reflected in this dataset. If you want to synchronize with changes made in the imported-from dataset, the graphs must be explicitly imported. This protects this dataset against unanticipated changes in the imported-from dataset, while still being able to stay in sync with the imported-from dataset if this is explicitly requested. Required Argument fromDataset is the dataset object from which one or more graphs are imported over to this dataset. Optional The optional properties accepted as arguments for importFromDataset graphMap Argument ` graphMap ` optionally is an object with keys and values that implements a mapping from existing graph names (keys) to newly created graph names (values). Each key must be an existing graph name in the `from` dataset. Each value must be the corresponding graph name in this dataset. If this argument is not specified, then graph names in the `from` dataset are identical to graph names in this dataset. Note that either graphNames or graphMap can be given as optional argument and not both. graphNames Argument ` graphNames ` optionally is an array of graph names. These names can be one of three types: 'string', instances of a 'Graph' class, or instances of 'NamedNodes'. Note that either graphNames or graphMap can be given as optional argument and not both. overwrite Accepts a Boolean value. An optional property that determines whether existing graph names in this dataset are allowed to be silently overwritten. If this argument is not specified, then `false` is used as the default value. Examples The following snippet creates a new dataset ( newDataset ) and imports one graph from an existing dataset ( existingDataset ). Notice that the graph can be renamed as part of the import. Example 1 Imports the complete 'existingDataset' dataset to the 'newDataset' . const account = await triply.getAccount() const existingDataset = await account.getDataset('existingDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(existingDataset) Example 2 Imports 'anotherDataset' dataset to a 'newDataset' Where a graph from the existing dataset is renamed to the a graphname in the new dataset. Only the graphs from the graphMap are imported. const account = await triply.getAccount() const anotherDataset = await account.getDataset('anotherDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(anotherDataset, { graphMap: { 'https://example.org/existingDataset/graph': 'https://example.org/newDataset/graph'} }) Example 3 Import 'oneMoreDataset' dataset to the 'newDataset' Where a graph specific graph from the existing dataset is added to the new dataset. If the graph name already occurs in the 'newDataset' it will get overwritten. const account = await triply.getAccount() const oneMoreDataset = await account.getDataset('oneMoreDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(oneMoreDataset, { graphNames: ['https://example.org/existingDataset/graph'], overwrite: true, }) Dataset.importFromFiles(files: list(string || File), defaultsConfig?: object) Required Imports one or more files into this dataset. The files must contain RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported file baseIRI Accepts a string value that is set as the default baseIRI for each imported file overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file) Supported files The files must contain RDF data and must be encoded in one of the following standardized RDF serialization formats: N-Quads, N-Triples, TriG, Turtle. Examples Example 1 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz']) Example 2 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz'], { defaultGraphName: 'https://triplydb.com/Triply/example/graph/default', overwriteAll: true, }) Dataset.importFromStore(store: n3.Store, defaultsConfig?: object) One of the most complete libraries for handling linked data in memory is the n3 library . The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of converting your data from the N3 Store to a file and uploading to TriplyDB. TriplyDB.js has a importFromStore() where a N3 store is given as first argument and uploaded directly to triplyDB. Examples const store = new Store() store.addQuad(DataFactory.namedNode('https://triplydb.com/id/me'),DataFactory.namedNode('http://www.w3.org/2000/01/rdf-schema#label'),DataFactory.literal('me'),DataFactory.namedNode('https://triplydb.com/Triply/example/graph/default')) const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const dataset = (await user.getDatasets().toArray())[0] dataset.importFromStore(store) Dataset.importFromUrls(urls: list(string), defaultsConfig?: object) Required Imports one or more URLs into this dataset. The URLs must provide access to RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported URL baseIRI Accepts a string value that is set as the default baseIRI for each imported URL overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file) Examples dataset1.importFromUrls(['url1', 'url2', 'url3']) Dataset.removeAllGraphs() Removes all graphs from this dataset. Examples The following snippet removed all graphs from a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') await dataset.removeAllGraphs() Dataset.removePrefixes(prefixes: string[]) Removes IRI prefixes from this dataset. The prefixes argument is a string array, containing the prefix labels to be removed. Examples The following snippet removes the def and id prefixes from the specified dataset. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.removePrefixes(['def', 'id']) Dataset.renameGraph(from: string, to: string) Renames a graph of this dataset, where from is the current graph name and to is the new graph name. The string arguments for from and to must be valid IRIs. Examples The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.renameGraph( 'https://example.org/old-graph', 'https://example.org/new-graph' ) Dataset.update(metadata: object) Updates the metadata for this dataset. Arguments The metadata argument takes a dictionary object with the following optional keys: Required: accessLevel The access level of the dataset. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. Optional: description The description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL' Example The following snippet updates the dataset's access level, description, display name and license: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') dataset.update({ accessLevel: 'private', description: 'desc', displayName: 'disp', license: 'PDDL', }) Dataset.uploadAsset(assetName: string, filePath: string) Uploads a file that does not contain RDF data as an asset. User cases There are several use cases for assets: Source data that will be used as input files to an ETL process. Documentation files that describe the dataset. Media files (audio/image/video) that are described in the RDF graph. Examples The following snippet uploads a source CSV data file and a PDF documentation file: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.uploadAsset('my-source-data', 'source.csv.gz') await dataset.uploadAsset('my-documentation', 'documentation.pdf') Graph Each dataset with data consists out of one or more named graphs. All graphs together is thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. A graph has as advantage that is can partition data while at the same time keep the data in the same dataset. Reducing the overhead of having to move between datasets to traverse a graph. You can retrieve either retrieve all graphs from a dataset in the form of an async iterator. Or retrieve a specific graph from a dataset. Examples The following snippet retrieves the graph 'https://example.com/my-graph' for a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') The following snippet retrieves all the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graphs = dataset.getGraphs() The Graph is the smallest object that can be individually deleted or modified. Graph.delete() Deletes the graph of this dataset. Any copies of the graph will not be deleted. All services containing this graph will still contain the graph until the service is synced again. Examples The following snippet deletes a specific graph that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.delete() Graph.getInfo() Returns information about this graph. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The following keys and values are returned for graph.getInfo() id A hexadecimal hash of the graph to identify the graph for internal identification. graphName The URL-friendly name of the graphName that is used as identifier and name. numberOfStatements The number of statements in the graph. uploadedAt (Optional) The date/time at which the graph was uploaded to TriplyDB. importedAt (Optional) The date/time at which the query was imported from another dataset. importedFrom (Optional) graphName The graphname of the graph from the dataset from which the graph was imported. dataset The dataset from which the graph was imported. Examples The following snippet prints the information from the specified graph of the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') console.log(await graph.getInfo()) Graph.rename(name: string) Renames the graph, the argument name is the new graph name. The string argument for name must be a valid IRI. Examples The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await dataset.rename('https://example.org/new-graph') Get the data locally Most of the time you do not need to download a graph locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use a graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from a graph. toFile() , toStore() and toStream() . Graph.toFile(destinationPath: string, arguments?: object) The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error. Optional The optional properties accepted as arguments for toFile Compressed Argument compressed optionally is an boolean defining if a graph is compresssed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension. Examples The following example downloads the graph to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.toFile('my-filename.ttl', {compressed: true}) Graph.toStore(graph?: Graph) The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a toStore() where a N3 store is returned as a result of the the toStore() function. Examples The following example downloads the graph as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const store = await graph.toStore() Graph.toStream(type: 'compressed' | 'rdf-js', arguments?: object) The final method to download linked data to a local source is the toStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard . Optional The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`. Examples The following example streams through the graph as rdf-js quad objects. and prints the quad to the screen. notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the graph as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) } Organization Instances of class Organization denote organizations in TriplyDB. Obtaining instances Organizations are obtained with method App.getOrganization(name: string) : const organization = await triply.getOrganization('Triply') Alternatively, organizations are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to an organization ( Account.asOrganization() ): const account = await triply.getAccount('Triply') const organization = account.asOrganization() Inheritance Organization is a subclass of Account , from which it inherits most of its methods. Organization.addDataset(name: string, metadata?: object) Adds a new TriplyDB dataset with the given name to the current organization. Inherited from Account.addDataset(name: string, metadata?: object) . Organization.addMember(user: User, role?: Role) Adds a member to the given Organization , with the given role of either member or owner. Arguments The user argument has to be a user object of the user which should be added to the organization. The role argument can be either 'member' or 'owner' . If this argument is not specified, then 'member' is used as the default. 'member' A regular member that is allowed to read and write the datasets that are published under the organization. 'owner' An owner of the organization. Owners have all the rights of regular users, plus the ability to add/remove users to/from the organization, the ability to change the roles of existing users, and the ability to delete the organization. Examples The following snippet adds user John Doe to the Triply organization as a regular member. const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.addMember(johnDoe) Organization.removeMember(user: User) Removes a member from the given Organization . Organization.addQuery(name: string, metadata: object) Adds a new TriplyDB query to the current organization. Inherited from Account.addQuery(name: string, metadata: object) . Organization.ensureStory(name: string, metadata: object) Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) . Organization.addStory(name: string, metadata?: object) Adds a new TriplyDB story with the given name to the current organization. Inherited from Account.addStory(name: string, metadata?: object) . Organization.delete() Deletes this account. This also deletes all datasets, stories and queries that belong to this organization. Examples The following code example deletes the specified organization: const organization = await triply.getOrganization('Neo4j') await organization.delete() Organization.ensureDataset(name: string, metadata?: object) Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) . Organization.getDataset(name: string) Returns the dataset with the given name that is published by this organization. Inherited from Account.getDataset(name: string) . Organization.getDatasets() Returns an async iterator over the accessible datasets that belong to this organization. Inherited from Account.getDatasets() . Organization.getMembers() Returns the list of memberships for the given organization. Return type A membership contains the following components: role The role of the membership ( OrgRole ): either 'owner' for owners of the organization, or 'member' for regular members. The difference between owners and regular members is that owners can perform user management for the organization (add/remove/change memberships). user An instance of class User . createdAt A date/time string. updatedAt A date/time string. Examples const org = await triply.getOrganization('acme') for (const membership of await org.getMembers()) { console.log(user) } See also Memberships of organization are TriplyDB users . Organization.getPinnedItems() Returns the list of datasets, stories and queries that are pinned for the current organization. Inherited from Account.getPinnedItems() . Organization.removeMember(user: User) Removes the specified user from this organization. Arguments The user argument has to be a User object of a user. Existence considerations The user must be a current member of the organization for this method to succeed. If the user is not a current member of the organization, an error is thrown. Examples The following snippet removes John Doe from the Triply organization, using a string argument: const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.removeMember(johnDoe) The following snippet removes John Doe from the Triply organization, using a User object: const organization = await triply.getOrganization('Triply') const user = await triply.getUser('john-doe') await organization.removeMember(user) Organization.setAvatar(file: string) Sets a new image that characterized this organization. Inherited from Account.setAvatar(file: string) . Organization.update(metadata: object) Updates the metadata for this account. Inherited from Account.update(metadata: object) . Query A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query. Saved queries come with a REST API that can be configured with the use a SPARQL API variables. Query.delete() Permanently deletes this query and all of its versions. Query.getInfo() The returned dictionary object includes the following keys: accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). createdAt The date/time at which the query was created. dataset A dictionary object representing the dataset against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. numberOfVersions The number of currently stored versions of this query. owner A dictionary object representing the account (organization or user) to which the query belongs. \ud83d\udea7 link Stores part of the URL to run the query. Please use Query.getRunLink() to obtain the full URL to run the query. service The location of the SPARQL endpoint that is used to run the query. updatedAt The date/time at which the query was last modified. Query.getString(apiVariables?: object) Returns the query string of the current version of this query. Optionally, arguments can be specified for the API variables to this query. Examples The following code stores the SPARQL query string for the query object: const queryString = await query.getString() Query.addVersion(metadata: object) Adds a new version to the query used. It requires similar options to that of Query.addQuery . Arguments At least one of the following arguments is required to create a new version. Any argument not given will be copied from the previous version of that query. queryString: string the SPARQL compliant query as a string value output: string The visualization plugin that is used to display the result set. If none is set it defaults to 'table' . Other options may include: 'response' , 'geo' , 'gallery' , 'markup' , etc variables: Variable[] A list of objects with the following keys: IRI variable An object of the form `Variable` (see [`Account.addQuery()`](#accountaddqueryname-string-metadata-object) You can see how many versions exist on a query accessing Query.getInfo().numOfVersions You can use a specified version of a query accessing Query.useVersion(x: number) Query.getRunLink() Returns the URL link to run the query. It currently does not support the use of variables. Query.results(apiVariables?: object, options?: object) Query.results() function will automatically return all the results from a saved query. You can retrieve both results from a select or ask query and a construct or describe query. The results are returned as an async iterator . If there are more than 10 000 query results, they could be retrieved using pagination with TriplyDB.js . Examples Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For select queries you use the `statements()` call: const results = query.results().statements() // For select queries you use the `bindings()` call: const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings() Query.update(metadata: object) Updates the metadata for the saved query. This does not result in a new query version. It requires similar options to that of Query.addQuery . Arguments At least one of the following arguments is required to update the metadata. Any argument given will be copied from the previous version of that query. accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). dataset A dictionary object representing the dataset against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. preferredService If the autoselectService is not selected the user can set the preferred service. Query.useVersion(version: number|'latest') A saved query is saved with a version number. Each time the query or the visualization changes the version number is incremented with one. When you want to retrieve a saved query with a particular version you need the useVersion function. The function returns the query object corresponding to that version of the query. If you want to use the latest version of the query you need to set the version argument to 'latest' . Example const user = await triply.getAccount('my-account') const query = await user.getQuery('my-query') const query_1 = await query.useVersion(1) Service Service objects describe specific functionalities that can be created over datasets in TriplyDB. Service objects are obtained through the the following methods: Dataset.addService Dataset.getServices A service always has one of the following statuses: Removing The service is being removed. Running The service is running normally. Starting The service is starting up. Stopped The services was stopped in the past. It cannot be used at the moment, but it can be enable again if needed. Stopping The service is currently being stopped. Service.delete() Permanently deletes this service. Examples const user = await triply.getAccount('my-account') const dataset = await user.getDataset('my-dataset') const service = await dataset.addService('my-service') await service.delete() Service.getInfo() Returns information about this service. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples The following snippet prints information about the newly created service (named my-service ): const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.addService('my-service') console.log(await service.getInfo()) Service.isUpToDate() Returns whether this service is synchronized with the dataset contents. Synchronization Because services must be explicitly synchronized in TriplyDB, it is possible to have services that expose an older version of the dataset and services that expose a newer version of the dataset running next to one another. There are two very common use cases for this: The production version of an application or website runs on an older service. The data does not change, so the application keeps working. The acceptance version of the same application or website runs on a newer service. Once the acceptance version is finished, it becomes the production version and a new service for the new acceptance version is created, etc. An old service is used by legacy software. New users are using the newer endpoint over the current version of the data, but a limited number of older users want to use the legacy version. Examples The following code checks whether a specific service is synchronized: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.ensureService('my-service', {type: 'sparql'}) console.log(await service.isUpToDate()) Service.update() Synchronizes the service. Synchronization means that the data that is used in the service is made consistent with the data that is present in the graphs of the dataset. When one or more graphs are added or deleted, existing services keep exposing the old state of the data. The changes in the data are only exposed in the services after synchronization is performed. Examples When there are multiple services, it is common to synchronize them all in sequence . This ensures that there are always one or more services available. This allows applications to use such services as their backend without any downtime during data changes. The following code synchronizes all services of a dataset in sequence: for (const service of await dataset.getServices()) { service.update() } Although less common, it is also possible to synchronize all services of a dataset in parallel . This is typically not used in production systems, where data changes must not result in any downtime. Still, parallel synchronization can be useful in development and/or acceptance environments. The following code synchronizes all services of a dataset in parallel: await Promise.all(dataset.getServices().map(service => service.update())) Service.waitUntilRunning() A service can be stopped or updated. The use of asynchronous code means that when a start command is given it takes a while before the service is ready for use. To make sure a service is available for querying you can uesr the function waitUntilRunning() to make sure that the script will wait until the service is ready for use. Example An example of a service being updated and afterwards a query needs to be executed: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('some-dataset') const service = await dataset.getService('some-service') // starting a service but does not wait until it is started await service.start() // Function that checks if a service is available await service.waitUntilRunning() Story A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results. To create Data stories with TriplyDB.js You can use the User.ensureStory or User.addStory functions to create. If you want to retrieve an already created data story you can use the functions User.getStories to iterate over all stories, or retrieve a particular one with User.getStory . Story objects are obtained through the the following methods: User.addStory User.ensureStory User.getStories User.getStory Story.delete() Deletes this story. This deletes all paragraphs that belong to this story. This does not delete the queries that are linked into this story. If you also want to delete the queries, then this must be done with distinct calls of Query.delete() . Examples The following code example deletes a story called 'example-story' under the current user's account: const user = await triply.getUser() const story = await user.getStory('example-story') await story.delete() Story.getInfo() Returns information about this data story. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Examples The following snippet prints the paragraphs that appear in a data story: for (const element of (await story.getInfo()).content) { if ((element.type = 'paragraph')) { console.log(element.paragraph) } } User Instances of class User denote users in TriplyDB. Obtaining instances Users are obtained with method App.getUser(name?: string) : const user = triply.getUser('john-doe') const user = triply.getUser() Alternatively, users are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to a use ( Account.asUser() ): const account = await triply.getAccount('john-doe') const user = account.asUser() Inheritance User is a subclass of Account , from which it inherits most of its methods. Limitations Users cannot be created or deleted through the TriplyDB.js library. See the Triply Console documentation for how to create and delete users through the web-based GUI. User.addDataset(name: string, metadata?: object) Adds a new TriplyDB dataset with the given name to the current account. Inherited from Account.addDataset(name: string, metadata?: object) . User.addQuery(metadata: object) Adds a new TriplyDB query to the current user. Inherited from Account.addQuery(metadata: object) . User.ensureStory(name: string, metadata: object) Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) . User.addStory(name: string, metadata?: object) Adds a new TriplyDB story with the given name to the current user. Inherited from Account.addStory(name: string, metadata?: object) . User.createOrganization(name: string, metadata?: object) Creates a new organization for which this user will be the owner. Access restrictions This method requires an API token with write access for this user. Arguments Argument name is the URL-friendly name of the new organization. This name can only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The optional metadata argument can be used to specify additional metadata. This is a dictionary object with the following optional keys: description The description of the organization. This description can make use of Markdown. email The email address at which the organization can be reached. name The human-readable name of the organization. This name may contain spaces and other non-alphanumeric characters. Examples The following snippet creates a new organization for which John Doe will be the owner. Notice that both a required URL-friendly name ( 'my-organization' ) and an optional display name ( 'My Organization' ) are specified. const user = await triply.getUser('john-doe') await user.createOrganization(my-organization, {name: 'My Organization'})) User.ensureDataset(name: string, metadata?: object) Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) . User.getDataset(name: string) Returns the TriplyDB dataset with the given name that is published by this user. Inherited from Account.getDataset(name: string) . User.getDatasets() Returns an async iterator over the accessible datasets for the current user. Inherited from Account.getDatasets() . User.getInfo() Returns information about this user. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for users includes the following keys: avatarUrl A URL to the user image. accountName The URL-friendly name of the user. name The human-readable display name of the user description The human-readable description of the user. createdAt The date and time on which the user was created. datasetCount The number of datasets for the user. queryCount The number of queries for the user. storyCount The number of stories for the user pinnedItems An array containing the pinned items (datasets, stories and queries) for the user. role The role of the user. Either 'light', 'regular' or 'siteAdmin'. orgs An array of organizations of which the user is a member. Email address The email address of the user. updatedAt The date and time on which the user was last updated. lastActivity The date and time on which the user was last online on TriplyDB. Examples The following snippet prints an overview of account that is associated with the used API token: const user = await triply.getUser() console.log(await user.getInfo()) User.getOrganizations() Returns an async iterator over the organizations that this user is a member of. Order considerations The order in the list reflects the order in which the organizations appear on the user page in the Triply GUI. Examples The following snippet prints the list of organizations that John Doe is a member of: const user = await triply.getUser('john-doe') for await (const organization of await user.getOrganizations()) { console.log((await organization.getInfo()).name) } See also The async iterator contains organization objects. See the section about the Organization class for methods that can be used on such objects. User.getPinnedItems() Returns the list of datasets, stories and queries that are pinned for the current user. Inherited from Account.getPinnedItems() . User.setAvatar(file: string) Sets a new image that characterized this user. Inherited from Account.setAvatar(file: string) . User.update(metadata: object) Updates the metadata for this user. Inherited from Account.update(metadata: object) . FAQ This section includes answers to frequently asked questions. Please contact info@triply.cc if you have a question that does not appear in this list. How to perform a SPARQL query? The SPARQL 1.1 Protocol standard specifies a native HTTP API for performing SPARQL requests. Such requests can be performed with regular HTTP libraries. Here we give an example indicating how such an HTTP library can be used: import SuperAgent from 'superagent'; const reply = await SuperAgent.post('SPARQL_ENDPOINT') .set('Accept', 'application/sparql-results+json') .set('Authorization', 'Bearer ' + process.env.TOKEN) .buffer(true) .send({ query: 'select * { WHERE_CLAUSE } offset 0 limit 10000' }) // break condition when the result set is empty. // downsides: caching, string manipulation What is the latest version of TriplyDB.js? The latest version of TriplyDB.js can be found in the NPM repository . What to do when the \u201cError: Unauthorized\u201d appears? This error appears whenever an operation is performed for which the user denoted by the current API token is not authorized. One common appearance of this error is when the environment variable TOKEN is not set to an API token. The current value of the environment variable can be tested by running the following command in the terminal: echo $TOKEN How do I get the results of a saved query using TriplyDB.js? To reliably retrieve a large number of results as the output of a construct or select query, follow these steps: Import the triplydb library. ts import App from '@triply/triplydb'; Set your parameters, regarding the TriplyDB server and the account in which you have saved the query as well as the name of the query. ts const triply = App.get({ url: 'https://api.triplydb.com' }) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') If the query is not public, you should set your API token rather than the URL. ts const triply = App.get({ token: process.env.TOKEN }) Do not forget that we perform TriplyDB.js requests within an async context . That is: ts async function run() { // Your code goes here. } run() Get the results of a query by setting a results variable. More specifically, for construct queries: ts const results = query.results().statements() For select queries: ts const results = query.results().bindings() Note that for SPARQL construct queries, we use method .statements() , while for SPARQL select queries, we use method .bindings() . Additionally, saved queries can have API variables that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: ts // For SPARQL construct queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .statements() // For SPARQL select queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .bindings() To read the results you have three options: 5a. Iterate through the results per row in a for -loop: ts // Iterating over the results per row for await (const row of results) { // execute something } 5b. Save the results to a file. For saving SPARQL construct queries: ts // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') For saving SPARQL select queries. Currently we only support saving the file to a .tsv format: ts // Saving the results of a SPARQL select query to a file. await results.toFile('my-file.tsv') 5c. Load all results into memory. Note that this is almost never used. If you want to process results, then option 5a is better; if you want to persist results, then option 5b is better. ts // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray() What is an async iterator? {#async-iterator} TriplyDB.js makes use of async iterators for retrieving lists of objects. Async iterators are a method of fetching and iterating through large lists, without having to first fetch the whole set. An example of an async iterator in TriplyDB.js is App.getAccounts() . The following code illustrates how it can be used. for await (const account of triply.getAccounts()) { console.log(account) } For cases where you want the complete list, you can use the toArray function of the iterator. const accounts = await triply.getAccounts().toArray() TriplyDB.js returns async iterators from the following methods: App.getAccounts() Account.getDatasets() Account.getQueries() Account.getStories() Dataset.getServices() Dataset.getAssets() Dataset.getGraphs() Dataset.getStatements() Query.results().statements() for SPARQL construct and describe queries Query.results().bindings() for SPARQL select queries","title":"TriplyDB.js"},{"location":"triplydb-js/#overview","text":"TriplyDB.js contains several classes, each with their own methods. The documentation for every method includes at least one code example. These code examples can be run by inserting them into the following overall script. Notice that process.env.TOKEN picks up an API token that is stored in the environment variable called TOKEN . Follow the steps on this page to create a new API token in the TriplyDB GUI. require('source-map-support/register') import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) async function run() { // This is where the code examples in this reference section should be placed. } run().catch(e => { console.error(e) process.exit(1) }) process.on('uncaughtException', function (e) { console.error('Uncaught exception', e) process.exit(1) }) process.on('unhandledRejection', (reason, p) => { console.error('Unhandled Rejection at: Promise', p, 'reason:', reason) process.exit(1) }) The following sections document the various TriplyDB.js classes. Each class comes with its own methods. Classes are related through methods that connect them. For example, calling the getAccount method on a App object returns an Account object. classDiagram class Account { asOrganization() asUser() } Account --> Dataset: getDataset Account --> Dataset: getDatasets Account --> Query: getQuery Account --> Query: getQueries Account --> Story: getStory Account --> Story: getStories class App { getInfo() } App --> Account: getAccount App --> Account: getAccounts App --> Organization: getOrganization App --> User: getUser class Asset { getInfo() } class Dataset { getInfo() } Dataset --> Asset: getAsset Dataset --> Asset: getAssets Dataset --> Service: getService Dataset --> Service: getServices class Organization { } Account <|-- Organization Organization --> User: getMember Organization --> User: getMembers class Query { getInfo() } class Story { getInfo() } class User { } Account <|-- User User --> Organization: getOrganizations","title":"Overview"},{"location":"triplydb-js/#app","text":"Instances of the App class are specific application connections that are set-up with a TriplyDB server. Connections to TriplyDB servers can be created with and without setting an API token. When no API token is set, the connection can be used to perform read-only operations over public data. When an API token is set, the connection can be used to perform read/write operations over public/private data the API token grants access to. The following snippet creates an instance of the App object that establishes read-only access to the TriplyDB server at https://triplydb.com : import App from '@triply/triplydb' const triply = App.get({ url: 'https://api.triplydb.com' }) Notice that the URL must point to the API of the TriplyDB server that the App object connects to. The API URL is typically created by adding the api. subdomain in front of the server's host name. For example, since [1] is the web-based GUI for the TriplyDB server, then [2] is the corresponding API for that instance. [1] https://triplydb.com [2] https://api.triplydb.com When an API token is specified, the operations that can be performed through the App object are determined by: The access level of the token: either \u201cRead access\u201d, \u201cWrite acces\u201d, or \u201cManagement access\u201d. The credentials of the user account for which the API token is created. When a user is a member of an organization, she has access to all its datasets, stories, and queries; a user always has access to her own datasets, stores and queries. The following token access levels are available: \u201cRead access\u201d allows: Read operations over data with access level \u201cPublic\u201d. Read operations over data with access level \u201cInternal\u201d. Read operations over data with access level \u201cPrivate\u201d that belongs to the user who created the token. Read operations over data with access level \u201cPrivate\u201d that belongs to organizations to which the user who created the token is a member. \u201cWrite acces\u201d allows: All operations allows by \u201cRead acces\u201d. Write operations over data that has access setting \u201cInternal\u201d. Write operations over data \u201cManagement access\u201d allows the following operations to be performed: creating organizations, adding/removing members to/from organizations. The following creates a App object with an API token that is made available through an environment variable (see section Setting up a secure read/write project ): import App from '@triply/triplydb' const triply = App.get({ token: process.env.TOKEN }) It is typical for one TriplyDB.js script to have exactly one App object.","title":"App"},{"location":"triplydb-js/#appgetaccountname-string","text":"Returns the TriplyDB account with the given name . If name is omitted, the TriplyDB account that is associated with the current API token is returned.","title":"App.getAccount(name?: string)"},{"location":"triplydb-js/#examples","text":"The following snippet returns the account called 'Triply' . ts const account = await triply.getAccount('Triply') The following snippet returns the current account. This is the account for which the currently configured API token was created. ts const account = await triply.getAccount()","title":"Examples"},{"location":"triplydb-js/#see-also","text":"This method returns an account object. See class Account for an overview of the methods that can be called on such objects. Class Account has two specializations: class Organization and class User . In line with these class specializations, there are also two method specializations: Method App.getOrganization(name: string) returns an organization object. Method App.getUser(name?: string) returns a user object.","title":"See also"},{"location":"triplydb-js/#appgetaccounts","text":"Returns an async iterator over all accounts in the TriplyDB server.","title":"App.getAccounts()"},{"location":"triplydb-js/#example","text":"The following snippet prints the display names for all accounts in the TriplyDB server at https://triplydb.com : ts const triply = App.get({ url: 'https://api.triplydb.com' }) for await (const account of triply.getAccounts()) { console.log((await account.getInfo()).name) } The following snippet returns an array that contains all account objects: ts console.log(await triply.getAccounts().toArray()) See class Account for an overview of the methods that can be used with account objects.","title":"Example"},{"location":"triplydb-js/#appgetinfo","text":"Returns information about the TriplyDB server that the App is connected to. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"App.getInfo()"},{"location":"triplydb-js/#examples_1","text":"The following snippet prints the contact email for the TriplyDB server: console.log((await triply.getInfo()).contactEmail) The following snippet returns an object describing the used TriplyDB server: console.log(await triply.getInfo())","title":"Examples"},{"location":"triplydb-js/#appgetorganizationname-string","text":"Returns the TriplyDB organization with the given name . This method is similar to App.getAccount(name?: string) , but differs in the following ways: This method only works for accounts that represent TriplyDB organizations. This method returns an organization object. Class Organization is a specialization of class Account .","title":"App.getOrganization(name: string)"},{"location":"triplydb-js/#examples_2","text":"The following snippet returns the organization called 'Triply' : const organization = await triply.getOrganization('Triply') See class Organization for an overview of the methods that can be used with organization objects.","title":"Examples"},{"location":"triplydb-js/#alternatives","text":"This method is a shorthand for calling the following two methods: Call method App.getAccount(name?: string) to retrieve an account object. Then call method Account.asOrganization() to cast the account object into an organization object. The following snippet returns the same result as the previous example, but uses two methods instead of one: const account = await triply.getAccount('Triply') const organization = account.asOrganization()","title":"Alternatives"},{"location":"triplydb-js/#see-also_1","text":"This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/#appgetusername-string","text":"Returns the TriplyDB user with the given name . If name is omitted, the TriplyDB user that is associated with the current API token is returned. This only works if an API token is configured for the current App object.","title":"App.getUser(name?: string)"},{"location":"triplydb-js/#examples_3","text":"The following snippet returns the user with name 'somebody' : const user = await triply.getUser('somebody') The following snippet returns the user for whom the API token was created. This only works if an API token was configured when the App object was created: const me = await triply.getUser()","title":"Examples"},{"location":"triplydb-js/#alternatives_1","text":"This method is a shorthand for the following two methods: Call method App.getAccount() to retrieve an account object. Then call method Account.asUser() to cast the account object into a user object. The following snippet returns the same result as the previous examples, but uses two methods instead of one: const account = await triply.getAccount('somebody') const user = account.asUser()","title":"Alternatives"},{"location":"triplydb-js/#see-also_2","text":"This method returns a user object. See class User for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/#appiscompatiblewithminimumversion-string","text":"Succeeds if and only if the currently connected to TriplyDB server has a version that is identical to or higher than the given minimum version.","title":"App.isCompatibleWith(minimumVersion: string)"},{"location":"triplydb-js/#arguments","text":"Argument minimumVersion must be a string that uses Semantic Versioning. For example '1.2.3' .","title":"Arguments"},{"location":"triplydb-js/#see-also_3","text":"To inspect the current version of the connected-to TriplyDB server, use App.getInfo() .","title":"See also"},{"location":"triplydb-js/#account","text":"Instances of the Account class denote TriplyDB accounts. Accounts can be either organizations ( Organization ) or users ( User ). Account objects are obtained by calling the following method: App.getAccount(name?: string)","title":"Account"},{"location":"triplydb-js/#accountadddatasetname-string-metadata-object","text":"Adds a new TriplyDB dataset with the given name to the current account. The optional metadata argument is used to specify the metadata for the dataset.","title":"Account.addDataset(name: string, metadata?: object)"},{"location":"triplydb-js/#access-restrictions","text":"Creating a new dataset only succeeds if an API token is configured that provides write access to the current account. The default access level for a newly created dataset is private . If you want to publish a dataset with a different access level, you must specify the accessLevel key in the metadata argument.","title":"Access restrictions"},{"location":"triplydb-js/#arguments_1","text":"The name argument specifies the URL-friendly name of the new dataset. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The full URL of the newly created dataset has the following structure: https://{host}/{account}/{dataset} The metadata argument optionally specifies the access level and other important metadata: accessLevel The access level of the dataset. The following values are supported: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. When no access level is specified, the most restrictive access level ( private ) is used. description The human-readable description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL' 'None' (default) prefixes The IRI prefix declarations that are configured for the dataset. This is specified as a dictionary object whose keys are aliases and whose values are IRI prefixes.","title":"Arguments"},{"location":"triplydb-js/#examples_4","text":"The following snippet creates a new dataset called 'iris' under the account called 'Triply' : The dataset has private access, because the access level is not specified explicitly. The dataset has a description. The dataset has a display name. The dataset has the PDDL license. const account = await triply.getAccount('Triply') const dataset = await account.addDataset('iris', { description: 'A multivariate dataset that quantifies morphologic variation of Iris flowers.', displayName: 'Iris', license: 'PDDL', name: 'iris', prefixes: { def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', }, })","title":"Examples"},{"location":"triplydb-js/#see-also_4","text":"This method returns a dataset object. See the Dataset section for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/#accountaddqueryname-string-metadata-object","text":"Adds a new SPARQL query to the account.","title":"Account.addQuery(name: string, metadata: object)"},{"location":"triplydb-js/#arguments_2","text":"Required: name: string The URL-friendly name of the new query. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). queryString: string The SPARQL query string (e.g., 'select * { ?s ?p ?o }' ). dataset: Dataset An instance of class Dataset that the current API token gives access to. or service: Service An instance of class Service that the current API token gives access to and that you want to be associated with this query. The Service given will be used as a preferred service for this query. Optional: The metadata argument specifies the required Dataset or Service and access level. Other important metadata can be set optionally: accessLevel The access level of the query. If none is set it defaults to 'private' . The following values are supported: 'private' The query can only be accessed by the Account object for which it is created. 'internal' The query can only be accessed by people who are logged into the TriplyDB server. 'public' The query can be accessed by everybody. description: string A human-readable description of the query. displayName: string The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name . output: string The visualization plugin that is used to display the result set of the query. If none is set it defaults to 'table' . 'boolean' The [boolean](https://triply.cc/docs/yasgui#table) view is a special view for ask queries. The value is either 'true' or 'false', and is visualized as `X` (False) or `V` (True). 'gallery' The [gallery](https://triply.cc/docs/yasgui#gallery) view allows SPARQL results to be displayed in an HTML gallery. 'gchart' The [gchart](https://triply.cc/docs/yasgui#charts) renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. 'geo' The [geo](https://triply.cc/docs/yasgui#geo) allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map. 'geoEvents' The [geoEvents](https://triply.cc/docs/yasgui#geoEvents) plugin renders geographical events as a story map. 'geo3d' The [geo3d](https://triply.cc/docs/yasgui#geo3d) allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. 'markup' The [markup](https://triply.cc/docs/yasgui#markup) can be used to render a variety of markup languages. This requires the use of the `?markup` variable to identify which variable to render. 'network' The [network](https://triply.cc/docs/yasgui#network) renders SPARQL Construct results in a graph representation. The maximum amount of results that can be visualized is 1.000 due to performance. 'pivot' The [pivot](https://triply.cc/docs/yasgui#pivot) view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows. 'response' The [response](https://triply.cc/docs/yasgui#response) view shows the body of the response and offers a easy way to download the result as a file. 'table' The [table](https://triply.cc/docs/yasgui#table) view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. 'timeline' The [timeline](https://triply.cc/docs/yasgui#timeline) timeline renders the SPARQL results on a Timeline. variables: Variable[] A list of objects with the following keys: IRI variable An object of the form Variable (see below) Instances of Variable are objects that can have the following keys: Required: name: string A SPARQL variable name. The variable name must appear in the query string. The question mark ( ? ) or dollar sign ( $ ) is not included. termType: 'Literal'|'NamedNode' The kind of variable. This must be either 'Literal' for literals or 'NamedNode' for IRIs. Optional: allowedValues: string[] The list of string values that is allowed for this variable. datatype: string (if termType='Literal' ) The datatype IRI for the literal variable. language: string (if termType='Literal' ) The language tag for the literal variable. Setting this implies that the dataset IRI is rdf:langString . defaultValue: string The default string value for the required: boolean Whether a query request must include an explicit value for this variable. The default value is false .","title":"Arguments"},{"location":"triplydb-js/#example_1","text":"The following snippet creates a query with the given query string: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const myDataset = await user.getDataset('my-dataset') const query = await user.addQuery('my-query', { dataset: myDataset, queryString: 'select (count(*) as ?n) { ?s ?p ?o. }', output: 'response', })","title":"Example"},{"location":"triplydb-js/#accountaddstoryname-string-metadata-object","text":"Adds a new data story.","title":"Account.addStory(name: string, metadata?: object)"},{"location":"triplydb-js/#required","text":"name: string The URL-friendly name of the data story. The name must only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ).","title":"Required"},{"location":"triplydb-js/#optional","text":"accessLevel The access level of the dataset. If none is given the default of 'private' is used. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. content: StoryElementUpdate[] A list of story elements. The building blocks of the Story. displayName: string The human-readable name of the data story. This name may include spaces and other characters that are not allowed in the URL-friendly name. A story element is an object with the following keys: caption: string The caption is an explanatory text about a specific query. id: string Each Story element gets an Id when it is created. When you want to update a Story element you will need this Id. The Id is only required when updating an element and not needed when adding an object. paragraph: string The Markdown content of a story paragraph. Only allowed when the type is set to 'paragraph' query: Query An instance of class Query . queryVersion: number The version that is used of the specified query. type Either 'paragraph' or 'query' .","title":"Optional"},{"location":"triplydb-js/#examples_5","text":"Example 1 - creates a new story that has access level 'private' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story') Example 2 - creates a new story that has access level 'public' : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const newStory = await user.addStory('name-of-story', { accessLevel: 'public', })","title":"Examples"},{"location":"triplydb-js/#accountasorganization","text":"Casts the TriplyDB account object to its corresponding organization object. Class Organization is a specialization of class Account . Calling this method on an Organization object does nothing.","title":"Account.asOrganization()"},{"location":"triplydb-js/#examples_6","text":"The following snippet retrieves the account named 'Triply' and casts it to an organization: const account = await triply.getAccount('Triply') const organization = account.asOrganization()","title":"Examples"},{"location":"triplydb-js/#alternatives_2","text":"This method is not needed if the organization is directly retrieved with the specialization method App.getOrganization(name: string) . The following snippet returns the same result as the above example, but in a more direct way: const organization = await triply.getOrganization('Triply')","title":"Alternatives"},{"location":"triplydb-js/#see-also_5","text":"This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/#accountasuser","text":"Casts the TriplyDB account object to its corresponding user object. Class User is a specialization of class Account . Calling this method on a User object does nothing.","title":"Account.asUser()"},{"location":"triplydb-js/#examples_7","text":"The following snippet retrieves the account that represents the current user, and casts it to a user object: const account = await triply.getAccount() const user = account.asUser()","title":"Examples"},{"location":"triplydb-js/#alternatives_3","text":"This method is not needed if the user is directly retrieved with the specialization method App.getUser(name?: string) . The following snippet returns the same result as the above example, but in a more direct way: const user = await triply.getUser()","title":"Alternatives"},{"location":"triplydb-js/#see-also_6","text":"This method returns an organization object. See class Organization for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/#accountensuredatasetname-string-metadata-object","text":"Ensures the existence of a dataset with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a dataset with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a dataset, and conditionally create a new dataset or make metadata changes to an existing dataset. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a dataset with the given name , then the behavior is identical to calling Account.addDataset(name: string, metadata?: object) with the same arguments. If this account already has a dataset with the given name and with the same metadata , then this method makes no changes.","title":"Account.ensureDataset(name: string, metadata?: object)"},{"location":"triplydb-js/#example_2","text":"const account = await triply.getAccount('Triply') const myDataset = await account.ensureDataset(`my-dataset`, { license: 'PDDL', })","title":"Example"},{"location":"triplydb-js/#see-also_7","text":"The meaning of the argument to this method are identical to those of the Account.addDataset(name: string, metadata?: object) method.","title":"See also"},{"location":"triplydb-js/#accountgetdatasetname-string","text":"Returns the dataset with the given name that is published by this account.","title":"Account.getDataset(name: string)"},{"location":"triplydb-js/#examples_8","text":"The following snippet prints the name of the Iris dataset that is published by the Triply account: const account = await triply.getAccount('Triply') const dataset = await triply.getDataset('iris') console.log((await dataset.getInfo()).name)","title":"Examples"},{"location":"triplydb-js/#see-also_8","text":"This method returns a dataset object. See class Dataset for an overview of the methods that can be called on such objects.","title":"See also"},{"location":"triplydb-js/#accountgetdatasets","text":"Returns an async iterator over the accessible datasets for the current account.","title":"Account.getDatasets()"},{"location":"triplydb-js/#access-restrictions_1","text":"The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public datasets belonging to this account. If an API token is configured, the iterator will include all public and internal datasets belonging to this account, and will include all private datasets belonging to this account if the API token gives read access to the account.","title":"Access restrictions"},{"location":"triplydb-js/#examples_9","text":"The following snippet prints the names of all accessible dataset under the Triply account: ts const account = await triply.getAccount('Triply') for await (const dataset of account.getDatasets()) { console.log((await dataset.getInfo()).name) } The following snippet prints the list of names of all accessible datasets under the Triply account: ts const account = await triply.getAccount('Triply') console.log(await account.getDatasets().toArray())","title":"Examples"},{"location":"triplydb-js/#accountgetinfo","text":"Returns information about this account. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for accounts includes the following keys: avatarUrl A URL to the account image. accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. createdAt The date and time on which the account was created. datasetCount The number of datasets for the account. queryCount The number of queries for the account. storyCount The number of stories for the account pinnedDatasets An array containing the pinned dataset for the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. type The account type: either organization or user . role The role of the account orgs An array of organizations of which the account is a member. Email address The email address of the account. updatedAt The date and time on which the account was last updated. lastActivity The date and time on which the account was last online on TriplyDB.","title":"Account.getInfo()"},{"location":"triplydb-js/#examples_10","text":"The following snippet prints the full information object for the account called \u2018Triply\u2019: ts const account = await triply.getAccount('Triply') console.log(await account.getInfo()) The output for this snippet can look as follows: json { 'accountName': 'Triply', 'avatarUrl': 'https://www.gravatar.com/avatar/9bc28997dd1074e405e1c66196d5e117?d=mm', 'createdAt': 'Mon Mar 19 2018 14:39:18 GMT+0000 (Coordinated Universal Time)', 'datasetCount': 16, 'name': 'Triply', 'queryCount': 37, 'storyCount': 7, 'type': 'org', 'updatedAt': 'Tue Nov 27 2018 09:29:38 GMT+0000 (Coordinated Universal Time)' } The following snippet prints the name of the account called \u2018Triply\u2019: ts const account = await triply.getAccount('Triply') console.log((await account.getInfo()).name)","title":"Examples"},{"location":"triplydb-js/#accountgetpinneditems","text":"Returns the list of datasets, stories and queries that are pinned for the current account. A pinned item is an item that is displayed in a prominent way on the account web page.","title":"Account.getPinnedItems()"},{"location":"triplydb-js/#order-considerations","text":"The order in which the pinned datasets are returned reflects the order in which they appear on the organization homepage (from top-left to bottom-right).","title":"Order considerations"},{"location":"triplydb-js/#examples_11","text":"The following snippet prints the names of the items that are pinned on the Triply account page: const account = await triply.getAccount('Triply') for await (const item of account.getPinnedItems()) { console.log((await item.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/#see-also_9","text":"This method returns various types of objects. Each class has different functionalities: See class Dataset for an overview of the methods for dataset objects. See class Query for an overview of the methods for query objects. See class Story for an overview of the methods for story objects.","title":"See also"},{"location":"triplydb-js/#accountgetqueryname-string","text":"Returns the TriplyDB query with the given name .","title":"Account.getQuery(name: string)"},{"location":"triplydb-js/#examples_12","text":"The following snippet prints the query string for a query called animal-gallery that belongs to the account called Triply : const account = await triply.getAccount('Triply') const query = await account.getQuery('animal-gallery') console.log((await query.getInfo()).requestConfig?.payload.query)","title":"Examples"},{"location":"triplydb-js/#see-also_10","text":"See class Query for an overview of the methods for query objects.","title":"See also"},{"location":"triplydb-js/#accountgetqueries","text":"Returns an async iterator over the accessible queries that belong to the account.","title":"Account.getQueries()"},{"location":"triplydb-js/#access-restrictions_2","text":"The iterator only includes datasets that are accessible for the current connection with a TriplyDB server: If no API token is configured, the iterator will include all and only public queries belonging to this account. If an API token is configured, the iterator will include all public and internal queries that belong to this account, and will include all private queries that belong to this account if the API token gives read access to the account.","title":"Access restrictions"},{"location":"triplydb-js/#examples_13","text":"The following snippet prints the names of the queries that belong to the account called Triply : const account = await triply.getAccount('Triply') for await (const query of account.getQueries()) { console.log((await query.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/#see-also_11","text":"See class Query for an overview of the methods for query objects.","title":"See also"},{"location":"triplydb-js/#accountensurestoryname-string-metadata-object","text":"Ensures the existence of a story with the given name and with the specified metadata , if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a story with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a story, and conditionally create a new story or make metadata changes to an existing story. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this account does not yet have a story with the given name , then the behavior is identical to calling Account.addStory(name: string, metadata?: object) with the same arguments. If this account already has a story with the given name and with the same metadata , then this method returns that story.","title":"Account.ensureStory(name: string, metadata: object)"},{"location":"triplydb-js/#optional_1","text":"displayName Accepts a string value to be used as the display name for the story. accessLevel Accepts either of the following values: 'private' (default), 'internal' , 'public' . content Accepts a list of StoryElementUpdate objects, defined below. Note: If no accessLevel is specified, the default used is 'private'. Examples Example 1: To ensure a Story only requires a name of type string. It's access level will default to private await someUser.ensureStory(`someStoryName`) Example 2: Ensure a Story setting it's accessLevel and displayName . await someUser.ensureStory(`someStoryName`, { accessLevel: 'public', displayName: `This is a Story`, })","title":"Optional"},{"location":"triplydb-js/#accountaddstoryname-string-newstoryoptions-object","text":"","title":"Account.addStory(name: string, newStoryOptions?: object)"},{"location":"triplydb-js/#required_1","text":"Adds and returns the TriplyDB story with the given name .","title":"Required"},{"location":"triplydb-js/#optional_2","text":"The optional new story object that can be passed accepts the following properties: displayName Accepts a string value to be used as a display name for the story accessLevel Sets the access level for the story. Accepts either of the following: 'private' (default), 'internal' , 'public' . If no accesslevel is specified, the default value private is used. Examples : Example 1 - creates a newStory that is 'private' const newStory = await someUser.addStory('name-of-story') Example 2 - creates a newStory that is 'public' const newStory = await someUser.addStory('name-of-story', { accessLevel: 'public', })","title":"Optional"},{"location":"triplydb-js/#accountgetstoryname-string","text":"Returns the TriplyDB story with the given name .","title":"Account.getStory(name: string)"},{"location":"triplydb-js/#examples_14","text":"The following snippet prints the paragraphs in the story called the-iris-dataset that is published under the account called Triply . Stories are sequences of paragraphs and queries. This program prints the paragraphs in the sequence in which they appear in the story. const account = await triply.getAccount('Triply') const story = await account.getStory('the-iris-dataset')","title":"Examples"},{"location":"triplydb-js/#see-also_12","text":"See class Story for an overview of the methods for story objects.","title":"See also"},{"location":"triplydb-js/#accountgetstories","text":"Returns an iterator with the TriplyDB stories that belong to the account.","title":"Account.getStories()"},{"location":"triplydb-js/#examples_15","text":"The following snippet prints the names of the queries that belong to the Triply account: const account = await triply.getAccount('Triply') for await (const story of account.getStories()) { console.log((await story.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/#see-also_13","text":"See class Story for an overview of the methods for story objects.","title":"See also"},{"location":"triplydb-js/#accountpinitemsitems-arraydatasetstoryquery","text":"Pins the given datasets, stores, and/or queries to the home page of this account. The pinned elements can be seen by people who visit the account online. They are also included in the account metadata. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const query = await user.getQuery('name-of-query') const newStory = await user.getStory('name-of-story') user.pinItems([query,newStory])","title":"Account.pinItems(items: array[Dataset|Story|Query])"},{"location":"triplydb-js/#accountsetavatarfile-string","text":"Sets a new image that characterizes this account. A circular version of this image is displayed inside the TriplyDB GUI. This image is also published as part of account metadata.","title":"Account.setAvatar(file: string)"},{"location":"triplydb-js/#examples_16","text":"The following snippet uploads the local image in file logo.svg and set it as the characterizing image for the Triply account: const account = await triply.getAccount('Triply') await account.setAvatar('logo.svg')","title":"Examples"},{"location":"triplydb-js/#accountupdatemetadata-object","text":"Updates the metadata for this account. To update the metadata profile with information within the metadata itself, we need the following steps: Obtain the relevant piece of information as a variable/const: getObject() Update the metadata profile with the obtained information stored in the variable/const: update() getObject() Define a constant ( const ) and assign it to ctx.store.getObjects() . The arguments for the function will be the subject, predicate, and graph. The function retrieves the object so the other 3 parts of a quad need to be specified. update() Update the relevant part of the metadata profile with the corresponding piece of information. .update({}) Example If one wants to update the display name of a metadata profile with the object of the following triple within the metadata: <https://example.org/example> <https://schema.org/name> 'Example Name'@en async (ctx) => { // Fetch displayName const displayName = ctx.store .getObjects( 'https://example.org/example', 'https://schema.org/name', graph.metadata ) .find( (node) => node.termType === 'Literal' && node.language === 'en' )?.value; // Specify the environment variable, if necessary const _dataset = process.env['MODE'] === 'Production' ? (await app.triplyDb.getOrganization(organization)).getDataset(dataset) : (await app.triplyDb.getUser()).getDataset(organization + '-' + dataset) // Update the display name if (displayName) await (await _dataset).update({ displayName }) }; The metadata object for accounts can include the following keys: accountName The URL-friendly name of the account. name The human-readable display name of the account description The human-readable description of the account. pinnedItems An array containing the pinned items (datasets, stories and queries) for the account. Email address The email address of the account.","title":"Account.update(metadata: object)"},{"location":"triplydb-js/#asset","text":"Not all data can be stored as RDF data. For example images and video files use a binary format. Such files can also be stored in TriplyDB as Assets and can be integrated into the Knowledge Graph. Each asset has a specific identifier that can be used in the Knowledge Graph. An asset is always uploaded per dataset, for which the function uploadAsset() is used. see Dataset.uploadAsset() for uploading an asset. If the asset already has been created following functions can retrieve it from the dataset. - Dataset.getAsset(assetName: string, versionNumber?: number) - Dataset.getAssets() TriplyDB.js supports several functions to manipulate an asset on TriplyDB.","title":"Asset"},{"location":"triplydb-js/#assetaddversionpath-file-string","text":"Update an asset with a new version of the document using the addVersion function. The input of this function is a path to the file location that you want to update the asset with. The file you want to add as a new version does not in any ways have to correspond to the asset.","title":"Asset.addVersion(path: File | string)"},{"location":"triplydb-js/#example_3","text":"The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.addVersion('my-file.pdf')","title":"Example"},{"location":"triplydb-js/#assetdelete","text":"To delete an asset with all of its versions execute the delete() function.","title":"Asset.delete()"},{"location":"triplydb-js/#example_4","text":"The following snippet uploads the an file my-file.pdf and upload it as the new version of the asset: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') await asset.delete()","title":"Example"},{"location":"triplydb-js/#assetgetinfoversion-number","text":"Returns information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. Optionally you can give the version number to retrieve the assetInfo of a particular version. The information object for assets includes the following keys: assetName The URL-friendly name of the asset. identifier The hexadecimal identifier of the asset createdAt The date and time on which the asset was created. url The url of the asset. versions An array containing all versions of the asset. uploadedAt The date and time on which the asset was uploaded. fileSize Number with the bytesize of the asset","title":"Asset.getInfo(version?: number)"},{"location":"triplydb-js/#examples_17","text":"The following snippet prints the full information object for the asset called \u2018my-asset\u2019: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getInfo())","title":"Examples"},{"location":"triplydb-js/#assetgetversioninfoversion-number","text":"Returns version specific information about this asset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The version specific information object for assets includes the following keys: id The hexadecimal identifier of the asset fileSize Number with the bytesize of the asset url The url of the asset. uploadedAt The date and time on which the asset was uploaded.","title":"Asset.getVersionInfo(version: number)"},{"location":"triplydb-js/#examples_18","text":"The following snippet prints the version information object for the asset called \u2018my-asset\u2019 at version 1 : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') console.log(await asset.getVersionInfo(1))","title":"Examples"},{"location":"triplydb-js/#assetselectversionversion-number","text":"With the selectVersion() function you can select a specific version of an Asset. Each version corresponds to a iteration of the file that is added as an asset. The argument of the selectVersion() function is a number of the version you want to retrieve.","title":"Asset.selectVersion(version: number)"},{"location":"triplydb-js/#example_5","text":"To select the first asset from the list of assets use the selectVersion with the argument 1 . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') const versionedAsset = asset.selectVersion(1)","title":"Example"},{"location":"triplydb-js/#assettofilepath-string-version-number","text":"The binary representation of an asset can be retrieved and written to file via the asset.toFile() function. This function takes as input a string path to the download location and optionally a version number.","title":"Asset.toFile(path: string, version?: number)"},{"location":"triplydb-js/#example_6","text":"To download the latest version of my-asset asset to the file my-file-location.txt . const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toFile('my-file-location.txt')","title":"Example"},{"location":"triplydb-js/#assettostreamversion-number","text":"If instead of downloading the asset to a file for later usage you want to directly use the asset. The toStream() functionality is available. This downloads the asset as a stream for use in a script. The toStream() has as optional argument a version number.","title":"Asset.toStream(version?: number)"},{"location":"triplydb-js/#example_7","text":"To get the latest version of my-asset asset as a stream available. const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('my-dataset') const asset = await dataset.getAsset('my-asset') asset.toStream()","title":"Example"},{"location":"triplydb-js/#dataset","text":"The Dataset class represents a TriplyDB dataset.","title":"Dataset"},{"location":"triplydb-js/#datasetaddprefixesprefixes-object","text":"Adds IRI prefix declarations to the dataset. The prefixes argument is a dictionary object whose keys are aliases and whose values are IRI prefixes.","title":"Dataset.addPrefixes(prefixes: object)"},{"location":"triplydb-js/#examples_19","text":"The following snippet adds prefix declarations for aliases id and def to the Iris dataset: const organization = await triply.getOrganization('Triply') const dataset = await organization.getDataset(iris) await dataset.addPrefixes({ def: 'https://triplydb.com/Triply/iris/def/', id: 'https://triplydb.com/Triply/iris/id/', })","title":"Examples"},{"location":"triplydb-js/#datasetensureservicename-string-metadata-object","text":"Ensures the existence of a service with the given name and with the specified metadata if given. Calling this method ensures that the necessary changes (if any) are made in the connected-to TriplyDB server that result in an end state in which a service with the given name and metadata exists. This method is useful in practice, because it removes the burden on the programmer to have to write custom code for checking for the existence of a service, and conditionally create a new service or make metadata changes to an existing service. The changes made as a result of calling this method depend on the current state of the connected-to TriplyDB server: If this dataset does not yet have a service with the given name , then the behavior is identical to calling Dataset.addService(name: string, metadata?: object) with the same arguments. If this dataset already has a service with the given name , but with different metadata specified for it, then the behavior is identical to calling Account.getDataset(name: string) and Dataset.update(metadata: object) . If this dataset already has a service with the given name and with the same metadata , then this method returns that service.","title":"Dataset.ensureService(name: string, metadata?: object)"},{"location":"triplydb-js/#required_2","text":"name Accepts a string value which is the name of the service to ensure.","title":"Required"},{"location":"triplydb-js/#optional-metadata","text":"serviceMetadata = { type: 'elasticsearch' | 'virtuoso' | 'jena' ; config?: { reasoner?: 'OWL' | 'RDFS' | 'None'; }; }; type Accepts a string value of one of the following: 'virtuoso' , 'elasticsearch' , 'jena' . config Config is an optional property. It accepts an object with a reasoner property. reasoner The reasoner property accepts a string value of either 'OWL' , 'RDFS' , or 'None' . Note: If no options are specified the default service is of type: virtuoso . Note that the config.reasoner will only accept a value when type is: 'jena' Examples Example 1: Ensure a service with no arguments. If not found it's type defaults to virtuoso . await someDataset.ensureService('someServiceName') Example 2: Ensure a service of type jena . await someDataset.ensureService('someServiceName', { type: 'jena' })","title":"Optional: metadata"},{"location":"triplydb-js/#datasetaddservicename-string-metadata-object","text":"Creates a new service for this dataset.","title":"Dataset.addService(name: string, metadata?: object)"},{"location":"triplydb-js/#arguments_3","text":"","title":"Arguments"},{"location":"triplydb-js/#required_3","text":"name The URL-friendly name of the service. The name must only contain alphanumeric characters and hyphens (`[A-Za-z0-9\\-]`).","title":"Required"},{"location":"triplydb-js/#optional_3","text":"The service type is specified with the type parameter. If no type is given, a default of 'virtuoso' is used. It supports the following values: 'virtuoso' Starts a SPARQL service. A SPARQL 1.1 compliant service is very scalable and performance, but without advanced reasoning capabilities. 'jena' Starts a SPARQL JENA service. A SPARQL 1.1 compliant service that is less scalable and less performant, but allows reasoning (RDFS or OWL) to be enabled. 'elasticSearch' Starts an Elasticsearch service. A text search engine that can be used to power a search bar or similar textual search API. The name argument can be used to distinguish between different endpoints over the same dataset that are used for different tasks.","title":"Optional"},{"location":"triplydb-js/#examples_20","text":"The following snippet starts two SPARQL endpoints over a specific dataset. One endpoint will be used in the acceptance environment while the other endpoint will be used in the production system. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const acceptance = await dataset.addService('acceptance') const production = await dataset.addService('production', { type: 'elasticsearch', }) const reasoning = await dataset.addService('reasoning', { type: 'jena', config: { reasoner: 'OWL' }, })","title":"Examples"},{"location":"triplydb-js/#see-also_14","text":"See class Service for an overview of the methods that can be used with service objects.","title":"See also"},{"location":"triplydb-js/#datasetclearresourcetype-string","text":"Removes one or more resource types from the current dataset.","title":"Dataset.clear(...resourceType: string)"},{"location":"triplydb-js/#arguments_4","text":"The resources are specified by the rest parameter resourceType , which supports the following values : 'assets' Removes all assets in the dataset. 'graphs' Removes all graphs in the dataset. 'services' Removes all services in the dataset.","title":"Arguments"},{"location":"triplydb-js/#examples_21","text":"The following example code removes all graphs and services for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.clear('graphs', 'services')","title":"Examples"},{"location":"triplydb-js/#datasetcopyaccount-string-dataset-string","text":"Creates a copy of the current dataset. The owner (user or organization) of the copy is specified with parameter account . The name of the copy is specified with parameter dataset . This operation does not overwrite existing datasets: if the copied-to dataset already exists, a new dataset with suffix -1 will be created.","title":"Dataset.copy(account: string, dataset: string)"},{"location":"triplydb-js/#examples_22","text":"const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.copy('account name', 'copy dataset name'))","title":"Examples"},{"location":"triplydb-js/#datasetdelete","text":"Deletes the dataset. This includes deleting the dataset metadata, all of its graphs, all of its services, and all of its assets.","title":"Dataset.delete()"},{"location":"triplydb-js/#examples_23","text":"The following snippet deletes a specific dataset that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.delete()","title":"Examples"},{"location":"triplydb-js/#see-also_15","text":"Sometimes it is more useful to only delete the graphs that belong to a dataset, but leave the dataset metadata, services, and assets in place. The following methods can be used for this purpose: Dataset.deleteGraph(graphName: string) Dataset.removeAllGraphs()","title":"See also"},{"location":"triplydb-js/#datasetdeletegraphname-string","text":"Deletes the graph with the given name from this dataset. Graph names are IRIs.","title":"Dataset.deleteGraph(name: string)"},{"location":"triplydb-js/#examples_24","text":"The following snippet deletes a specific graph from a specified dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.deleteGraph('https://example.org/some-graph')","title":"Examples"},{"location":"triplydb-js/#datasetdescribeiri-stringnamednode","text":"Each dataset is a collection of triples that describe objects in linked data. Each object is defined with an IRI, an identifier for that object. An object often has incoming and outgoing connections. The Dataset.describe() call can retrieve the incoming and outgoing triples per object. The function returns for a given iri a list of quads where the iri is either in the subject or the object position.","title":"Dataset.describe(iri: string|NamedNode)"},{"location":"triplydb-js/#examples_25","text":"The following snippet returns all triples that have https://example.org/id/some-instance in the subject or the object position: const user = await triply.getUser() const dataset = await account.getDataset('my-dataset') console.log(await dataset.describe('https://example.org/id/some-instance'))","title":"Examples"},{"location":"triplydb-js/#datasetgetassetname-string-version-number","text":"Returns the asset with the given name for this dataset. Optionally allows the version number ( version ) of the asset to be specified. If the version number is absent, the latest version of the assert with the given name is returned.","title":"Dataset.getAsset(name: string, version?: number)"},{"location":"triplydb-js/#examples_26","text":"The following snippet returns the original version of an image of a dog from the animals dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') const asset = await dataset.getAsset('file.png', 1)","title":"Examples"},{"location":"triplydb-js/#datasetgetassets","text":"Returns an async iterator over the assets that belong to this dataset. Assets are binary files that are stored together with data graphs. Common examples include documents, images and videos.","title":"Dataset.getAssets()"},{"location":"triplydb-js/#examples_27","text":"The following snippet prints the assets for a specific dataset: const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const asset of dataset.getAssets()) { console.log(asset) } The following snippet prints the list of assets for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getAssets().toArray())","title":"Examples"},{"location":"triplydb-js/#datasetgetgraphname-string","text":"Each dataset with data consists out of one or more named graphs. All graphs together are thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. Instead of searching over the complete dataset where you want to scope it to a certain graph you can use the getGraph() function to specify the graph. Dataset.getGraph(name: string) returns the graph with the given name that belongs to this dataset. The name is the string representation of the graph IRI. The Dataset.getGraph returns a graph object.","title":"Dataset.getGraph(name: string)"},{"location":"triplydb-js/#examples_28","text":"The following snippet returns the graph about cats from the dataset about animals: const user = await triply.getUser() const dataset = await user.getDataset('animals') const graph = dataset.getGraph('https://example.com/cats')","title":"Examples"},{"location":"triplydb-js/#datasetgetgraphs","text":"Returns an async iterator over graphs that belong to this dataset.","title":"Dataset.getGraphs()"},{"location":"triplydb-js/#examples_29","text":"The following snippet retrieves the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getGraphs().toArray())","title":"Examples"},{"location":"triplydb-js/#datasetgetinfo","text":"Returns information about this dataset. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"Dataset.getInfo()"},{"location":"triplydb-js/#examples_30","text":"The following snippet prints the information from the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') console.log(await dataset.getInfo())","title":"Examples"},{"location":"triplydb-js/#datasetgetprefixes","text":"Returns the prefixes that are defined for this dataset. This contains prefix declarations that are generic and configured for this TriplyDB server, and prefix declarations that are defined for this specific dataset.","title":"Dataset.getPrefixes()"},{"location":"triplydb-js/#examples_31","text":"The following snippet prints the prefix declarations that hold for my-dataset : const user = await triply.getUser() const dataset = user.getDataset('my-dataset') for await (const prefix of dataset.getPrefixes()) { console.log(prefix) }","title":"Examples"},{"location":"triplydb-js/#datasetgetservicename-string","text":"Returns the service with the given name for this dataset.","title":"Dataset.getService(name: string)"},{"location":"triplydb-js/#examples_32","text":"The following snippet retrieves the acceptance service for the product catalog of an imaginary company: const organization = triply.getOrganization('some-company') const dataset = organization.getDataset('product-catalog') const service = dataset.getService('acceptance')","title":"Examples"},{"location":"triplydb-js/#datasetgetservices","text":"Returns an async iterator over TriplyDB services under a dataset. See class Service for an overview of the methods for service objects.","title":"Dataset.getServices()"},{"location":"triplydb-js/#examples_33","text":"The following snippet emits the services that are enabled for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') for await (const service of dataset.getServices()) { console.log(service) } If you do not want to iterate over the services with an async iterator, but instead want to get an array of services use the .toArray() call instead: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') console.log(await dataset.getServices().toArray())","title":"Examples"},{"location":"triplydb-js/#datasetgetstatementssubject-string-predicate-string-object-string-graph-string","text":"Returns an async iterator with statements (quadruples) that fit the specified pattern.","title":"Dataset.getStatements({subject?: string, predicate?: string, object?: string, graph?: string})"},{"location":"triplydb-js/#arguments_5","text":"subject , if specified, is the subject term that should be matched. predicate , if specified, is the predicate term that should be matched. object , if specified, is the object term that should be matched. graph , if specified, is the graph name that should be matched.","title":"Arguments"},{"location":"triplydb-js/#example_8","text":"The following prints all statements in the dataset: const user = triply.getUser() const dataset = await user.getDataset('my-dataset') for await (const statement of dataset.getStatements()) { console.log(statement) } The following prints the description of the Amsterdam resource in the DBpedia dataset: const association = triply.getOrganization('DBpedia-association') const dbpedia = association.getDataset('dbpedia') for await (const statement of dbpedia.getStatements({subject: 'http://dbpedia.org/resource/Amsterdam'})) { console.log(statement) }","title":"Example"},{"location":"triplydb-js/#get-the-data-locally","text":"Most of the time you do not need to download the entire dataset locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use the entire graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from TriplyDB. graphsToFile() , graphsToStore() and graphsToStream() .","title":"Get the data locally"},{"location":"triplydb-js/#datasetgraphstofiledestinationpath-string-arguments-object","text":"The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error.","title":"Dataset.graphsToFile(destinationPath: string, arguments?: object)"},{"location":"triplydb-js/#optional_4","text":"The optional properties accepted as arguments for graphsToFile Compressed Argument compressed optionally is a boolean defining if a graph is compressed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension. Graph Argument Graph optionally is an specific graph that you want to write to file. These graph is an instance of a 'Graph' class","title":"Optional"},{"location":"triplydb-js/#examples_34","text":"The following example downloads the dataset to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') await dataset.graphsToFile('my-filename.ttl', {compressed: true})","title":"Examples"},{"location":"triplydb-js/#datasetgraphstostoregraph-graph","text":"The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a graphsToStore() where a N3 store is returned as a result of the graphsToStore() function.","title":"Dataset.graphsToStore(graph?: Graph)"},{"location":"triplydb-js/#optional_5","text":"The optional argument for graphsToStore is Graph . With Graph you can optionally define a specific graph that you want to write to file. These graph is an instance of a 'Graph' class.","title":"Optional"},{"location":"triplydb-js/#examples_35","text":"The following example downloads the dataset as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const store = await dataset.graphsToStore()","title":"Examples"},{"location":"triplydb-js/#datasetgraphstostreamtype-compressed-rdf-js-arguments-object","text":"The final method to download linked data to a local source is the graphsToStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard .","title":"Dataset.graphsToStream(type: 'compressed' | 'rdf-js', arguments?: object)"},{"location":"triplydb-js/#optional_6","text":"The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`. Graph Argument Graph optionally is an specific graph that you want to write to file. This graph is an instance of a 'Graph' class","title":"Optional"},{"location":"triplydb-js/#examples_36","text":"The following example streams through the dataset as rdf-js quad objects and prints the quad to the screen. Notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the dataset as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const stream = await dataset.graphsToStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) }","title":"Examples"},{"location":"triplydb-js/#datasetimportfromdatasetfromdataset-dataset-arguments-object","text":"Imports one or more named graphs from a different dataset into this dataset. Data reuse is an important principle in linked data. This functionality makes it very easy to pull in vocabularies and datasets from other places. Changes in the fromDataset dataset are not automatically reflected in this dataset. If you want to synchronize with changes made in the imported-from dataset, the graphs must be explicitly imported. This protects this dataset against unanticipated changes in the imported-from dataset, while still being able to stay in sync with the imported-from dataset if this is explicitly requested.","title":"Dataset.importFromDataset(fromDataset: Dataset, arguments?: object)"},{"location":"triplydb-js/#required_4","text":"Argument fromDataset is the dataset object from which one or more graphs are imported over to this dataset.","title":"Required"},{"location":"triplydb-js/#optional_7","text":"The optional properties accepted as arguments for importFromDataset graphMap Argument ` graphMap ` optionally is an object with keys and values that implements a mapping from existing graph names (keys) to newly created graph names (values). Each key must be an existing graph name in the `from` dataset. Each value must be the corresponding graph name in this dataset. If this argument is not specified, then graph names in the `from` dataset are identical to graph names in this dataset. Note that either graphNames or graphMap can be given as optional argument and not both. graphNames Argument ` graphNames ` optionally is an array of graph names. These names can be one of three types: 'string', instances of a 'Graph' class, or instances of 'NamedNodes'. Note that either graphNames or graphMap can be given as optional argument and not both. overwrite Accepts a Boolean value. An optional property that determines whether existing graph names in this dataset are allowed to be silently overwritten. If this argument is not specified, then `false` is used as the default value.","title":"Optional"},{"location":"triplydb-js/#examples_37","text":"The following snippet creates a new dataset ( newDataset ) and imports one graph from an existing dataset ( existingDataset ). Notice that the graph can be renamed as part of the import. Example 1 Imports the complete 'existingDataset' dataset to the 'newDataset' . const account = await triply.getAccount() const existingDataset = await account.getDataset('existingDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(existingDataset) Example 2 Imports 'anotherDataset' dataset to a 'newDataset' Where a graph from the existing dataset is renamed to the a graphname in the new dataset. Only the graphs from the graphMap are imported. const account = await triply.getAccount() const anotherDataset = await account.getDataset('anotherDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(anotherDataset, { graphMap: { 'https://example.org/existingDataset/graph': 'https://example.org/newDataset/graph'} }) Example 3 Import 'oneMoreDataset' dataset to the 'newDataset' Where a graph specific graph from the existing dataset is added to the new dataset. If the graph name already occurs in the 'newDataset' it will get overwritten. const account = await triply.getAccount() const oneMoreDataset = await account.getDataset('oneMoreDataset') const newDataset = await account.addDataset('newDataset') await newDataset.importFromDataset(oneMoreDataset, { graphNames: ['https://example.org/existingDataset/graph'], overwrite: true, })","title":"Examples"},{"location":"triplydb-js/#datasetimportfromfilesfiles-liststring-file-defaultsconfig-object","text":"","title":"Dataset.importFromFiles(files: list(string || File), defaultsConfig?: object)"},{"location":"triplydb-js/#required_5","text":"Imports one or more files into this dataset. The files must contain RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported file baseIRI Accepts a string value that is set as the default baseIRI for each imported file overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file)","title":"Required"},{"location":"triplydb-js/#supported-files","text":"The files must contain RDF data and must be encoded in one of the following standardized RDF serialization formats: N-Quads, N-Triples, TriG, Turtle.","title":"Supported files"},{"location":"triplydb-js/#examples_38","text":"Example 1 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz']) Example 2 const account = await triply.getAccount('Triply') const dataset = await account.getDataset(iris) await dataset.importFromFiles('test.nt') await dataset.importFromFiles(['file.nq', 'file.tar.gz'], { defaultGraphName: 'https://triplydb.com/Triply/example/graph/default', overwriteAll: true, })","title":"Examples"},{"location":"triplydb-js/#datasetimportfromstorestore-n3store-defaultsconfig-object","text":"One of the most complete libraries for handling linked data in memory is the n3 library . The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of converting your data from the N3 Store to a file and uploading to TriplyDB. TriplyDB.js has a importFromStore() where a N3 store is given as first argument and uploaded directly to triplyDB.","title":"Dataset.importFromStore(store: n3.Store, defaultsConfig?: object)"},{"location":"triplydb-js/#examples_39","text":"const store = new Store() store.addQuad(DataFactory.namedNode('https://triplydb.com/id/me'),DataFactory.namedNode('http://www.w3.org/2000/01/rdf-schema#label'),DataFactory.literal('me'),DataFactory.namedNode('https://triplydb.com/Triply/example/graph/default')) const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getUser() const dataset = (await user.getDatasets().toArray())[0] dataset.importFromStore(store)","title":"Examples"},{"location":"triplydb-js/#datasetimportfromurlsurls-liststring-defaultsconfig-object","text":"","title":"Dataset.importFromUrls(urls: list(string), defaultsConfig?: object)"},{"location":"triplydb-js/#required_6","text":"Imports one or more URLs into this dataset. The URLs must provide access to RDF data. Optional: defaultsConfig: object defaultGraphName Accepts a string value that is set as the default graph name for each imported URL baseIRI Accepts a string value that is set as the default baseIRI for each imported URL overwriteAll Accepts a boolean value that overwrites previously added graph names or baseIRIs (regardless of whether they came from a URL or a file)","title":"Required"},{"location":"triplydb-js/#examples_40","text":"dataset1.importFromUrls(['url1', 'url2', 'url3'])","title":"Examples"},{"location":"triplydb-js/#datasetremoveallgraphs","text":"Removes all graphs from this dataset.","title":"Dataset.removeAllGraphs()"},{"location":"triplydb-js/#examples_41","text":"The following snippet removed all graphs from a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') await dataset.removeAllGraphs()","title":"Examples"},{"location":"triplydb-js/#datasetremoveprefixesprefixes-string","text":"Removes IRI prefixes from this dataset. The prefixes argument is a string array, containing the prefix labels to be removed.","title":"Dataset.removePrefixes(prefixes: string[])"},{"location":"triplydb-js/#examples_42","text":"The following snippet removes the def and id prefixes from the specified dataset. const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.removePrefixes(['def', 'id'])","title":"Examples"},{"location":"triplydb-js/#datasetrenamegraphfrom-string-to-string","text":"Renames a graph of this dataset, where from is the current graph name and to is the new graph name. The string arguments for from and to must be valid IRIs.","title":"Dataset.renameGraph(from: string, to: string)"},{"location":"triplydb-js/#examples_43","text":"The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.renameGraph( 'https://example.org/old-graph', 'https://example.org/new-graph' )","title":"Examples"},{"location":"triplydb-js/#datasetupdatemetadata-object","text":"Updates the metadata for this dataset.","title":"Dataset.update(metadata: object)"},{"location":"triplydb-js/#arguments_6","text":"The metadata argument takes a dictionary object with the following optional keys: Required: accessLevel The access level of the dataset. The following values are supported: 'private' The dataset can only be accessed by the Account object for which it is created. 'internal' The dataset can only be accessed by people who are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. Optional: description The description of the dataset. This description can make use of Markdown. displayName The human-readable name of the dataset. This name may contain spaces and other characters that are not allowed in the URL-friendly name. license The license of the dataset. The following license strings are currently supported: 'CC-BY-SA' 'CC0 1.0' 'GFDL' 'ODC-By' 'ODC-ODbL' 'PDDL'","title":"Arguments"},{"location":"triplydb-js/#example_9","text":"The following snippet updates the dataset's access level, description, display name and license: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') dataset.update({ accessLevel: 'private', description: 'desc', displayName: 'disp', license: 'PDDL', })","title":"Example"},{"location":"triplydb-js/#datasetuploadassetassetname-string-filepath-string","text":"Uploads a file that does not contain RDF data as an asset.","title":"Dataset.uploadAsset(assetName: string, filePath: string)"},{"location":"triplydb-js/#user-cases","text":"There are several use cases for assets: Source data that will be used as input files to an ETL process. Documentation files that describe the dataset. Media files (audio/image/video) that are described in the RDF graph.","title":"User cases"},{"location":"triplydb-js/#examples_44","text":"The following snippet uploads a source CSV data file and a PDF documentation file: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') await dataset.uploadAsset('my-source-data', 'source.csv.gz') await dataset.uploadAsset('my-documentation', 'documentation.pdf')","title":"Examples"},{"location":"triplydb-js/#graph","text":"Each dataset with data consists out of one or more named graphs. All graphs together is thus the collection of triples of the dataset. Often the graph is used to denote a part of the dataset. For example the data model of the dataset or the metadata of the dataset. A graph has as advantage that is can partition data while at the same time keep the data in the same dataset. Reducing the overhead of having to move between datasets to traverse a graph. You can retrieve either retrieve all graphs from a dataset in the form of an async iterator. Or retrieve a specific graph from a dataset.","title":"Graph"},{"location":"triplydb-js/#examples_45","text":"The following snippet retrieves the graph 'https://example.com/my-graph' for a specific dataset: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') The following snippet retrieves all the graphs for a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graphs = dataset.getGraphs() The Graph is the smallest object that can be individually deleted or modified.","title":"Examples"},{"location":"triplydb-js/#graphdelete","text":"Deletes the graph of this dataset. Any copies of the graph will not be deleted. All services containing this graph will still contain the graph until the service is synced again.","title":"Graph.delete()"},{"location":"triplydb-js/#examples_46","text":"The following snippet deletes a specific graph that is part of the account associated with the current API token: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.delete()","title":"Examples"},{"location":"triplydb-js/#graphgetinfo","text":"Returns information about this graph. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The following keys and values are returned for graph.getInfo() id A hexadecimal hash of the graph to identify the graph for internal identification. graphName The URL-friendly name of the graphName that is used as identifier and name. numberOfStatements The number of statements in the graph. uploadedAt (Optional) The date/time at which the graph was uploaded to TriplyDB. importedAt (Optional) The date/time at which the query was imported from another dataset. importedFrom (Optional) graphName The graphname of the graph from the dataset from which the graph was imported. dataset The dataset from which the graph was imported.","title":"Graph.getInfo()"},{"location":"triplydb-js/#examples_47","text":"The following snippet prints the information from the specified graph of the specified dataset of the current user: const user = await triply.getUser() const dataset = await user.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') console.log(await graph.getInfo())","title":"Examples"},{"location":"triplydb-js/#graphrenamename-string","text":"Renames the graph, the argument name is the new graph name. The string argument for name must be a valid IRI.","title":"Graph.rename(name: string)"},{"location":"triplydb-js/#examples_48","text":"The following snippet renames a specific graph of a specific dataset: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const graph = await dataset.getGraph('https://example.com/my-graph') await dataset.rename('https://example.org/new-graph')","title":"Examples"},{"location":"triplydb-js/#get-the-data-locally_1","text":"Most of the time you do not need to download a graph locally as TriplyDB supports a variety of methods to use linked data directly. But if you want to use a graph locally that is possible with TriplyDB.js . There are three methods to retrieve linked data from a graph. toFile() , toStore() and toStream() .","title":"Get the data locally"},{"location":"triplydb-js/#graphtofiledestinationpath-string-arguments-object","text":"The first method downloads the linked data graphs directly and writes the data to the location of the destinationPath . The extension on the destinationPath defines the linked data type that is downloaded. The extensions that are supported are: nt , nq , trig , ttl , jsonld , json . If no extension is set or the extension is not recognized the function will throw an error.","title":"Graph.toFile(destinationPath: string, arguments?: object)"},{"location":"triplydb-js/#optional_8","text":"The optional properties accepted as arguments for toFile Compressed Argument compressed optionally is an boolean defining if a graph is compresssed with GNU zip (gzip) compression algorithm and will end with a `.gz` extension.","title":"Optional"},{"location":"triplydb-js/#examples_49","text":"The following example downloads the graph to file: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') await graph.toFile('my-filename.ttl', {compressed: true})","title":"Examples"},{"location":"triplydb-js/#graphtostoregraph-graph","text":"The second method is to download the file into a N3.store . The n3 library is one of the most complete libraries for handling linked data in memory. The N3.js library is an implementation of the RDF.js low-level specification that lets you handle RDF in JavaScript easily, with an asynchronous, streaming approach. To reduce the overhead of downloading your data to file and then insert it in the N3 Store. TriplyDB.js has a toStore() where a N3 store is returned as a result of the the toStore() function.","title":"Graph.toStore(graph?: Graph)"},{"location":"triplydb-js/#examples_50","text":"The following example downloads the graph as N3.store : const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const store = await graph.toStore()","title":"Examples"},{"location":"triplydb-js/#graphtostreamtype-compressed-rdf-js-arguments-object","text":"The final method to download linked data to a local source is the toStream this function returns a stream of quads that can directly be iterated over. The Stream is either of the type compressed which returns a gzipped stream of linked data, or type rdf-js which returns a stream of quads parsed according to the rdf-js standard .","title":"Graph.toStream(type: 'compressed' | 'rdf-js', arguments?: object)"},{"location":"triplydb-js/#optional_9","text":"The following arguments can be defined in the optional arguments object. Extension Argument Extension optionally defines the linked data type that is streamed. The extensions that are supported are: `nt`, `nq`, `trig`, `ttl`, `jsonld`, `json`.","title":"Optional"},{"location":"triplydb-js/#examples_51","text":"The following example streams through the graph as rdf-js quad objects. and prints the quad to the screen. notice that the stream is an async iterator. Example 1 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('rdf-js', {extension: '.nq'}) for await(const quad of stream){ console.log(quad) } The following example streams through the graph as chunks of ttl. and prints the buffer to the screen. Example 2 const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('pokemon') const graph = await dataset.getGraph('https://example.com/my-graph') const stream = await graph.toStream('compressed', {extension: '.ttl'}) for await(const quad of stream.pipe(zlib.createGunzip())){ console.log((quad as Buffer).toString()) }","title":"Examples"},{"location":"triplydb-js/#organization","text":"Instances of class Organization denote organizations in TriplyDB.","title":"Organization"},{"location":"triplydb-js/#obtaining-instances","text":"Organizations are obtained with method App.getOrganization(name: string) : const organization = await triply.getOrganization('Triply') Alternatively, organizations are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to an organization ( Account.asOrganization() ): const account = await triply.getAccount('Triply') const organization = account.asOrganization()","title":"Obtaining instances"},{"location":"triplydb-js/#inheritance","text":"Organization is a subclass of Account , from which it inherits most of its methods.","title":"Inheritance"},{"location":"triplydb-js/#organizationadddatasetname-string-metadata-object","text":"Adds a new TriplyDB dataset with the given name to the current organization. Inherited from Account.addDataset(name: string, metadata?: object) .","title":"Organization.addDataset(name: string, metadata?: object)"},{"location":"triplydb-js/#organizationaddmemberuser-user-role-role","text":"Adds a member to the given Organization , with the given role of either member or owner.","title":"Organization.addMember(user: User, role?: Role)"},{"location":"triplydb-js/#arguments_7","text":"The user argument has to be a user object of the user which should be added to the organization. The role argument can be either 'member' or 'owner' . If this argument is not specified, then 'member' is used as the default. 'member' A regular member that is allowed to read and write the datasets that are published under the organization. 'owner' An owner of the organization. Owners have all the rights of regular users, plus the ability to add/remove users to/from the organization, the ability to change the roles of existing users, and the ability to delete the organization.","title":"Arguments"},{"location":"triplydb-js/#examples_52","text":"The following snippet adds user John Doe to the Triply organization as a regular member. const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.addMember(johnDoe)","title":"Examples"},{"location":"triplydb-js/#organizationremovememberuser-user","text":"Removes a member from the given Organization .","title":"Organization.removeMember(user: User)"},{"location":"triplydb-js/#organizationaddqueryname-string-metadata-object","text":"Adds a new TriplyDB query to the current organization. Inherited from Account.addQuery(name: string, metadata: object) .","title":"Organization.addQuery(name: string, metadata: object)"},{"location":"triplydb-js/#organizationensurestoryname-string-metadata-object","text":"Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) .","title":"Organization.ensureStory(name: string, metadata: object)"},{"location":"triplydb-js/#organizationaddstoryname-string-metadata-object","text":"Adds a new TriplyDB story with the given name to the current organization. Inherited from Account.addStory(name: string, metadata?: object) .","title":"Organization.addStory(name: string, metadata?: object)"},{"location":"triplydb-js/#organizationdelete","text":"Deletes this account. This also deletes all datasets, stories and queries that belong to this organization.","title":"Organization.delete()"},{"location":"triplydb-js/#examples_53","text":"The following code example deletes the specified organization: const organization = await triply.getOrganization('Neo4j') await organization.delete()","title":"Examples"},{"location":"triplydb-js/#organizationensuredatasetname-string-metadata-object","text":"Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) .","title":"Organization.ensureDataset(name: string, metadata?: object)"},{"location":"triplydb-js/#organizationgetdatasetname-string","text":"Returns the dataset with the given name that is published by this organization. Inherited from Account.getDataset(name: string) .","title":"Organization.getDataset(name: string)"},{"location":"triplydb-js/#organizationgetdatasets","text":"Returns an async iterator over the accessible datasets that belong to this organization. Inherited from Account.getDatasets() .","title":"Organization.getDatasets()"},{"location":"triplydb-js/#organizationgetmembers","text":"Returns the list of memberships for the given organization.","title":"Organization.getMembers()"},{"location":"triplydb-js/#return-type","text":"A membership contains the following components: role The role of the membership ( OrgRole ): either 'owner' for owners of the organization, or 'member' for regular members. The difference between owners and regular members is that owners can perform user management for the organization (add/remove/change memberships). user An instance of class User . createdAt A date/time string. updatedAt A date/time string.","title":"Return type"},{"location":"triplydb-js/#examples_54","text":"const org = await triply.getOrganization('acme') for (const membership of await org.getMembers()) { console.log(user) }","title":"Examples"},{"location":"triplydb-js/#see-also_16","text":"Memberships of organization are TriplyDB users .","title":"See also"},{"location":"triplydb-js/#organizationgetpinneditems","text":"Returns the list of datasets, stories and queries that are pinned for the current organization. Inherited from Account.getPinnedItems() .","title":"Organization.getPinnedItems()"},{"location":"triplydb-js/#organizationremovememberuser-user_1","text":"Removes the specified user from this organization.","title":"Organization.removeMember(user: User)"},{"location":"triplydb-js/#arguments_8","text":"The user argument has to be a User object of a user.","title":"Arguments"},{"location":"triplydb-js/#existence-considerations","text":"The user must be a current member of the organization for this method to succeed. If the user is not a current member of the organization, an error is thrown.","title":"Existence considerations"},{"location":"triplydb-js/#examples_55","text":"The following snippet removes John Doe from the Triply organization, using a string argument: const organization = await triply.getOrganization('Triply') const johnDoe = await app.getUser('john-doe') await organization.removeMember(johnDoe) The following snippet removes John Doe from the Triply organization, using a User object: const organization = await triply.getOrganization('Triply') const user = await triply.getUser('john-doe') await organization.removeMember(user)","title":"Examples"},{"location":"triplydb-js/#organizationsetavatarfile-string","text":"Sets a new image that characterized this organization. Inherited from Account.setAvatar(file: string) .","title":"Organization.setAvatar(file: string)"},{"location":"triplydb-js/#organizationupdatemetadata-object","text":"Updates the metadata for this account. Inherited from Account.update(metadata: object) .","title":"Organization.update(metadata: object)"},{"location":"triplydb-js/#query","text":"A Saved Query is a versioned SPARQL query with its own URL. Using this URL, users are able to view any version of the query and its results. It can also be used to run the query and retrieve the results from a browser or a program, removing the hassle of figuring out how to run a SPARQL query. Saved queries come with a REST API that can be configured with the use a SPARQL API variables.","title":"Query"},{"location":"triplydb-js/#querydelete","text":"Permanently deletes this query and all of its versions.","title":"Query.delete()"},{"location":"triplydb-js/#querygetinfo","text":"The returned dictionary object includes the following keys: accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). createdAt The date/time at which the query was created. dataset A dictionary object representing the dataset against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. numberOfVersions The number of currently stored versions of this query. owner A dictionary object representing the account (organization or user) to which the query belongs. \ud83d\udea7 link Stores part of the URL to run the query. Please use Query.getRunLink() to obtain the full URL to run the query. service The location of the SPARQL endpoint that is used to run the query. updatedAt The date/time at which the query was last modified.","title":"Query.getInfo()"},{"location":"triplydb-js/#querygetstringapivariables-object","text":"Returns the query string of the current version of this query. Optionally, arguments can be specified for the API variables to this query.","title":"Query.getString(apiVariables?: object)"},{"location":"triplydb-js/#examples_56","text":"The following code stores the SPARQL query string for the query object: const queryString = await query.getString()","title":"Examples"},{"location":"triplydb-js/#queryaddversionmetadata-object","text":"Adds a new version to the query used. It requires similar options to that of Query.addQuery .","title":"Query.addVersion(metadata: object)"},{"location":"triplydb-js/#arguments_9","text":"At least one of the following arguments is required to create a new version. Any argument not given will be copied from the previous version of that query. queryString: string the SPARQL compliant query as a string value output: string The visualization plugin that is used to display the result set. If none is set it defaults to 'table' . Other options may include: 'response' , 'geo' , 'gallery' , 'markup' , etc variables: Variable[] A list of objects with the following keys: IRI variable An object of the form `Variable` (see [`Account.addQuery()`](#accountaddqueryname-string-metadata-object) You can see how many versions exist on a query accessing Query.getInfo().numOfVersions You can use a specified version of a query accessing Query.useVersion(x: number)","title":"Arguments"},{"location":"triplydb-js/#querygetrunlink","text":"Returns the URL link to run the query. It currently does not support the use of variables.","title":"Query.getRunLink()"},{"location":"triplydb-js/#queryresultsapivariables-object-options-object","text":"Query.results() function will automatically return all the results from a saved query. You can retrieve both results from a select or ask query and a construct or describe query. The results are returned as an async iterator . If there are more than 10 000 query results, they could be retrieved using pagination with TriplyDB.js .","title":"Query.results(apiVariables?: object, options?: object)"},{"location":"triplydb-js/#examples_57","text":"Get the results of a query by setting a results variable. More specifically, for construct queries you use the statements() call: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For select queries you use the `statements()` call: const results = query.results().statements() // For select queries you use the `bindings()` call: const results = query.results().bindings() Additionally, saved queries can have 'API variables' that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: const triply = App.get({token: process.env.TOKEN}) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') // For SPARQL construct queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).statements() // For SPARQL select queries. const results = query.results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable' }).bindings()","title":"Examples"},{"location":"triplydb-js/#queryupdatemetadata-object","text":"Updates the metadata for the saved query. This does not result in a new query version. It requires similar options to that of Query.addQuery .","title":"Query.update(metadata: object)"},{"location":"triplydb-js/#arguments_10","text":"At least one of the following arguments is required to update the metadata. Any argument given will be copied from the previous version of that query. accessLevel The access level of the query. The following values are possible: 'private' (default) The dataset can only be accessed by organization members. 'internal' The dataset can only be accessed by users that are logged into the TriplyDB server. 'public' The dataset can be accessed by everybody. autoselectService Whether the SPARQL service is automatically chosen ( true ), or whether a specific SPARQL service is configured ( false ). dataset A dictionary object representing the dataset against which the query is evaluated. description The human-readable description of the query. This typically explains what the query does in natural language. displayName The human-readable name of the query. This name may include spaces and other characters that are not allowed in the URL-friendly name. name The URL-friendly name of the query that is used in URL paths. This name can only include ASCII letters and hyphens. preferredService If the autoselectService is not selected the user can set the preferred service.","title":"Arguments"},{"location":"triplydb-js/#queryuseversionversion-numberlatest","text":"A saved query is saved with a version number. Each time the query or the visualization changes the version number is incremented with one. When you want to retrieve a saved query with a particular version you need the useVersion function. The function returns the query object corresponding to that version of the query. If you want to use the latest version of the query you need to set the version argument to 'latest' .","title":"Query.useVersion(version: number|'latest')"},{"location":"triplydb-js/#example_10","text":"const user = await triply.getAccount('my-account') const query = await user.getQuery('my-query') const query_1 = await query.useVersion(1)","title":"Example"},{"location":"triplydb-js/#service","text":"Service objects describe specific functionalities that can be created over datasets in TriplyDB. Service objects are obtained through the the following methods: Dataset.addService Dataset.getServices A service always has one of the following statuses: Removing The service is being removed. Running The service is running normally. Starting The service is starting up. Stopped The services was stopped in the past. It cannot be used at the moment, but it can be enable again if needed. Stopping The service is currently being stopped.","title":"Service"},{"location":"triplydb-js/#servicedelete","text":"Permanently deletes this service.","title":"Service.delete()"},{"location":"triplydb-js/#examples_58","text":"const user = await triply.getAccount('my-account') const dataset = await user.getDataset('my-dataset') const service = await dataset.addService('my-service') await service.delete()","title":"Examples"},{"location":"triplydb-js/#servicegetinfo","text":"Returns information about this service. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"Service.getInfo()"},{"location":"triplydb-js/#examples_59","text":"The following snippet prints information about the newly created service (named my-service ): const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.addService('my-service') console.log(await service.getInfo())","title":"Examples"},{"location":"triplydb-js/#serviceisuptodate","text":"Returns whether this service is synchronized with the dataset contents.","title":"Service.isUpToDate()"},{"location":"triplydb-js/#synchronization","text":"Because services must be explicitly synchronized in TriplyDB, it is possible to have services that expose an older version of the dataset and services that expose a newer version of the dataset running next to one another. There are two very common use cases for this: The production version of an application or website runs on an older service. The data does not change, so the application keeps working. The acceptance version of the same application or website runs on a newer service. Once the acceptance version is finished, it becomes the production version and a new service for the new acceptance version is created, etc. An old service is used by legacy software. New users are using the newer endpoint over the current version of the data, but a limited number of older users want to use the legacy version.","title":"Synchronization"},{"location":"triplydb-js/#examples_60","text":"The following code checks whether a specific service is synchronized: const account = await triply.getAccount() const dataset = await account.getDataset('my-dataset') const service = await dataset.ensureService('my-service', {type: 'sparql'}) console.log(await service.isUpToDate())","title":"Examples"},{"location":"triplydb-js/#serviceupdate","text":"Synchronizes the service. Synchronization means that the data that is used in the service is made consistent with the data that is present in the graphs of the dataset. When one or more graphs are added or deleted, existing services keep exposing the old state of the data. The changes in the data are only exposed in the services after synchronization is performed.","title":"Service.update()"},{"location":"triplydb-js/#examples_61","text":"When there are multiple services, it is common to synchronize them all in sequence . This ensures that there are always one or more services available. This allows applications to use such services as their backend without any downtime during data changes. The following code synchronizes all services of a dataset in sequence: for (const service of await dataset.getServices()) { service.update() } Although less common, it is also possible to synchronize all services of a dataset in parallel . This is typically not used in production systems, where data changes must not result in any downtime. Still, parallel synchronization can be useful in development and/or acceptance environments. The following code synchronizes all services of a dataset in parallel: await Promise.all(dataset.getServices().map(service => service.update()))","title":"Examples"},{"location":"triplydb-js/#servicewaituntilrunning","text":"A service can be stopped or updated. The use of asynchronous code means that when a start command is given it takes a while before the service is ready for use. To make sure a service is available for querying you can uesr the function waitUntilRunning() to make sure that the script will wait until the service is ready for use.","title":"Service.waitUntilRunning()"},{"location":"triplydb-js/#example_11","text":"An example of a service being updated and afterwards a query needs to be executed: const triply = App.get({ token: process.env.TOKEN }) const user = await triply.getAccount() const dataset = await user.getDataset('some-dataset') const service = await dataset.getService('some-service') // starting a service but does not wait until it is started await service.start() // Function that checks if a service is available await service.waitUntilRunning()","title":"Example"},{"location":"triplydb-js/#story","text":"A TriplyDB data story is a way of communicating information about your linked data along with explanatory text while also being able to integrate query results. To create Data stories with TriplyDB.js You can use the User.ensureStory or User.addStory functions to create. If you want to retrieve an already created data story you can use the functions User.getStories to iterate over all stories, or retrieve a particular one with User.getStory . Story objects are obtained through the the following methods: User.addStory User.ensureStory User.getStories User.getStory","title":"Story"},{"location":"triplydb-js/#storydelete","text":"Deletes this story. This deletes all paragraphs that belong to this story. This does not delete the queries that are linked into this story. If you also want to delete the queries, then this must be done with distinct calls of Query.delete() .","title":"Story.delete()"},{"location":"triplydb-js/#examples_62","text":"The following code example deletes a story called 'example-story' under the current user's account: const user = await triply.getUser() const story = await user.getStory('example-story') await story.delete()","title":"Examples"},{"location":"triplydb-js/#storygetinfo","text":"Returns information about this data story. Information is returned in a dictionary object. Individual keys can be accessed for specific information values.","title":"Story.getInfo()"},{"location":"triplydb-js/#examples_63","text":"The following snippet prints the paragraphs that appear in a data story: for (const element of (await story.getInfo()).content) { if ((element.type = 'paragraph')) { console.log(element.paragraph) } }","title":"Examples"},{"location":"triplydb-js/#user","text":"Instances of class User denote users in TriplyDB.","title":"User"},{"location":"triplydb-js/#obtaining-instances_1","text":"Users are obtained with method App.getUser(name?: string) : const user = triply.getUser('john-doe') const user = triply.getUser() Alternatively, users are obtained by first obtaining an account ( App.getAccount(name?: string) ) and then casting it to a use ( Account.asUser() ): const account = await triply.getAccount('john-doe') const user = account.asUser()","title":"Obtaining instances"},{"location":"triplydb-js/#inheritance_1","text":"User is a subclass of Account , from which it inherits most of its methods.","title":"Inheritance"},{"location":"triplydb-js/#limitations","text":"Users cannot be created or deleted through the TriplyDB.js library. See the Triply Console documentation for how to create and delete users through the web-based GUI.","title":"Limitations"},{"location":"triplydb-js/#useradddatasetname-string-metadata-object","text":"Adds a new TriplyDB dataset with the given name to the current account. Inherited from Account.addDataset(name: string, metadata?: object) .","title":"User.addDataset(name: string, metadata?: object)"},{"location":"triplydb-js/#useraddquerymetadata-object","text":"Adds a new TriplyDB query to the current user. Inherited from Account.addQuery(metadata: object) .","title":"User.addQuery(metadata: object)"},{"location":"triplydb-js/#userensurestoryname-string-metadata-object","text":"Ensures the existence of a story with the given name and with the specified metadata . Inherited from Account.ensureStory(name: string, metadata: object) .","title":"User.ensureStory(name: string, metadata: object)"},{"location":"triplydb-js/#useraddstoryname-string-metadata-object","text":"Adds a new TriplyDB story with the given name to the current user. Inherited from Account.addStory(name: string, metadata?: object) .","title":"User.addStory(name: string, metadata?: object)"},{"location":"triplydb-js/#usercreateorganizationname-string-metadata-object","text":"Creates a new organization for which this user will be the owner.","title":"User.createOrganization(name: string, metadata?: object)"},{"location":"triplydb-js/#access-restrictions_3","text":"This method requires an API token with write access for this user.","title":"Access restrictions"},{"location":"triplydb-js/#arguments_11","text":"Argument name is the URL-friendly name of the new organization. This name can only contain alphanumeric characters and hyphens ( [A-Za-z0-9\\-] ). The optional metadata argument can be used to specify additional metadata. This is a dictionary object with the following optional keys: description The description of the organization. This description can make use of Markdown. email The email address at which the organization can be reached. name The human-readable name of the organization. This name may contain spaces and other non-alphanumeric characters.","title":"Arguments"},{"location":"triplydb-js/#examples_64","text":"The following snippet creates a new organization for which John Doe will be the owner. Notice that both a required URL-friendly name ( 'my-organization' ) and an optional display name ( 'My Organization' ) are specified. const user = await triply.getUser('john-doe') await user.createOrganization(my-organization, {name: 'My Organization'}))","title":"Examples"},{"location":"triplydb-js/#userensuredatasetname-string-metadata-object","text":"Ensures the existence of a dataset with the given name and with the specified metadata . Inherited from Account.ensureDataset(name: string, metadata?: object) .","title":"User.ensureDataset(name: string, metadata?: object)"},{"location":"triplydb-js/#usergetdatasetname-string","text":"Returns the TriplyDB dataset with the given name that is published by this user. Inherited from Account.getDataset(name: string) .","title":"User.getDataset(name: string)"},{"location":"triplydb-js/#usergetdatasets","text":"Returns an async iterator over the accessible datasets for the current user. Inherited from Account.getDatasets() .","title":"User.getDatasets()"},{"location":"triplydb-js/#usergetinfo","text":"Returns information about this user. Information is returned in a dictionary object. Individual keys can be accessed for specific information values. The information object for users includes the following keys: avatarUrl A URL to the user image. accountName The URL-friendly name of the user. name The human-readable display name of the user description The human-readable description of the user. createdAt The date and time on which the user was created. datasetCount The number of datasets for the user. queryCount The number of queries for the user. storyCount The number of stories for the user pinnedItems An array containing the pinned items (datasets, stories and queries) for the user. role The role of the user. Either 'light', 'regular' or 'siteAdmin'. orgs An array of organizations of which the user is a member. Email address The email address of the user. updatedAt The date and time on which the user was last updated. lastActivity The date and time on which the user was last online on TriplyDB.","title":"User.getInfo()"},{"location":"triplydb-js/#examples_65","text":"The following snippet prints an overview of account that is associated with the used API token: const user = await triply.getUser() console.log(await user.getInfo())","title":"Examples"},{"location":"triplydb-js/#usergetorganizations","text":"Returns an async iterator over the organizations that this user is a member of.","title":"User.getOrganizations()"},{"location":"triplydb-js/#order-considerations_1","text":"The order in the list reflects the order in which the organizations appear on the user page in the Triply GUI.","title":"Order considerations"},{"location":"triplydb-js/#examples_66","text":"The following snippet prints the list of organizations that John Doe is a member of: const user = await triply.getUser('john-doe') for await (const organization of await user.getOrganizations()) { console.log((await organization.getInfo()).name) }","title":"Examples"},{"location":"triplydb-js/#see-also_17","text":"The async iterator contains organization objects. See the section about the Organization class for methods that can be used on such objects.","title":"See also"},{"location":"triplydb-js/#usergetpinneditems","text":"Returns the list of datasets, stories and queries that are pinned for the current user. Inherited from Account.getPinnedItems() .","title":"User.getPinnedItems()"},{"location":"triplydb-js/#usersetavatarfile-string","text":"Sets a new image that characterized this user. Inherited from Account.setAvatar(file: string) .","title":"User.setAvatar(file: string)"},{"location":"triplydb-js/#userupdatemetadata-object","text":"Updates the metadata for this user. Inherited from Account.update(metadata: object) .","title":"User.update(metadata: object)"},{"location":"triplydb-js/#faq","text":"This section includes answers to frequently asked questions. Please contact info@triply.cc if you have a question that does not appear in this list.","title":"FAQ"},{"location":"triplydb-js/#how-to-perform-a-sparql-query","text":"The SPARQL 1.1 Protocol standard specifies a native HTTP API for performing SPARQL requests. Such requests can be performed with regular HTTP libraries. Here we give an example indicating how such an HTTP library can be used: import SuperAgent from 'superagent'; const reply = await SuperAgent.post('SPARQL_ENDPOINT') .set('Accept', 'application/sparql-results+json') .set('Authorization', 'Bearer ' + process.env.TOKEN) .buffer(true) .send({ query: 'select * { WHERE_CLAUSE } offset 0 limit 10000' }) // break condition when the result set is empty. // downsides: caching, string manipulation","title":"How to perform a SPARQL query?"},{"location":"triplydb-js/#what-is-the-latest-version-of-triplydbjs","text":"The latest version of TriplyDB.js can be found in the NPM repository .","title":"What is the latest version of TriplyDB.js?"},{"location":"triplydb-js/#what-to-do-when-the-error-unauthorized-appears","text":"This error appears whenever an operation is performed for which the user denoted by the current API token is not authorized. One common appearance of this error is when the environment variable TOKEN is not set to an API token. The current value of the environment variable can be tested by running the following command in the terminal: echo $TOKEN","title":"What to do when the \u201cError: Unauthorized\u201d appears?"},{"location":"triplydb-js/#how-do-i-get-the-results-of-a-saved-query-using-triplydbjs","text":"To reliably retrieve a large number of results as the output of a construct or select query, follow these steps: Import the triplydb library. ts import App from '@triply/triplydb'; Set your parameters, regarding the TriplyDB server and the account in which you have saved the query as well as the name of the query. ts const triply = App.get({ url: 'https://api.triplydb.com' }) const account = await triply.getAccount('account-name') const query = await account.getQuery('name-of-some-query') If the query is not public, you should set your API token rather than the URL. ts const triply = App.get({ token: process.env.TOKEN }) Do not forget that we perform TriplyDB.js requests within an async context . That is: ts async function run() { // Your code goes here. } run() Get the results of a query by setting a results variable. More specifically, for construct queries: ts const results = query.results().statements() For select queries: ts const results = query.results().bindings() Note that for SPARQL construct queries, we use method .statements() , while for SPARQL select queries, we use method .bindings() . Additionally, saved queries can have API variables that allow you to specify variables that are used in the query. Thus, if you have query parameters, pass their values as the first argument to results as follows: ts // For SPARQL construct queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .statements() // For SPARQL select queries. const results = query .results({ someVariable: 'value of someVariable', anotherVariable: 'value of anotherVariable', }) .bindings() To read the results you have three options: 5a. Iterate through the results per row in a for -loop: ts // Iterating over the results per row for await (const row of results) { // execute something } 5b. Save the results to a file. For saving SPARQL construct queries: ts // Saving the results of a SPARQL construct query to a file. await results.toFile('my-file.nt') For saving SPARQL select queries. Currently we only support saving the file to a .tsv format: ts // Saving the results of a SPARQL select query to a file. await results.toFile('my-file.tsv') 5c. Load all results into memory. Note that this is almost never used. If you want to process results, then option 5a is better; if you want to persist results, then option 5b is better. ts // Loading results for a SPARQL construct or SPARQL select query into memory. const array = await results.toArray()","title":"How do I get the results of a saved query using TriplyDB.js?"},{"location":"triplydb-js/#what-is-an-async-iterator-async-iterator","text":"TriplyDB.js makes use of async iterators for retrieving lists of objects. Async iterators are a method of fetching and iterating through large lists, without having to first fetch the whole set. An example of an async iterator in TriplyDB.js is App.getAccounts() . The following code illustrates how it can be used. for await (const account of triply.getAccounts()) { console.log(account) } For cases where you want the complete list, you can use the toArray function of the iterator. const accounts = await triply.getAccounts().toArray() TriplyDB.js returns async iterators from the following methods: App.getAccounts() Account.getDatasets() Account.getQueries() Account.getStories() Dataset.getServices() Dataset.getAssets() Dataset.getGraphs() Dataset.getStatements() Query.results().statements() for SPARQL construct and describe queries Query.results().bindings() for SPARQL select queries","title":"What is an async iterator? {#async-iterator}"},{"location":"yasgui/","text":"This section explains the use of SPARQL via Yasgui. Yasgui provides various advanced features for creating, sharing, and visualizing SPARQL queries and their results. SPARQL Editor {#sparql-editor} The Yasgui SPARQL editor is a query editor that offers syntax highlighting, syntax validation, autocompletion, a variety of different SPARQL result visualizations, with a plugin architecture that enables customization . By default, the query editor provides autocomplete suggestions via the LOV API. Website maintainers can add their own autocompletion logic as well. For example, the Yasgui integration in TriplyDB uses the TriplyDB API to more accurately provide suggestions based on the underlying data. Sharing queries now involves less than having to copy/past complete SPARQL queries. Instead, you can share your query (and the corresponding visualization settings) using a simple URL. Supported key combinations The following table enumerates the key combinations that are supported by the SPARQL Editor. Key combination Behavior Alt + Left Move the cursor to the beginning of the current line. Alt + Right Move the cursor to the end of the current line. Alt + U Redo the last change within the current selection. Ctrl + Backspace Delete to the beginning of the group before the cursor. Ctrl + Delete Delete to the beginning of the group after the cursor. Ctrl + End Move the cursor to the end of the query. Ctrl + Home Move the cursor to the start of the query. Ctrl + Left Move the cursor to the left of the group before the cursor. Ctrl + Right Move the cursor to the right of the group the cursor. Ctrl + [ Decrements the indentation for the current line or the lines involved in the current selection. Ctrl + ] Increments the indentation for the current line or the lines involved in the current selection. Ctrl + / Toggles on/off the commenting of the current line or the lines involved in the current selection. Ctrl + A Select the whole query. Ctrl + D Deletes the current line or all lines involved in the current selection. Ctrl + U Undo the last change within the current selection. Ctrl + Y Redo the last undone edit action. Ctrl + Z Undo the last edit action. Ctrl + Shift + F Auto-formats the whole query or the lines involved in the current selection. Shift + Tab Auto-indents the current line or the lines involved in the current selection. Tab Indents the current line or the lines involved in the current selection. Templates SPARQL has standardized capabilities for constructing complex strings and literals. This allows human-readable label and HTML widgets to be generated from within SPARQL. Unfortunately, the syntax for constructing such labels and widgets is a bit cumbersome. SPARQL-concat For example, the following SPARQL query returns HTML widgets that can be displayed in a web browser (see SPARQL Gallery ). It uses the concat function which allows an arbitrary number of string arguments to be concatenated into one string. Notice that this requires extensive quoting for each argument (e.g., '<h3>' ), as well as conversions from literals to strings (e.g., str(?typeName) ). Finally, in order to return an HTML literal we need to first bind the concatenated string to some variable ?lex , and then apply the strdt function in order to construct a literal with datatype IRI rdf:HTML . You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(concat('<h3>',str(?typeName),' \u300b ',str(?name),'</h3>', '<img src=\"',str(?image),'\">', '<audio controls src=\"',str(?cry),'\"></audio>') as ?lex) bind(strdt(?lex,rdf:HTML) as ?widget) } limit 25 Handlebars The SPARQL Editor in TriplyDB supports SPARQL Templates, which makes it easier to write human-readable labels and HTML widgets. SPARQL Templates are strings in which occurrences of {{x}} will be replaced with the to-string converted results of bindings to SPARQL variable ?x . The following example query produces the same result set as the above one, but allows the entire HTML string to be written at once as a SPARQL Template. Notice that this removes the need for concatenating ( concat/n ), explicit to-string conversion ( str/1 ), and also allows the HTML literal to be constructed more easily (no strdt/2 needed). You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(''' <h3>{{typeName}} \u300b {{name}}</h3> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio>'''^^rdf:HTML as ?widget) } limit 25 SPARQL Templates can be combined with the SPARQL Gallery feature in order to generate galleries of HTML widgets. Rendering HTML {#htmlRender} To distinguish between text and HTML result values the visualization library checks for the rdf:HTML datatype. The following query will return as plain text select * { bind('<p>Test</p>' as ?widget) } This query will render the result as HTML PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> select * { bind('<p>Test</p>'^^rdf:HTML as ?widget) } In order to guarantee safety, TriplyDB sanitizes HTML literals before rendering them. This means that tags like <embed> , <iframe> and <script> are sanitized away, as are attributes such as onerror and onload . Visualizations {#visualizations} Table {#table} This view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. Each row in the table corresponds to one query result. Each cell contains an RDF term or NULL . Features In addition to displaying the SPARQL result set, the SPARQL Table has the following features: Abbreviations The SPARQL Table uses the prefix declarations in the SPARQL query in order to abbreviate IRIs that appear in table cells. Filter By entering a search string into the \u201cFilter query results\u201d field, the table will only display result rows in which the entered search string appears. Indices The first column in the table indicates the index of each row in the SPARQL result set. Pagination By default the Table displays at most 50 rows. This maximum value can be changed to 10, 100, 1.000, or \u201cAll\u201d. Sorting A sort widget appears to the right of each header label. By pressing on the upward pointing arrow of the sort widget, rows will be sorted based on the lexicographic order of the values within the corresponding column. By pressing the downward pointing arrow of the sort widget, rows will be inversely sorted according to the same lexicographic order. Table Example The following SPARQL query (or see here ) returns a table of Pok\u00e9mon dragons (column pokemon ) and their happiness (column happiness ). Notice that the prefix for pokemon is not used in the query, but is used in order to abbreviate the IRI syntax in the pokemon column. By clicking on the sort widget next to the happiness header, the results can be (inversely) sorted based on the happiness values. PREFIX pokemon: <https://triplydb.com/academy/pokemon/id/pokemon/> PREFIX type: <https://triplydb.com/academy/pokemon/id/type/>a PREFIX vocab: <https://triplydb.com/academy/pokemon/vocab/> select ?pokemon ?happiness { ?pokemon vocab:type type:dragon; vocab:happiness ?happiness. } Response {#response} This view shows the body of the response and offers an easy way to download the result as a file. Gallery ( TriplyDB Plugin ) {#gallery} This view allows SPARQL results to be displayed in an HTML gallery. Each individual result corresponds to one HTML widget. Widgets are displayed in rows and columns to make up a widget gallery. Variables The gallery will render an item based on variables in the following table: Variable name Purpose ?widget The text or HTML content. meant for creating widget from scrap ?widgetLabel Title of the widget. Also used as the alternative text for the image ?widgetLabelLink A url which converts the title into a link, depends on ?widgetLabel ?widgetImage A url of an image to display ?widgetImageLink A url which adds a link to the image, depends on ?widgetImage ?widgetImageCaption A text or HTML description of the image, depends on ?widgetImage ?widgetDescription A text or HTML description, meant for adding links and Format The widget will display the variables in the following order: - ?widgetLabel and ?widgetLabelLink - ?widgetImage and ?widgetImageLink - ?widgetImageCaption - ?widgetDescription - ?widget Styling The ?widget display is restricted in height. This might not always be desired. In such cases the following style tweaks can help to make them the right size: bind('''<div style=\"max-height:unset; width:275px;\"> # The HTML that composes the widget goes here. </div>'''^^rdf:HTML as ?widget) Gallery Example The following SPARQL query binds an HTML string consisting of a header ( h3 ), an image ( img ), and an audio element ( audio ) to the ?widget variable. This results in a gallery with 25 widgets, each displaying a Pok\u00e9mon. (This SPARQL query also uses [[SPARQL Templates]] in order to simplify its syntax.) This query can be run online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon a def:Pokemon; def:baseAttack ?attack; def:baseDefense ?defense; def:baseExp ?experience; def:baseHP ?health; def:baseSpeed ?speed; def:cry ?cry; def:femaleRatio ?female; def:happiness ?happiness; def:maleRatio ?male; def:name ?name; foaf:depiction ?image; rdfs:label ?label. filter(langmatches(lang(?name),'ja')) bind(''' <h2>{{name}} ({{label}})</h2> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio> <ul> <li>Experience: {{experience}}</li> <li>Attack: {{attack}}</li> <li>Defense: {{defense}}</li> <li>Experience: {{experience}}</li> <li>Health: {{health}}</li> <li>Female ratio: {{female}}</li> <li>Happiness: {{happiness}}</li> <li>Male ratio: {{male}}</li> <li>Speed: {{speed}}</li> </ul>'''^^rdf:HTML as ?widget) } order by desc(?experience) limit 20 Chart ( TriplyDB Plugin ) {#charts} The chart plugin renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. The chart plugin also includes a treemap representation, that is suitable for displaying hierarchies. To use the treemap plugin, you must use the following projection variables in your SPARQL query (in this order): ?node ?parent ?size ?color The label of a tree node. Either the label of the node that is the parent of ?node , or the value UNDEF in case ?node is the root node. (optional) :: For leaf nodes, a positive integer indicating the relative size of ?node . (optional) :: For leaf nodes, a double indicating the relative color of ?node . Once the TreeMap is drawn it is possible to navigate the tree with the mouse: left clicking on a node will drill down into the corresponding subtree; right clicking on a node will move up to the subtree of its parent node. The chart configuration enables tweaking the treemap properties such as the number of displayed hierarchy levels. Geo ( TriplyDB Plugin ) {#geo} This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map. Variables {#geo-variables} This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xLabel The text or HTML content of popups that appear when clicking the shape bound to ?x . ?xTooltip Text or HTML that will appear when the shape of bound to ?x is hovered ?mapEndpoint A URL pointing to a WMS tile-server Color values Variable ?xColor must include a value of the following types: CSS color names . RGB color codes . HSL color codes . Gradients : Strings of the form {{PALETTE}},{{VALUE}} , where {{VALUE}} is a floating-point number between 0.0 and 1.0 and {{PALETTE}} is the name of a color palette. We support color schemes from the Colormap and Color Brewer libraries WMS tile-servers To include layers from a WMS tile-server, use the mapEndpoint variable to refer to a server. The plugin will then retrieve the layer information from the server. Usage of the layers can be toggled using the layer selector. Try this one: https://maps.heigit.org/histosm/wms Geo-3D (TriplyDB-only) {#geo-3d} This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. It supports both 3D and 2.5D visualizations, depending on whether the GeoSPARQL data is stored in native 3D or in 2D Variables {#geo-3d-variables} This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to 2D or 3D literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xHeight The height in meters of the 2.5D shape that is based on the 2D shape that is bound to ?x . This variable is not needed if data is stored in native 3D. ?xLabel The text or HTML content of the popups that appears when the shape that is bound to ?x is clicked. ?xZ The height in meters at which the 2.5D shape that is based on the 2D shape that is bound to ?x starts. This variable is not needed if data is stored in native 3D. Geo Events ( TriplyDB Plugin ) {#geo-events} The SPARQL Geo Events plugin renders geographical events as a story map ( example ). This view recognizes the following SPARQL variable names: Variable name Purpose ?eventLocation (required) A geo:wktLiteral . ?eventLabel Text or HTML event label. ?eventDescription Text or HTML event description. ?eventMedia A URL pointing to a media source. Supported media types are described here . ?eventMediaCaption Text or HTML media caption. ?eventMediaCredit Text or HTML media credit. Pivot Table ( TriplyDB Plugin ) {#pivot} This view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows. Timeline ( TriplyDB Plugin ) {#timeline} The SPARQL timeline renders the SPARQL results on a Timeline ( example ) To get started with this visualization you need at least a result containing a ?eventStart or ?eventDate with either a ?eventDescription , ?eventLabel or a ?eventMedia . (Combinations are also possible) The following parameters can be used, Parameters in Italic are experimental: Variable name Purpose ?eventStart A date when an event started ?eventEnd A date when an event Stopped ?eventDate A date when an event happened ?eventDescription Text/ HTML about the event ?eventLabel Text/ HTML title ?eventMedia Link to most forms of media see documentation for which type of links are supported ?eventType Groups events ?eventColor Colors event ?eventBackground Background of the event when selected ?eventMediaCaption Text/ HTML caption of the Media ?eventMediaCredit Text/ HTML credit of the Media ?eventMediaThumbnail The thumbnail of Media ?eventMediaAlt The Alt text of the Media ?eventMediaTitle The Title of the Media ?eventMediaLink The URL the image should link to Network ( TriplyDB Plugin ) {#network} This view renders SPARQL Construct results in a graph representation. It works for Turtle , Trig , N-Triples and N-Quads responses. The maximum amount of results that can be visualized is 1.000 due to performance. Markup ( TriplyDB Plugin ) {#markup} The markup view can be used to render a variety of markup languages. This requires the use of the ?markup variable to identify which variable to render. Based on the datatype of the variable the plugin will identify which markup language to use: Markup language Datatype HTML http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML Mermaid https://triplydb.com/Triply/vocab/def/mermaid * Plain text Other * This is currently a placeholder IRI, If you find a (dereferenceable) IRI for one of these datatypes please contact us .","title":"Yasgui"},{"location":"yasgui/#sparql-editor-sparql-editor","text":"The Yasgui SPARQL editor is a query editor that offers syntax highlighting, syntax validation, autocompletion, a variety of different SPARQL result visualizations, with a plugin architecture that enables customization . By default, the query editor provides autocomplete suggestions via the LOV API. Website maintainers can add their own autocompletion logic as well. For example, the Yasgui integration in TriplyDB uses the TriplyDB API to more accurately provide suggestions based on the underlying data. Sharing queries now involves less than having to copy/past complete SPARQL queries. Instead, you can share your query (and the corresponding visualization settings) using a simple URL.","title":"SPARQL Editor {#sparql-editor}"},{"location":"yasgui/#supported-key-combinations","text":"The following table enumerates the key combinations that are supported by the SPARQL Editor. Key combination Behavior Alt + Left Move the cursor to the beginning of the current line. Alt + Right Move the cursor to the end of the current line. Alt + U Redo the last change within the current selection. Ctrl + Backspace Delete to the beginning of the group before the cursor. Ctrl + Delete Delete to the beginning of the group after the cursor. Ctrl + End Move the cursor to the end of the query. Ctrl + Home Move the cursor to the start of the query. Ctrl + Left Move the cursor to the left of the group before the cursor. Ctrl + Right Move the cursor to the right of the group the cursor. Ctrl + [ Decrements the indentation for the current line or the lines involved in the current selection. Ctrl + ] Increments the indentation for the current line or the lines involved in the current selection. Ctrl + / Toggles on/off the commenting of the current line or the lines involved in the current selection. Ctrl + A Select the whole query. Ctrl + D Deletes the current line or all lines involved in the current selection. Ctrl + U Undo the last change within the current selection. Ctrl + Y Redo the last undone edit action. Ctrl + Z Undo the last edit action. Ctrl + Shift + F Auto-formats the whole query or the lines involved in the current selection. Shift + Tab Auto-indents the current line or the lines involved in the current selection. Tab Indents the current line or the lines involved in the current selection.","title":"Supported key combinations"},{"location":"yasgui/#templates","text":"SPARQL has standardized capabilities for constructing complex strings and literals. This allows human-readable label and HTML widgets to be generated from within SPARQL. Unfortunately, the syntax for constructing such labels and widgets is a bit cumbersome.","title":"Templates"},{"location":"yasgui/#sparql-concat","text":"For example, the following SPARQL query returns HTML widgets that can be displayed in a web browser (see SPARQL Gallery ). It uses the concat function which allows an arbitrary number of string arguments to be concatenated into one string. Notice that this requires extensive quoting for each argument (e.g., '<h3>' ), as well as conversions from literals to strings (e.g., str(?typeName) ). Finally, in order to return an HTML literal we need to first bind the concatenated string to some variable ?lex , and then apply the strdt function in order to construct a literal with datatype IRI rdf:HTML . You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(concat('<h3>',str(?typeName),' \u300b ',str(?name),'</h3>', '<img src=\"',str(?image),'\">', '<audio controls src=\"',str(?cry),'\"></audio>') as ?lex) bind(strdt(?lex,rdf:HTML) as ?widget) } limit 25","title":"SPARQL-concat"},{"location":"yasgui/#handlebars","text":"The SPARQL Editor in TriplyDB supports SPARQL Templates, which makes it easier to write human-readable labels and HTML widgets. SPARQL Templates are strings in which occurrences of {{x}} will be replaced with the to-string converted results of bindings to SPARQL variable ?x . The following example query produces the same result set as the above one, but allows the entire HTML string to be written at once as a SPARQL Template. Notice that this removes the need for concatenating ( concat/n ), explicit to-string conversion ( str/1 ), and also allows the HTML literal to be constructed more easily (no strdt/2 needed). You can try this query online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon def:cry ?cry; def:type/rdfs:label ?typeName; foaf:depiction ?image; rdfs:label ?name. bind(''' <h3>{{typeName}} \u300b {{name}}</h3> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio>'''^^rdf:HTML as ?widget) } limit 25 SPARQL Templates can be combined with the SPARQL Gallery feature in order to generate galleries of HTML widgets.","title":"Handlebars"},{"location":"yasgui/#rendering-html-htmlrender","text":"To distinguish between text and HTML result values the visualization library checks for the rdf:HTML datatype. The following query will return as plain text select * { bind('<p>Test</p>' as ?widget) } This query will render the result as HTML PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> select * { bind('<p>Test</p>'^^rdf:HTML as ?widget) } In order to guarantee safety, TriplyDB sanitizes HTML literals before rendering them. This means that tags like <embed> , <iframe> and <script> are sanitized away, as are attributes such as onerror and onload .","title":"Rendering HTML {#htmlRender}"},{"location":"yasgui/#visualizations-visualizations","text":"","title":"Visualizations {#visualizations}"},{"location":"yasgui/#table-table","text":"This view allows SPARQL results to be displayed in a table. Each column in the table corresponds to a variable that belongs to the outer projection. Each row in the table corresponds to one query result. Each cell contains an RDF term or NULL .","title":"Table {#table}"},{"location":"yasgui/#features","text":"In addition to displaying the SPARQL result set, the SPARQL Table has the following features: Abbreviations The SPARQL Table uses the prefix declarations in the SPARQL query in order to abbreviate IRIs that appear in table cells. Filter By entering a search string into the \u201cFilter query results\u201d field, the table will only display result rows in which the entered search string appears. Indices The first column in the table indicates the index of each row in the SPARQL result set. Pagination By default the Table displays at most 50 rows. This maximum value can be changed to 10, 100, 1.000, or \u201cAll\u201d. Sorting A sort widget appears to the right of each header label. By pressing on the upward pointing arrow of the sort widget, rows will be sorted based on the lexicographic order of the values within the corresponding column. By pressing the downward pointing arrow of the sort widget, rows will be inversely sorted according to the same lexicographic order.","title":"Features"},{"location":"yasgui/#table-example","text":"The following SPARQL query (or see here ) returns a table of Pok\u00e9mon dragons (column pokemon ) and their happiness (column happiness ). Notice that the prefix for pokemon is not used in the query, but is used in order to abbreviate the IRI syntax in the pokemon column. By clicking on the sort widget next to the happiness header, the results can be (inversely) sorted based on the happiness values. PREFIX pokemon: <https://triplydb.com/academy/pokemon/id/pokemon/> PREFIX type: <https://triplydb.com/academy/pokemon/id/type/>a PREFIX vocab: <https://triplydb.com/academy/pokemon/vocab/> select ?pokemon ?happiness { ?pokemon vocab:type type:dragon; vocab:happiness ?happiness. }","title":"Table Example"},{"location":"yasgui/#response-response","text":"This view shows the body of the response and offers an easy way to download the result as a file.","title":"Response {#response}"},{"location":"yasgui/#gallery-triplydb-plugin-gallery","text":"This view allows SPARQL results to be displayed in an HTML gallery. Each individual result corresponds to one HTML widget. Widgets are displayed in rows and columns to make up a widget gallery.","title":"Gallery (TriplyDB Plugin) {#gallery}"},{"location":"yasgui/#variables","text":"The gallery will render an item based on variables in the following table: Variable name Purpose ?widget The text or HTML content. meant for creating widget from scrap ?widgetLabel Title of the widget. Also used as the alternative text for the image ?widgetLabelLink A url which converts the title into a link, depends on ?widgetLabel ?widgetImage A url of an image to display ?widgetImageLink A url which adds a link to the image, depends on ?widgetImage ?widgetImageCaption A text or HTML description of the image, depends on ?widgetImage ?widgetDescription A text or HTML description, meant for adding links and","title":"Variables"},{"location":"yasgui/#format","text":"The widget will display the variables in the following order: - ?widgetLabel and ?widgetLabelLink - ?widgetImage and ?widgetImageLink - ?widgetImageCaption - ?widgetDescription - ?widget","title":"Format"},{"location":"yasgui/#styling","text":"The ?widget display is restricted in height. This might not always be desired. In such cases the following style tweaks can help to make them the right size: bind('''<div style=\"max-height:unset; width:275px;\"> # The HTML that composes the widget goes here. </div>'''^^rdf:HTML as ?widget)","title":"Styling"},{"location":"yasgui/#gallery-example","text":"The following SPARQL query binds an HTML string consisting of a header ( h3 ), an image ( img ), and an audio element ( audio ) to the ?widget variable. This results in a gallery with 25 widgets, each displaying a Pok\u00e9mon. (This SPARQL query also uses [[SPARQL Templates]] in order to simplify its syntax.) This query can be run online . prefix def: <https://triplydb.com/academy/pokemon/def/> prefix foaf: <http://xmlns.com/foaf/0.1/> prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> select * { ?pokemon a def:Pokemon; def:baseAttack ?attack; def:baseDefense ?defense; def:baseExp ?experience; def:baseHP ?health; def:baseSpeed ?speed; def:cry ?cry; def:femaleRatio ?female; def:happiness ?happiness; def:maleRatio ?male; def:name ?name; foaf:depiction ?image; rdfs:label ?label. filter(langmatches(lang(?name),'ja')) bind(''' <h2>{{name}} ({{label}})</h2> <img src=\"{{image}}\"> <audio controls src=\"{{cry}}\"></audio> <ul> <li>Experience: {{experience}}</li> <li>Attack: {{attack}}</li> <li>Defense: {{defense}}</li> <li>Experience: {{experience}}</li> <li>Health: {{health}}</li> <li>Female ratio: {{female}}</li> <li>Happiness: {{happiness}}</li> <li>Male ratio: {{male}}</li> <li>Speed: {{speed}}</li> </ul>'''^^rdf:HTML as ?widget) } order by desc(?experience) limit 20","title":"Gallery Example"},{"location":"yasgui/#chart-triplydb-plugin-charts","text":"The chart plugin renders geographical, temporal and numerical data in interactive charts such as bar-, line- and pie charts. The chart plugin also includes a treemap representation, that is suitable for displaying hierarchies. To use the treemap plugin, you must use the following projection variables in your SPARQL query (in this order): ?node ?parent ?size ?color The label of a tree node. Either the label of the node that is the parent of ?node , or the value UNDEF in case ?node is the root node. (optional) :: For leaf nodes, a positive integer indicating the relative size of ?node . (optional) :: For leaf nodes, a double indicating the relative color of ?node . Once the TreeMap is drawn it is possible to navigate the tree with the mouse: left clicking on a node will drill down into the corresponding subtree; right clicking on a node will move up to the subtree of its parent node. The chart configuration enables tweaking the treemap properties such as the number of displayed hierarchy levels.","title":"Chart (TriplyDB Plugin) {#charts}"},{"location":"yasgui/#geo-triplydb-plugin-geo","text":"This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 2D map.","title":"Geo (TriplyDB Plugin) {#geo}"},{"location":"yasgui/#variables-geo-variables","text":"This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xLabel The text or HTML content of popups that appear when clicking the shape bound to ?x . ?xTooltip Text or HTML that will appear when the shape of bound to ?x is hovered ?mapEndpoint A URL pointing to a WMS tile-server","title":"Variables {#geo-variables}"},{"location":"yasgui/#color-values","text":"Variable ?xColor must include a value of the following types: CSS color names . RGB color codes . HSL color codes . Gradients : Strings of the form {{PALETTE}},{{VALUE}} , where {{VALUE}} is a floating-point number between 0.0 and 1.0 and {{PALETTE}} is the name of a color palette. We support color schemes from the Colormap and Color Brewer libraries","title":"Color values"},{"location":"yasgui/#wms-tile-servers","text":"To include layers from a WMS tile-server, use the mapEndpoint variable to refer to a server. The plugin will then retrieve the layer information from the server. Usage of the layers can be toggled using the layer selector. Try this one: https://maps.heigit.org/histosm/wms","title":"WMS tile-servers"},{"location":"yasgui/#geo-3d-triplydb-only-geo-3d","text":"This view allows SPARQL results that contain GeoSPARQL semantics to be automatically interpreted and displayed on a 3D globe. It supports both 3D and 2.5D visualizations, depending on whether the GeoSPARQL data is stored in native 3D or in 2D","title":"Geo-3D (TriplyDB-only) {#geo-3d}"},{"location":"yasgui/#variables-geo-3d-variables","text":"This view recognizes the following SPARQL variable names: Variable name Purpose ?x An arbitrary variable name that is bound to 2D or 3D literals with datatype IRI geo:wktLiteral , and whose name is the prefix of the other variable names in this table. ?xColor The color of the shape bound to ?x . ?xHeight The height in meters of the 2.5D shape that is based on the 2D shape that is bound to ?x . This variable is not needed if data is stored in native 3D. ?xLabel The text or HTML content of the popups that appears when the shape that is bound to ?x is clicked. ?xZ The height in meters at which the 2.5D shape that is based on the 2D shape that is bound to ?x starts. This variable is not needed if data is stored in native 3D.","title":"Variables {#geo-3d-variables}"},{"location":"yasgui/#geo-events-triplydb-plugin-geo-events","text":"The SPARQL Geo Events plugin renders geographical events as a story map ( example ). This view recognizes the following SPARQL variable names: Variable name Purpose ?eventLocation (required) A geo:wktLiteral . ?eventLabel Text or HTML event label. ?eventDescription Text or HTML event description. ?eventMedia A URL pointing to a media source. Supported media types are described here . ?eventMediaCaption Text or HTML media caption. ?eventMediaCredit Text or HTML media credit.","title":"Geo Events (TriplyDB Plugin) {#geo-events}"},{"location":"yasgui/#pivot-table-triplydb-plugin-pivot","text":"This view renders SPARQL results in an interactive pivot table where you are able to aggregate the results by dragging your binding variables to columns or rows.","title":"Pivot Table (TriplyDB Plugin) {#pivot}"},{"location":"yasgui/#timeline-triplydb-plugin-timeline","text":"The SPARQL timeline renders the SPARQL results on a Timeline ( example ) To get started with this visualization you need at least a result containing a ?eventStart or ?eventDate with either a ?eventDescription , ?eventLabel or a ?eventMedia . (Combinations are also possible) The following parameters can be used, Parameters in Italic are experimental: Variable name Purpose ?eventStart A date when an event started ?eventEnd A date when an event Stopped ?eventDate A date when an event happened ?eventDescription Text/ HTML about the event ?eventLabel Text/ HTML title ?eventMedia Link to most forms of media see documentation for which type of links are supported ?eventType Groups events ?eventColor Colors event ?eventBackground Background of the event when selected ?eventMediaCaption Text/ HTML caption of the Media ?eventMediaCredit Text/ HTML credit of the Media ?eventMediaThumbnail The thumbnail of Media ?eventMediaAlt The Alt text of the Media ?eventMediaTitle The Title of the Media ?eventMediaLink The URL the image should link to","title":"Timeline (TriplyDB Plugin) {#timeline}"},{"location":"yasgui/#network-triplydb-plugin-network","text":"This view renders SPARQL Construct results in a graph representation. It works for Turtle , Trig , N-Triples and N-Quads responses. The maximum amount of results that can be visualized is 1.000 due to performance.","title":"Network (TriplyDB Plugin) {#network}"},{"location":"yasgui/#markup-triplydb-plugin-markup","text":"The markup view can be used to render a variety of markup languages. This requires the use of the ?markup variable to identify which variable to render. Based on the datatype of the variable the plugin will identify which markup language to use: Markup language Datatype HTML http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML Mermaid https://triplydb.com/Triply/vocab/def/mermaid * Plain text Other * This is currently a placeholder IRI, If you find a (dereferenceable) IRI for one of these datatypes please contact us .","title":"Markup (TriplyDB Plugin) {#markup}"},{"location":"yasgui-api/","text":"Yasgui consists of three components: Yasqe (a SPARQL Query Editor), Yasr (a SPARQL result visualizer), and Yasgui which binds the former together. Here you can find documentation on ways to include, configure and extend these components as suitable to your use-case. . About additional plugins {#triplyDbPlugins} Yasgui, Yasqe and Yasr are all open source and MIT licensed. Triply provides additional plugins that are only free to use via https://yasgui.triply.cc or TriplyDB . These additional plugins are not MIT licensed and cannot be used or included programmatically. Installation Via package managers To include Yasgui in a project include the package run the commands below. npm npm i @triply/yasgui yarn yarn add @triply/yasgui Via cdn {#web} To include Yasgui in your webpage, all that's needed is importing the Yasgui JavaScript and CSS files, and initializing a Yasgui object: <head> <link href=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.css\" rel=\"stylesheet\" type=\"text/css\" /> <script src=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.js\"></script> </head> <body> <div id=\"yasgui\"></div> <script> const yasgui = new Yasgui(document.getElementById(\"yasgui\")); </script> </body> If you only want to use Yasgui for querying a specific endpoint, you can add the following styling to disable the endpoint selector: <style> .yasgui .autocompleteWrapper { display: none !important; } </style> And pass a second argument to the Yasgui initializer to specify the default endpoint: const yasgui = new Yasgui(document.getElementById(\"yasgui\"), { requestConfig: { endpoint: \"http://example.com/sparql\" }, copyEndpointOnNewTab: false, }); Note: If you've already opened the Yasgui page before, you must first clear your local-storage cache before you will see the changes taking effect. API Reference Yasgui API Yasgui features tabs. Each tab has its own isolated query and results. These are persistent as the user switches between tabs. // Add a new Tab. Returns the new Tab object. yasgui.addTab( true, // set as active tab { ...Yasgui.Tab.getDefaults(), name: \"my new tab\" } ); // Get a Tab. Returns the current Tab if no tab id is given. yasgui.getTab(\"tab_id_x\"); Tab API // set the query of the tab tab.setQuery(\"select * where {...}\"); // close the tab tab.close(); // access the Yasqe API for the tab tab.yasqe; // access the Yasr API for the tab tab.yasr; Events {#yasgui-events} Yasgui emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasgui.on(\"query\", (instance: Yasgui, tab: Tab) => {}); // Fires when a query is finished yasgui.on(\"queryResponse\", (instance: Yasgui, tab: tab) => {}); Configuration {#yasgui-config} This configuration object is accessible/changeable via Yasgui.defaults or yasgui.config . You can pass these along when initializing Yasgui as well. To change settings to the Yasqe and Yasr components used by Yasgui, you are best off changing the Yasgui.Yasqe.defaults and Yasgui.Yasr.defaults objects before initializing Yasgui. { /** * Change the default request configuration, such as the headers * and default yasgui endpoint. * Define these fields as plain values, or as a getter function */ requestConfig: { endpoint: 'https://example.org/sparql', //Example of using a getter function to define the headers field: headers: () => ({ 'key': 'value' }), method: 'POST', }, // Allow resizing of the Yasqe editor resizeable: true, // Whether to autofocus on Yasqe on page load autofocus: true, // Use the default endpoint when a new tab is opened copyEndpointOnNewTab: false, // Configuring which endpoints appear in the endpoint catalogue list endpointCatalogueOptions { getData: () => { return [ //List of objects should contain the endpoint field //Feel free to include any other fields (e.g. a description or icon //that you'd like to use when rendering) { endpoint: \"https://dbpedia.org/sparql\" }, { endpoint: \"https://query.wikidata.org\" } // ... ]; }, //Data object keys that are used for filtering. The 'endpoint' key already used by default keys: [], //Data argument contains a `value` property for the matched data object //Source argument is the suggestion DOM element to append your rendered item to renderItem: (data, source) => { const contentDiv = document.createElement(\"div\"); contentDiv.innerText = data.value.endpoint; source.appendChild(contentDiv); } } } Yasqe Yasqe extends the CodeMirror Library. For an overview of CodeMirror functionality, see the CodeMirror documentation . Note: Where CodeMirror provides CodeMirror in the global namespace, we provide Yasqe . Yasqe API The Yasqe API can be accessed via yasqe (if Yasqe is run standalone) or via a tab yasgui.getTab().yasqe when run in Yasgui // Set query value in editor yasqe.setValue(\"select * where {...}\"); // Get query value from editor yasqe.getValue(); // execute a query yasqe.query({ url: \"https://dbpedia.org/sparql\", reqMethod: \"POST\", // or \"GET\" headers: { Accept: \"...\" /*...*/ }, args: { arg1: \"val1\" /*...*/ }, withCredentials: false, }); // get whether we're in query or update mode yasqe.getQueryMode(); // get the query type (select, ask, construct, ...) yasqe.getQueryType(); // get prefixes map from the query string yasqe.getPrefixesFromQuery(); // Add prefixes to the query. yasqe.addPrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // Remove prefixes to the query. yasqe.removePrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // set size of input area yasqe.setSize(500, 300); // Collapsing prefixes if there are any. Use false to expand them. yasqe.collapsePrefixes(true); Events {#yasqe-events} Yasqe emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasqe.on(\"query\", (instance: Yasqe, req: superagent.SuperAgentRequest) => {}); // Fires when a query is finished yasqe.on(\"queryResponse\", (instance: Yasqe, req: superagent.SuperAgentRequest, duration: number) => {}); Configuration {#yasqe-config} The configuration options, for Yasqe can be accessed through Yasgui.Yasqe or yasqe.options . Here are some configurable fields for Yasqe. They can be accessed and set through Yasqe.defaults and yasqe.options . The configuration object extends the CodeMirror config , meaning fields like for example tabSize may also be set. // number of seconds to persist query input, stored in the browser // set to 0 to always load the default query on page load persistencyExpire // default: 30 days // default settings for how to query the endpoint requestOpts: { endpoint: \"http://dbpedia.org/sparql\", method: \"POST\", headers: {} }, Yasr Yasr is an extendable library that renders SPARQL results. Yasr is responsible for gluing the different visualization plugins together, and providing utilities such as SPARQL result parsers. Yasr API // Set and draw a SPARQL response. The parameter is either // - a plain response string // - a SuperAgent response // - or an object with the specified keys yasr.setResponse({ data: \"...\"; contentType: \"application/sparql-results+json\"; status: 200; executionTime: 1000; // ms // error to show }) // Draw results with current plugin yasr.draw() // Check whether a result has been drawn yasr.somethingDrawn() // Select a plugin yasr.selectPlugin(\"table\") // Download a result set (if possible) yasr.download() Events {#yasr-events} // Fires just before a plugins draws the results yasr.on(\"draw\",(instance: Yasr, plugin: Plugin) => void); // Fires when a plugin finished drawing the results yasr.on(\"drawn\",(instance: Yasr, plugin: Plugin) => void); Configuration {#yasr-configuration} This configuration object is accessible/changeable via Yasr.defaults and yasr.options , and you can pass these along when initializing Yasr as well. Output visualizations are defined separately. // Ordered list of enabled output plugins pluginOrder = [\"table\", \"response\"] // The default plugin defaultPlugin = \"table\" // seconds before results expire in the browser // Set to 0 to disable results persistency persistencyExpire // default: 30 days // Map of prefixes to use in results views prefixes: {\"dbo\":\"http://dbpedia.org/ontology/\",/*...*/} Yasr plugins Each plugin has its own configuration options. These options can be accessed through Yasr.plugins . Table This plugin shows SPARQL results as a table, using the DataTables.net plugin. This plugin is defined in Yasr.plugins.table and can be configured using Yasr.plugins.table.defaults . // Open URIs in results in a new window rather than the current. openIriInNewWindow = true; Raw Response A plugin which uses CodeMirror to present the SPARQL results as-is. This plugin is defined at Yasr.plugins.response and can be configured using Yasr.plugins.response.defaults . // Number of lines to show before hiding rest of response // (too large value may cause browser performance issues) maxLines = 30; Writing a Yasr plugin To register a Yasr plugin, add it to Yasr by running Yasr.registerPlugin(pluginName: string, plugin: Plugin) . Below is an example implementation for rendering the result of an ASK query, which returns either true or false . See also the implementations of the Table and Raw Response plugins. class Boolean { // A priority value. If multiple plugin support rendering of a result, this value is used // to select the correct plugin priority = 10; // Whether to show a select-button for this plugin hideFromSelection = true; constructor(yasr) { this.yasr = yasr; } // Draw the result set. This plugin simply draws the string 'True' or 'False' draw() { const el = document.createElement(\"div\"); el.textContent = this.yasr.results.getBoolean() ? \"True\" : \"False\"; this.yasr.resultsEl.appendChild(el); } // A required function, used to indicate whether this plugin can draw the current // resultset from yasr canHandleResults() { return ( this.yasr.results.getBoolean && (this.yasr.results.getBoolean() === true || this.yasr.results.getBoolean() == false) ); } // A required function, used to identify the plugin, works best with an svg getIcon() { const textIcon = document.createElement(\"p\"); textIcon.innerText = \"\u2713/\u2717\"; return textIcon; } } //Register the plugin to Yasr Yasr.registerPlugin(\"MyBooleanPlugin\", Boolean); FAQ Using Yasgui in react To include Yasgui in React, use the following snippet. This snippet assumes a React repository configured via create-react-app , and a minimum React version of 16.8. import Yasgui from \"@triply/yasgui\"; import \"@triply/yasgui/build/yasgui.min.css\"; export default function App() { useEffect(() => { const yasgui = new Yasgui(document.getElementById(\"yasgui\")); return () => {}; }, []); return <div id=\"yasgui\" />; }","title":"Yasgui API Reference"},{"location":"yasgui-api/#about-additional-plugins-triplydbplugins","text":"Yasgui, Yasqe and Yasr are all open source and MIT licensed. Triply provides additional plugins that are only free to use via https://yasgui.triply.cc or TriplyDB . These additional plugins are not MIT licensed and cannot be used or included programmatically.","title":"About additional plugins {#triplyDbPlugins}"},{"location":"yasgui-api/#installation","text":"","title":"Installation"},{"location":"yasgui-api/#via-package-managers","text":"To include Yasgui in a project include the package run the commands below.","title":"Via package managers"},{"location":"yasgui-api/#npm","text":"npm i @triply/yasgui","title":"npm"},{"location":"yasgui-api/#yarn","text":"yarn add @triply/yasgui","title":"yarn"},{"location":"yasgui-api/#via-cdn-web","text":"To include Yasgui in your webpage, all that's needed is importing the Yasgui JavaScript and CSS files, and initializing a Yasgui object: <head> <link href=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.css\" rel=\"stylesheet\" type=\"text/css\" /> <script src=\"https://unpkg.com/@triply/yasgui/build/yasgui.min.js\"></script> </head> <body> <div id=\"yasgui\"></div> <script> const yasgui = new Yasgui(document.getElementById(\"yasgui\")); </script> </body> If you only want to use Yasgui for querying a specific endpoint, you can add the following styling to disable the endpoint selector: <style> .yasgui .autocompleteWrapper { display: none !important; } </style> And pass a second argument to the Yasgui initializer to specify the default endpoint: const yasgui = new Yasgui(document.getElementById(\"yasgui\"), { requestConfig: { endpoint: \"http://example.com/sparql\" }, copyEndpointOnNewTab: false, }); Note: If you've already opened the Yasgui page before, you must first clear your local-storage cache before you will see the changes taking effect.","title":"Via cdn {#web}"},{"location":"yasgui-api/#api-reference","text":"","title":"API Reference"},{"location":"yasgui-api/#yasgui-api","text":"Yasgui features tabs. Each tab has its own isolated query and results. These are persistent as the user switches between tabs. // Add a new Tab. Returns the new Tab object. yasgui.addTab( true, // set as active tab { ...Yasgui.Tab.getDefaults(), name: \"my new tab\" } ); // Get a Tab. Returns the current Tab if no tab id is given. yasgui.getTab(\"tab_id_x\");","title":"Yasgui API"},{"location":"yasgui-api/#tab-api","text":"// set the query of the tab tab.setQuery(\"select * where {...}\"); // close the tab tab.close(); // access the Yasqe API for the tab tab.yasqe; // access the Yasr API for the tab tab.yasr;","title":"Tab API"},{"location":"yasgui-api/#events-yasgui-events","text":"Yasgui emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasgui.on(\"query\", (instance: Yasgui, tab: Tab) => {}); // Fires when a query is finished yasgui.on(\"queryResponse\", (instance: Yasgui, tab: tab) => {});","title":"Events {#yasgui-events}"},{"location":"yasgui-api/#configuration-yasgui-config","text":"This configuration object is accessible/changeable via Yasgui.defaults or yasgui.config . You can pass these along when initializing Yasgui as well. To change settings to the Yasqe and Yasr components used by Yasgui, you are best off changing the Yasgui.Yasqe.defaults and Yasgui.Yasr.defaults objects before initializing Yasgui. { /** * Change the default request configuration, such as the headers * and default yasgui endpoint. * Define these fields as plain values, or as a getter function */ requestConfig: { endpoint: 'https://example.org/sparql', //Example of using a getter function to define the headers field: headers: () => ({ 'key': 'value' }), method: 'POST', }, // Allow resizing of the Yasqe editor resizeable: true, // Whether to autofocus on Yasqe on page load autofocus: true, // Use the default endpoint when a new tab is opened copyEndpointOnNewTab: false, // Configuring which endpoints appear in the endpoint catalogue list endpointCatalogueOptions { getData: () => { return [ //List of objects should contain the endpoint field //Feel free to include any other fields (e.g. a description or icon //that you'd like to use when rendering) { endpoint: \"https://dbpedia.org/sparql\" }, { endpoint: \"https://query.wikidata.org\" } // ... ]; }, //Data object keys that are used for filtering. The 'endpoint' key already used by default keys: [], //Data argument contains a `value` property for the matched data object //Source argument is the suggestion DOM element to append your rendered item to renderItem: (data, source) => { const contentDiv = document.createElement(\"div\"); contentDiv.innerText = data.value.endpoint; source.appendChild(contentDiv); } } }","title":"Configuration {#yasgui-config}"},{"location":"yasgui-api/#yasqe","text":"Yasqe extends the CodeMirror Library. For an overview of CodeMirror functionality, see the CodeMirror documentation . Note: Where CodeMirror provides CodeMirror in the global namespace, we provide Yasqe .","title":"Yasqe"},{"location":"yasgui-api/#yasqe-api","text":"The Yasqe API can be accessed via yasqe (if Yasqe is run standalone) or via a tab yasgui.getTab().yasqe when run in Yasgui // Set query value in editor yasqe.setValue(\"select * where {...}\"); // Get query value from editor yasqe.getValue(); // execute a query yasqe.query({ url: \"https://dbpedia.org/sparql\", reqMethod: \"POST\", // or \"GET\" headers: { Accept: \"...\" /*...*/ }, args: { arg1: \"val1\" /*...*/ }, withCredentials: false, }); // get whether we're in query or update mode yasqe.getQueryMode(); // get the query type (select, ask, construct, ...) yasqe.getQueryType(); // get prefixes map from the query string yasqe.getPrefixesFromQuery(); // Add prefixes to the query. yasqe.addPrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // Remove prefixes to the query. yasqe.removePrefixes({ dbo: \"http://dbpedia.org/ontology/\" }); // set size of input area yasqe.setSize(500, 300); // Collapsing prefixes if there are any. Use false to expand them. yasqe.collapsePrefixes(true);","title":"Yasqe API"},{"location":"yasgui-api/#events-yasqe-events","text":"Yasqe emits several Events. For information on how to use Events, see NodeJS's Event documentation . // Fires when a query is executed yasqe.on(\"query\", (instance: Yasqe, req: superagent.SuperAgentRequest) => {}); // Fires when a query is finished yasqe.on(\"queryResponse\", (instance: Yasqe, req: superagent.SuperAgentRequest, duration: number) => {});","title":"Events {#yasqe-events}"},{"location":"yasgui-api/#configuration-yasqe-config","text":"The configuration options, for Yasqe can be accessed through Yasgui.Yasqe or yasqe.options . Here are some configurable fields for Yasqe. They can be accessed and set through Yasqe.defaults and yasqe.options . The configuration object extends the CodeMirror config , meaning fields like for example tabSize may also be set. // number of seconds to persist query input, stored in the browser // set to 0 to always load the default query on page load persistencyExpire // default: 30 days // default settings for how to query the endpoint requestOpts: { endpoint: \"http://dbpedia.org/sparql\", method: \"POST\", headers: {} },","title":"Configuration {#yasqe-config}"},{"location":"yasgui-api/#yasr","text":"Yasr is an extendable library that renders SPARQL results. Yasr is responsible for gluing the different visualization plugins together, and providing utilities such as SPARQL result parsers.","title":"Yasr"},{"location":"yasgui-api/#yasr-api","text":"// Set and draw a SPARQL response. The parameter is either // - a plain response string // - a SuperAgent response // - or an object with the specified keys yasr.setResponse({ data: \"...\"; contentType: \"application/sparql-results+json\"; status: 200; executionTime: 1000; // ms // error to show }) // Draw results with current plugin yasr.draw() // Check whether a result has been drawn yasr.somethingDrawn() // Select a plugin yasr.selectPlugin(\"table\") // Download a result set (if possible) yasr.download()","title":"Yasr API"},{"location":"yasgui-api/#events-yasr-events","text":"// Fires just before a plugins draws the results yasr.on(\"draw\",(instance: Yasr, plugin: Plugin) => void); // Fires when a plugin finished drawing the results yasr.on(\"drawn\",(instance: Yasr, plugin: Plugin) => void);","title":"Events {#yasr-events}"},{"location":"yasgui-api/#configuration-yasr-configuration","text":"This configuration object is accessible/changeable via Yasr.defaults and yasr.options , and you can pass these along when initializing Yasr as well. Output visualizations are defined separately. // Ordered list of enabled output plugins pluginOrder = [\"table\", \"response\"] // The default plugin defaultPlugin = \"table\" // seconds before results expire in the browser // Set to 0 to disable results persistency persistencyExpire // default: 30 days // Map of prefixes to use in results views prefixes: {\"dbo\":\"http://dbpedia.org/ontology/\",/*...*/}","title":"Configuration {#yasr-configuration}"},{"location":"yasgui-api/#yasr-plugins","text":"Each plugin has its own configuration options. These options can be accessed through Yasr.plugins .","title":"Yasr plugins"},{"location":"yasgui-api/#table","text":"This plugin shows SPARQL results as a table, using the DataTables.net plugin. This plugin is defined in Yasr.plugins.table and can be configured using Yasr.plugins.table.defaults . // Open URIs in results in a new window rather than the current. openIriInNewWindow = true;","title":"Table"},{"location":"yasgui-api/#raw-response","text":"A plugin which uses CodeMirror to present the SPARQL results as-is. This plugin is defined at Yasr.plugins.response and can be configured using Yasr.plugins.response.defaults . // Number of lines to show before hiding rest of response // (too large value may cause browser performance issues) maxLines = 30;","title":"Raw Response"},{"location":"yasgui-api/#writing-a-yasr-plugin","text":"To register a Yasr plugin, add it to Yasr by running Yasr.registerPlugin(pluginName: string, plugin: Plugin) . Below is an example implementation for rendering the result of an ASK query, which returns either true or false . See also the implementations of the Table and Raw Response plugins. class Boolean { // A priority value. If multiple plugin support rendering of a result, this value is used // to select the correct plugin priority = 10; // Whether to show a select-button for this plugin hideFromSelection = true; constructor(yasr) { this.yasr = yasr; } // Draw the result set. This plugin simply draws the string 'True' or 'False' draw() { const el = document.createElement(\"div\"); el.textContent = this.yasr.results.getBoolean() ? \"True\" : \"False\"; this.yasr.resultsEl.appendChild(el); } // A required function, used to indicate whether this plugin can draw the current // resultset from yasr canHandleResults() { return ( this.yasr.results.getBoolean && (this.yasr.results.getBoolean() === true || this.yasr.results.getBoolean() == false) ); } // A required function, used to identify the plugin, works best with an svg getIcon() { const textIcon = document.createElement(\"p\"); textIcon.innerText = \"\u2713/\u2717\"; return textIcon; } } //Register the plugin to Yasr Yasr.registerPlugin(\"MyBooleanPlugin\", Boolean);","title":"Writing a Yasr plugin"},{"location":"yasgui-api/#faq","text":"","title":"FAQ"},{"location":"yasgui-api/#using-yasgui-in-react","text":"To include Yasgui in React, use the following snippet. This snippet assumes a React repository configured via create-react-app , and a minimum React version of 16.8. import Yasgui from \"@triply/yasgui\"; import \"@triply/yasgui/build/yasgui.min.css\"; export default function App() { useEffect(() => { const yasgui = new Yasgui(document.getElementById(\"yasgui\")); return () => {}; }, []); return <div id=\"yasgui\" />; }","title":"Using Yasgui in react"}]}